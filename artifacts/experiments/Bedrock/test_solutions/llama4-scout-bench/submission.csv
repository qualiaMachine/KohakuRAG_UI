"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet from wu2021b provides information about the total energy consumption of US data centers. It states that the total energy consumption increased by about 4% from 2010-2014.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context snippet from wu2021b provides information about the total energy consumption of US data centers. It states that the total energy consumption increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context provides information about the Amazon Solar Farm Maryland-CPV Backbone, which is expected to avoid more than 64,000 metric tons of CO2e each year. This is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context provides information about the Amazon Solar Farm Maryland-CPV Backbone, which is expected to avoid more than 64,000 metric tons of CO2e each year. This is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context snippet from amazon2023 states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. This directly answers the question of how many data centers AWS began using recycled water for cooling in 2023.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context snippet from amazon2023 states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. This directly answers the question of how many data centers AWS began using recycled water for cooling in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context provides an estimate of embodied carbon emissions per GPU based on the assumption of 3700 kg of CO2eq per 8x server node, which equals 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context provides an estimate of embodied carbon emissions per GPU based on the assumption of 3700 kg of CO2eq per 8x server node, which equals 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 was $40 million, while the total training budget for FLM-101B was $100,000. To find the factor by which the estimated amortized training cost of GPT-4 was greater than the total training budget for FLM-101B, we divide the cost of GPT-4 by the budget of FLM-101B.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","The amortized hardware and energy cost for GPT-4 was $40M (cotti2024). The total training budget for FLM-101B was $100,000 (li2025a).","The estimated amortized training cost of GPT-4 was $40 million, while the total training budget for FLM-101B was $100,000. To find the factor by which the estimated amortized training cost of GPT-4 was greater than the total training budget for FLM-101B, we divide the cost of GPT-4 by the budget of FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides multiple references to the CO2e emissions for a single passenger round trip from San Francisco (SF) to New York (NY). According to the text, a single passenger round trip SF-NY is approximately 1.2t CO2e.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context provides multiple references to the CO2e emissions for a single passenger round trip from San Francisco (SF) to New York (NY). According to the text, a single passenger round trip SF-NY is approximately 1.2t CO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context snippet from li2025a provides a detailed evaluation of FLM-101B on the Open LLM Leaderboard. It mentions that FLM-101B achieves an average score of 43.94, reaching over 90% of the performance of GLM-130B.","43.94","score","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10 21) of the training process for reference.","The context snippet from li2025a provides a detailed evaluation of FLM-101B on the Open LLM Leaderboard. It mentions that FLM-101B achieves an average score of 43.94, reaching over 90% of the performance of GLM-130B."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides information about the improvement in processor clock speed between the Intel 4004 (1971) and typical 2021 microprocessors. It states that there was a more than 6,750 fold improvement in processor clock speed.","6,750","fold","[""wu2021b""]","is_blank","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context provides information about the improvement in processor clock speed between the Intel 4004 (1971) and typical 2021 microprocessors. It states that there was a more than 6,750 fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The training time of GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec is estimated to be approximately 14.8 days. This information is derived from the context provided in [ref_id=patterson2021] and [ref_id=luccioni2023].","14.8","days","[""patterson2021"", ""luccioni2023""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The training time of GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec is estimated to be approximately 14.8 days. This information is derived from the context provided in [ref_id=patterson2021] and [ref_id=luccioni2023]."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model is found in the table from the context. The value is directly provided as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates.","The estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model is found in the table from the context. The value is directly provided as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The total permitted annual emission limits for data center backup generators in northern Virginia were specified in the context. The context mentions that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The total permitted annual emission limits for data center backup generators in northern Virginia were specified in the context. The context mentions that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context snippet from the 2025 paper provides information about the training time cost for FLM-101B using a growth strategy. It states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context snippet from the 2025 paper provides information about the training time cost for FLM-101B using a growth strategy. It states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides an estimate of the public health impacts of U.S. data centers, including premature deaths caused by scope-2 pollutants. According to the text, in 2028, U.S. data centers could contribute to approximately 1,300 premature deaths.","1300","deaths","[""han2024""]","is_blank","Our analysis demonstrates that driven by the growing demand for AI, the U.S. data centers could contribute to, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths in 2028, exceeding 1/3 of asthma deaths in the U.S. each year [40].","The context provides an estimate of the public health impacts of U.S. data centers, including premature deaths caused by scope-2 pollutants. According to the text, in 2028, U.S. data centers could contribute to approximately 1,300 premature deaths."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context snippet from dodge2022 mentions that a 6 billion parameter language model training run took 8 days. This directly answers the question about the approximate training time for a full training run of the 6.1 billion parameter model.","8","days","[""dodge2022""]","is_blank","for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context snippet from dodge2022 mentions that a 6 billion parameter language model training run took 8 days. This directly answers the question about the approximate training time for a full training run of the 6.1 billion parameter model."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The question asks for the model with the highest GPU energy consumption for 1,000 queries during inference. The context provides a table listing various models, their organizations, and GPU energy consumption for 1,000 queries (Wh) for different tasks. The model with the highest GPU energy consumption is Command-R Plus by Cohere, with 3,426.38 Wh.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed)","The question asks for the model with the highest GPU energy consumption for 1,000 queries during inference. The context provides a table listing various models, their organizations, and GPU energy consumption for 1,000 queries (Wh) for different tasks. The model with the highest GPU energy consumption is Command-R Plus by Cohere, with 3,426.38 Wh."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context clearly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context clearly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled. This information is found in multiple references within the context.","22","percent","[""luccioni2025a"", ""jegham2025""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN's Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled. This information is found in multiple references within the context."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides information about the energy consumption of various models, including BLOOM. However, it does not directly provide the energy consumption for pre-training the BLOOM model in MWh. But we can calculate it based on given data.","103.5","MWh","[""cottier2024""]","is_blank","The final training run for the 176B model used 37.24% of the energy of the BLOOM project; if the total cost of the project was C3M as in the grant description, this implies that BLOOM-176B had a cost of $1.2M, which is between our two estimates and aligns more closely with the amortized cost approach ($900K) than the cloud cost approach ($2M).","The context provides information about the energy consumption of various models, including BLOOM. However, it does not directly provide the energy consumption for pre-training the BLOOM model in MWh. But we can calculate it based on given data."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context snippet from [ref_id=shen2024] and [ref_id=patterson2021] mentions that the Switch Transformer model has 1500 billion parameters but only 0.1% are activated per token.","0.1","percent","[""patterson2021"", ""shen2024""]","is_blank","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context snippet from [ref_id=shen2024] and [ref_id=patterson2021] mentions that the Switch Transformer model has 1500 billion parameters but only 0.1% are activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context provides detailed information about the JetMoE-8B architecture, which is a Mixture-of-Experts (MoE) model. It mentions that each MoE layer comprises 8 modules or experts.","8","experts","[""shen2024"", ""xia2024""]","[""https://github.com/myshell-ai/JetMoE""]","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context provides detailed information about the JetMoE-8B architecture, which is a Mixture-of-Experts (MoE) model. It mentions that each MoE layer comprises 8 modules or experts."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides an execution time breakdown for Mixtral and BlackMamba models. For BlackMamba with a batch size of 30, the total execution time can be inferred from the provided figure.","1.5","second","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","The context provides an execution time breakdown for Mixtral and BlackMamba models. For BlackMamba with a batch size of 30, the total execution time can be inferred from the provided figure."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. The data ratio of FLM-101B is 53.5% : 46.5% for English and Chinese.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. The data ratio of FLM-101B is 53.5% : 46.5% for English and Chinese."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context snippet from document [ref_id=khan2025] provides details about the hardware and software settings used for the experimental setup of energy-efficient local inference in financial sentiment classification. The specific hardware processor mentioned is an 11th Gen Intel(R) Core(TM) i7-1165G7 processor.","i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context snippet from document [ref_id=khan2025] provides details about the hardware and software settings used for the experimental setup of energy-efficient local inference in financial sentiment classification. The specific hardware processor mentioned is an 11th Gen Intel(R) Core(TM) i7-1165G7 processor."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","","","models","[]","is_blank","is_blank",""
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context snippet from wu2021a provides a figure (Fig. 9) illustrating that as GPU utilization improves for LM training on GPUs, both embodied and operational carbon emissions will reduce. Specifically, it states that increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","3","multiplier","[""wu2021a""]","is_blank","Fig. 9. As accelerator utilization improves over time, both operational and embodied carbon footprints of AI improve. Carbon-free energy helps reduce the operational carbon footprint, making embodied carbon cost the dominating factor. To reduce the rising carbon footprint of AI computing at-scale, we must complement efﬁciency and utilization optimization with novel approaches to reduce the remaining embodied carbon footprint of AI systems.","The context snippet from wu2021a provides a figure (Fig. 9) illustrating that as GPU utilization improves for LM training on GPUs, both embodied and operational carbon emissions will reduce. Specifically, it states that increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) estimate that the total compute for model development is 1.2x to 4x larger than the final training run compute. This is based on a log-normal distribution with a 90% CI of 1.2x to 4x.","[1.2, 4]","multiplier","[""cottier2024""]","is_blank","Appendix A.6 provides further details. Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2025) estimate that the total compute for model development is 1.2x to 4x larger than the final training run compute. This is based on a log-normal distribution with a 90% CI of 1.2x to 4x."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model is calculated based on the energy consumption of a partial training run. The model was trained for 8 days on 256 NVIDIA A100s, consuming 13.8 MWh. Since this was only 13% of the full training run, which would take 60 days, the estimated total energy consumption is (60/8) * 13.8 = 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model is calculated based on the energy consumption of a partial training run. The model was trained for 8 days on 256 NVIDIA A100s, consuming 13.8 MWh. Since this was only 13% of the full training run, which would take 60 days, the estimated total energy consumption is (60/8) * 13.8 = 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons' Paradox, an economic principle that suggests efficiency gains can lead to increased consumption. This principle is mentioned in the context as a critical rebound mechanism.","is_blank","is_blank","[""luccioni2025a""]","is_blank","is_blank","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons' Paradox, an economic principle that suggests efficiency gains can lead to increased consumption. This principle is mentioned in the context as a critical rebound mechanism."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a projection for the global AI demand's water withdrawal by 2027. According to the text, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context provides a projection for the global AI demand's water withdrawal by 2027. According to the text, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context clearly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost.' This indicates that despite the awareness of diminishing returns, Red AI continues to grow, not decline.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context clearly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost.' This indicates that despite the awareness of diminishing returns, Red AI continues to grow, not decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The total wall-clock time required to train the FLM-101B model using a growth strategy is 21.54 days. This information is directly provided in the context snippet.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The total wall-clock time required to train the FLM-101B model using a growth strategy is 21.54 days. This information is directly provided in the context snippet."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context clearly states that a vast majority of model experimentation workflows utilize GPUs at only 30-50%, leaving room for utilization and efficiency improvements. This directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context clearly states that a vast majority of model experimentation workflows utilize GPUs at only 30-50%, leaving room for utilization and efficiency improvements. This directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The training of GPT-3 is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity. This information is directly provided in the context snippets.","1287","MWh","[""jegham2025"", ""li2025b""]","[""https://shrinkthatfootprint.com/carbon-footprint-of-training-gpt-3-and-large-language-models/"", ""https://arxiv.org/abs/2505.09598v6""]","GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.","The training of GPT-3 is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity. This information is directly provided in the context snippets."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The question asks for the name of a collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models. The context mentions the 'AI Energy Score' project, which provides a standardized methodology for comparing models across different tasks.","AI Energy Score","is_blank","[""luccioni2025c"", ""chung2025""]","[""https://arxiv.org/abs/2501.12948"", ""https://ml.energy/leaderboard""]","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The question asks for the name of a collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models. The context mentions the 'AI Energy Score' project, which provides a standardized methodology for comparing models across different tasks."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides a figure (Fig. 6) showing the kernel-level MoE time breakdown for different batch sizes and models. For a dense BlackMamba model with a batch size of 30, we can see that the execution time for the longest kernel is approximately 2000 microseconds.","2000","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels.","The context provides a figure (Fig. 6) showing the kernel-level MoE time breakdown for different batch sizes and models. For a dense BlackMamba model with a batch size of 30, we can see that the execution time for the longest kernel is approximately 2000 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context provides information about the JetMoE-8B model, specifically its hyperparameters. According to the table in the context, the top-k value for JetMoE-8B is 2, meaning that for each token, 2 experts are selected for activation in each layer.","2","experts","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context provides information about the JetMoE-8B model, specifically its hyperparameters. According to the table in the context, the top-k value for JetMoE-8B is 2, meaning that for each token, 2 experts are selected for activation in each layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context provides information about the increasing computational intensity of deep learning models. According to the text from schwartz2019, there has been a 300,000x increase in the amount of compute used to train deep learning models over a six-year span (2012-2018). This directly answers the question.","300,000x","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","The context provides information about the increasing computational intensity of deep learning models. According to the text from schwartz2019, there has been a 300,000x increase in the amount of compute used to train deep learning models over a six-year span (2012-2018). This directly answers the question."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from wu2021b mentions that the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction. This information directly answers the question about the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context snippet from wu2021b mentions that the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction. This information directly answers the question about the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context snippet from amazon2023 states that 'Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.'","22","data centers","[""amazon2023""]","is_blank","['Data Centers Powered with Renewable Energy section in amazon2023']","The context snippet from amazon2023 states that 'Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.'"
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find the approximate age of the field in 2025, we calculate 2025 - 1956 = 69 years.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find the approximate age of the field in 2025, we calculate 2025 - 1956 = 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate, originating from a 2019 study by Strubell et al., is based on the energy required for neural architecture search (NAS), a large-scale procedure performed less frequently than the average AI model training workload.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,12 which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original “base” (65M) and “big” (213M parameter) variants of the Transformer neural network architecture30 that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.  In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The 'five cars' carbon footprint estimate, originating from a 2019 study by Strubell et al., is based on the energy required for neural architecture search (NAS), a large-scale procedure performed less frequently than the average AI model training workload."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context provides information about the Llama 3.1 8B model and its energy consumption characteristics. Specifically, it mentions that targeting an average Time Per Output Token (TPOT) of 77 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency. Since the question asks for the percentage decrease when targeting an average TPOT of 100 ms, we can infer that the 44% reduction is associated with a TPOT of 77 ms, which is close to but not exactly 100 ms. However, this is the closest relevant information provided.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context provides information about the Llama 3.1 8B model and its energy consumption characteristics. Specifically, it mentions that targeting an average Time Per Output Token (TPOT) of 77 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency. Since the question asks for the percentage decrease when targeting an average TPOT of 100 ms, we can infer that the 44% reduction is associated with a TPOT of 77 ms, which is close to but not exactly 100 ms. However, this is the closest relevant information provided."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III of the provided context. The table lists the maximum batch size for different model and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 8.","8","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III of the provided context. The table lists the maximum batch size for different model and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 8."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to the context snippet [ref_id=amazon2023], Amazon held 1.3 GW of storage capacity as of 2023. The context directly provides this information, and the unit is already in gigawatts (GW), matching the expected format.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","According to the context snippet [ref_id=amazon2023], Amazon held 1.3 GW of storage capacity as of 2023. The context directly provides this information, and the unit is already in gigawatts (GW), matching the expected format."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context provides information on the estimated annual carbon footprint of GPT-4o inference. It states that the annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","[272, 272]","flights","[""jegham2025""]","is_blank","Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context provides information on the estimated annual carbon footprint of GPT-4o inference. It states that the annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide specific information on the percentage of AI inference workloads in Asia powered by coal in 2023. However, it does mention that globally, 38 models out of 61 used coal as their primary energy source for training, with an average carbon intensity of 512.3 gCO2eq/kWh.","is_blank","percent","[""is_blank""]","is_blank","Table 2. Main Energy Sources for the models analyzed and their carbon intensities [24, 52]","The context does not provide specific information on the percentage of AI inference workloads in Asia powered by coal in 2023. However, it does mention that globally, 38 models out of 61 used coal as their primary energy source for training, with an average carbon intensity of 512.3 gCO2eq/kWh."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context provides information on the average power usage effectiveness (PUE) of data centers worldwide. According to the text, the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024"", ""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The power usage effectiveness (PUE) metric reflects the energy efficiency of a data center. It indicates the ratio of the total energy needed by a data center, including components such as cooling, to the energy used solely by computational devices. The average data center PUE in 2023 was 1.58 globally.","The context provides information on the average power usage effectiveness (PUE) of data centers worldwide. According to the text, the average data center PUE in 2023 was 1.58 globally."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context snippet from shen2024 states that JetMoE-8B has 8B parameters while only activating 2B for each input token. This directly answers the question about the number of parameters activated during inference for each input token.","2","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context snippet from shen2024 states that JetMoE-8B has 8B parameters while only activating 2B for each input token. This directly answers the question about the number of parameters activated during inference for each input token."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The GHG emissions associated with pre-training the Llama 7B model are 356 tCO2e. This information is directly obtained from a table in the provided context.","356","tCO2e","[""luccioni2025c""]","is_blank","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed) in luccioni2025c","The GHG emissions associated with pre-training the Llama 7B model are 356 tCO2e. This information is directly obtained from a table in the provided context."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The question asks for the total number of Amazon electric delivery vans added across 2022 and 2023. The context provides information on the number of electric delivery vans deployed in the U.S., Europe, and India for both years. In 2022, the total number of electric delivery vans was 2,600 (U.S.) + 1,220 (Europe) + 3,800 (India) = 7,620. In 2023, the total number was 11,800 (U.S.) + 300 (Europe) + 3,600 (India) = 15,700. Therefore, the total number of electric delivery vans added across 2022 and 2023 is 7,620 + 15,700 = 23,320.","19000","electric delivery vans","[""amazon2023"", ""amazon2023""]","is_blank","In 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles, including 19,000 electric delivery vans, around the world.","The question asks for the total number of Amazon electric delivery vans added across 2022 and 2023. The context provides information on the number of electric delivery vans deployed in the U.S., Europe, and India for both years. In 2022, the total number of electric delivery vans was 2,600 (U.S.) + 1,220 (Europe) + 3,800 (India) = 7,620. In 2023, the total number was 11,800 (U.S.) + 300 (Europe) + 3,600 (India) = 15,700. Therefore, the total number of electric delivery vans added across 2022 and 2023 is 7,620 + 15,700 = 23,320."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The energy consumption of the o3 model for a long prompt is found in Table 4 of the document. The table lists the energy consumption for various models across three prompt sizes. For the o3 model, the energy consumption for a long prompt (10k input-1.5k output) is 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).","The energy consumption of the o3 model for a long prompt is found in Table 4 of the document. The table lists the energy consumption for various models across three prompt sizes. For the o3 model, the energy consumption for a long prompt (10k input-1.5k output) is 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The field of Artificial Intelligence was officially christened at a workshop in 1956. The context clearly states that 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.'","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially christened at a workshop in 1956. The context clearly states that 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.'"
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide a specific value for Google's AI-dedicated data centers' average water use effectiveness (WUE) in 2024. However, it mentions AWS's industry-leading global data center WUE of 0.18 liters of water per kilowatt-hour (L/kWh) in 2023.","is_blank","WUE","[""amazon2023""]","is_blank","AWS Water Use Effectiveness: improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.","The context does not provide a specific value for Google's AI-dedicated data centers' average water use effectiveness (WUE) in 2024. However, it mentions AWS's industry-leading global data center WUE of 0.18 liters of water per kilowatt-hour (L/kWh) in 2023."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context snippet from wu2021b provides information that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context snippet from wu2021b provides information that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity according to the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context provides information about the energy consumption of LLaMA-65B at different maximum generation lengths. For a maximum generation length of 512 tokens, it is mentioned that 'it takes about 3-4 Joules for an output token'. This directly answers the question about energy per token.","[3, 4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context provides information about the energy consumption of LLaMA-65B at different maximum generation lengths. For a maximum generation length of 512 tokens, it is mentioned that 'it takes about 3-4 Joules for an output token'. This directly answers the question about energy per token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context snippet from wu2021a provides information on the reduction of the overall model size of RM2 after quantization. By converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context snippet from wu2021a provides information on the reduction of the overall model size of RM2 after quantization. By converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","","","is_blank","[]","is_blank","is_blank",""
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context clearly states that sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy. This is mentioned in multiple references, including patterson2021 and patterson2021 (again, in the abstract).","1","is_blank","[""patterson2021"", ""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context clearly states that sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy. This is mentioned in multiple references, including patterson2021 and patterson2021 (again, in the abstract)."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The estimated cost of training AI2's Grover on 256 TPU chips for two weeks is provided directly in the context. The cost is mentioned as $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The estimated cost of training AI2's Grover on 256 TPU chips for two weeks is provided directly in the context. The cost is mentioned as $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context snippet from [ref_id=xia2024] mentions that 'The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)'. This directly provides the percentage of the running time the optimizer stage takes in BlackMamba sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context snippet from [ref_id=xia2024] mentions that 'The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)'. This directly provides the percentage of the running time the optimizer stage takes in BlackMamba sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context does not provide direct information about the energy consumption of Google Translate or a comparable multi-purpose model like Flan-T5-xxl. However, it mentions that a large language model can perform 1 billion queries per day. Given the average energy consumption of 0.083 kWh/1k queries for a comparable model, we can estimate the daily energy consumption.","83","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide direct information about the energy consumption of Google Translate or a comparable multi-purpose model like Flan-T5-xxl. However, it mentions that a large language model can perform 1 billion queries per day. Given the average energy consumption of 0.083 kWh/1k queries for a comparable model, we can estimate the daily energy consumption."
"q067","What was the average global data center PUE in 2023?","The average global data center PUE in 2023 was 1.58. This information is directly provided in the context snippet from ebert2024.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally and 1.6 in the EU.","The average global data center PUE in 2023 was 1.58. This information is directly provided in the context snippet from ebert2024."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. The context mentions Amazon's renewable energy projects and investments but does not mention Microsoft's specific actions regarding wind turbine contracts for Azure AI clusters.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. The context mentions Amazon's renewable energy projects and investments but does not mention Microsoft's specific actions regarding wind turbine contracts for Azure AI clusters."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provides a detailed breakdown of the costs associated with developing various frontier AI models, including Gemini Ultra. According to the text, R&D staff costs, including equity, make up between 29% and 49% of the total amortized model development costs for different models. Specifically, it is mentioned that Gemini Ultra has the highest fraction of R&D staff cost at 49%.","49","percent","[""cottier2024""]","is_blank","Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total. Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context provides a detailed breakdown of the costs associated with developing various frontier AI models, including Gemini Ultra. According to the text, R&D staff costs, including equity, make up between 29% and 49% of the total amortized model development costs for different models. Specifically, it is mentioned that Gemini Ultra has the highest fraction of R&D staff cost at 49%."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context snippet from stone2022 provides information about the Study Panel of the One Hundred Year Study on Artificial Intelligence. It mentions that the seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of  experts in AI from academia, corporate laboratories and industry , and AI-savvy scholars in law , political science, policy , and economics, was launched in mid-fall 2015.","The context snippet from stone2022 provides information about the Study Panel of the One Hundred Year Study on Artificial Intelligence. It mentions that the seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context snippet from wu2021a states that 'manufacturing carbon cost accounts for 74% of the total footprint of client devices.' This directly answers the question about the percentage of a client device's total carbon footprint accounted for by its manufacturing.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context snippet from wu2021a states that 'manufacturing carbon cost accounts for 74% of the total footprint of client devices.' This directly answers the question about the percentage of a client device's total carbon footprint accounted for by its manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context does not support a blanket statement that a model with more parameters will always consume more energy during inference. While it is generally observed that models with more parameters tend to consume more energy, there are exceptions based on model architecture and hardware specifications.","0","is_blank","[""chung2025"", ""zschache2025""]","is_blank","Generally, models with more parameters consume more energy, but this is not always the case. For example, the Phi-3 Mini (3.8B) and Small (7B) models show that the larger model can consume less energy than the smaller one under certain conditions due to differences in attention mechanisms.","The context does not support a blanket statement that a model with more parameters will always consume more energy during inference. While it is generally observed that models with more parameters tend to consume more energy, there are exceptions based on model architecture and hardware specifications."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel from the 100 Year Study on AI found no cause for concern that AI is an imminent threat to humankind. This information directly answers the question.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The Study Panel from the 100 Year Study on AI found no cause for concern that AI is an imminent threat to humankind. This information directly answers the question."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide specific information on the amount of CO2 emitted by OpenAI's API requests in January 2024. The provided context snippets discuss the environmental impact of AI in general, but do not include specific data on OpenAI's API requests for January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the amount of CO2 emitted by OpenAI's API requests in January 2024. The provided context snippets discuss the environmental impact of AI in general, but do not include specific data on OpenAI's API requests for January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The reported GHG emissions from the pre-training process for Meta's Llama 3 family of models is 11,390 tons CO2e. This is compared to the 'five cars' estimate, which is stated to be over 40x less than the actual emissions of Llama 3.","11390","tCO2e","[""luccioni2025c""]","is_blank","Recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the 'five cars' estimate: ... Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e, or over 40x the 'five cars' estimate.","The reported GHG emissions from the pre-training process for Meta's Llama 3 family of models is 11,390 tons CO2e. This is compared to the 'five cars' estimate, which is stated to be over 40x less than the actual emissions of Llama 3."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context snippet from wu2021a provides information on the increase in AI training infrastructure capacity at Facebook over a 1.5-year period. It states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context snippet from wu2021a provides information on the increase in AI training infrastructure capacity at Facebook over a 1.5-year period. It states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years."
"q079","How many miles is the Earth from the Sun?","The context does not provide any information about the distance of the Earth from the Sun.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the distance of the Earth from the Sun."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that AlphaGo defeated the human Go champion in a five-game match. This information is found in multiple references, confirming the fact.","1","is_blank","[""stone2022"", ""schwartz2019""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html"", ""https://www.nature.com/articles/nature16961""]","AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","The context clearly states that AlphaGo defeated the human Go champion in a five-game match. This information is found in multiple references, confirming the fact."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions 'continuous batching' as a strategy that mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.","continuous batching","is_blank","[""fernandez2025"", ""jegham2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions 'continuous batching' as a strategy that mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. This information is directly provided in the context.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. This information is directly provided in the context."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context provides a comparison between the Max-Performance policy and InferSave for an offline workload experiment with a 100 TPS SLO. InferSave selected g4dn.xlarge at $2.13, while Max-Performance selected g6e.xlarge at $2.699. The increase in cost is calculated as ((2.699 - 2.13) / 2.13) * 100, which equals approximately 26.7%.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context provides a comparison between the Max-Performance policy and InferSave for an offline workload experiment with a 100 TPS SLO. InferSave selected g4dn.xlarge at $2.13, while Max-Performance selected g6e.xlarge at $2.699. The increase in cost is calculated as ((2.699 - 2.13) / 2.13) * 100, which equals approximately 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context snippet from luccioni2024 provides information about the carbon intensity of various AI models. Specifically, it mentions that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) generates as much carbon as 0.0006 miles driven by a similar vehicle, i.e.","The context snippet from luccioni2024 provides information about the carbon intensity of various AI models. Specifically, it mentions that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The range of GPU energy usage for performing 1,000 inference queries can be derived from the data provided in the appendix of the 2025 study. The table lists various models with their corresponding energy usage in Wh for 1,000 queries. The lowest energy usage is 0.06 Wh (bert-tiny-finetuned-squadv2) and the highest is 3,426.38 Wh (Command-R Plus).","[0.06, 3426.38]","Wh","[""luccioni2025c""]","is_blank","Table 2 in the appendix of [ref_id=luccioni2025c]","The range of GPU energy usage for performing 1,000 inference queries can be derived from the data provided in the appendix of the 2025 study. The table lists various models with their corresponding energy usage in Wh for 1,000 queries. The lowest energy usage is 0.06 Wh (bert-tiny-finetuned-squadv2) and the highest is 3,426.38 Wh (Command-R Plus)."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context suggests that researchers do not believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. This is because different types of AI approaches exist, and various contextual factors influence their application, making it difficult to define universal guidelines.","0","is_blank","[""luccioni2025b""]","is_blank","For instance, a 2019 analysis by Jobin et al. reviewed 84 sets of guidelines mentioning a variety of principles, finding very limited convergence between them but identifying the values of transparency, fairness, non-maleficence, privacy and responsibility as being most common [2019]. Given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.","The context suggests that researchers do not believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. This is because different types of AI approaches exist, and various contextual factors influence their application, making it difficult to define universal guidelines."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The gross carbon intensity of energy according to the U.S. average mix in 2021 was found to be 0.429 kg of CO2e/KWh. This information is directly provided in the context snippet from patterson2021.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The gross carbon intensity of energy according to the U.S. average mix in 2021 was found to be 0.429 kg of CO2e/KWh. This information is directly provided in the context snippet from patterson2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context provides information about a PyTorch-based framework called Hivemind, which enables collaborative deep learning training in a decentralized fashion. It allows participants to donate their heterogeneous hardware to train a single model together, handling peers that drop out at any stage of the training.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context provides information about a PyTorch-based framework called Hivemind, which enables collaborative deep learning training in a decentralized fashion. It allows participants to donate their heterogeneous hardware to train a single model together, handling peers that drop out at any stage of the training."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","","","is_blank","[]","is_blank","is_blank",""
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. Specifically, for emotion classification, the linear model with sentence embeddings is among the top-performing models.","linear model with sentence embeddings","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. ... Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models.","The context mentions that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. Specifically, for emotion classification, the linear model with sentence embeddings is among the top-performing models."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context provides information about a paper by Chen et al. in 2024, not 2025, which introduces model-attention disaggregation for improving LLM decoding efficiency. The LLM inference system developed using this approach is called Lamina.","Lamina","is_blank","[""chen2024""]","is_blank","To validate our analysis, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context provides information about a paper by Chen et al. in 2024, not 2025, which introduces model-attention disaggregation for improving LLM decoding efficiency. The LLM inference system developed using this approach is called Lamina."
"q093","How many parameters does the largest T5 model have?","The context does not provide information on the largest T5 model. The provided context snippets discuss various models, including T5, but do not specify the parameter count for the largest T5 model.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide information on the largest T5 model. The provided context snippets discuss various models, including T5, but do not specify the parameter count for the largest T5 model."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context provides information about the JetMoE-8B model, including its hyperparameters. The total number of parameters in the JetMoE-8B model is mentioned as 8B.","8B","parameters","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters, Ptotal: 8B","The context provides information about the JetMoE-8B model, including its hyperparameters. The total number of parameters in the JetMoE-8B model is mentioned as 8B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","","","percent","[]","is_blank","is_blank",""
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context provides a table of common carbon emission metrics in Green AI, which includes the definition of Carbon Intensity as 'CO₂ emissions per unit of electricity consumed'.","Carbon Intensity","is_blank","[""khan2025"", ""dodge2022""]","is_blank","TABLE I
COMMON CARBON EMISSION METRICS IN GREEN AI
Metric Unit Definition Reference
Carbon Dioxide Equivalent (CO2e) Metric tons (tCO2e) A measure of greenhouse gases expressed as CO2 equivalent IPCC, GHG Protocol
Carbon Intensity gCO2/ kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context provides a table of common carbon emission metrics in Green AI, which includes the definition of Carbon Intensity as 'CO₂ emissions per unit of electricity consumed'."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The final growth stage of FLM-101B training refers to the 101B stage. According to Table 2 in the context, the FLOPs utilization percentage for the 101B stage is 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size. Params (billion) Tensor Pipeline Data Number Batch teraFLOP/s FLOPs Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 16 2 1 96 192 2304 162 51.90% 51 4 2 24 192 2304 160 51.30% 101 4 4 12 192 2160 165 52.88%","The final growth stage of FLM-101B training refers to the 101B stage. According to Table 2 in the context, the FLOPs utilization percentage for the 101B stage is 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The estimated amortized training cost for OpenAI's GPT-4 model is $40 million. This information is found in the context snippets from documents [cottier2024] and [xia2024].","40","USD","[""cottier2024"", ""xia2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. The estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, rendering it financially prohibitive for most small-to-medium size enterprises and the academic community.","The estimated amortized training cost for OpenAI's GPT-4 model is $40 million. This information is found in the context snippets from documents [cottier2024] and [xia2024]."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","","","multiplier","[]","is_blank","is_blank",""
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information on the performance of NLP when training across different regions and continents. Specifically, it mentions that for NLP, training across four continents results in a 41% drop in throughput compared to the fully local experiment.","0.59","multiplier","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","Intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context provides information on the performance of NLP when training across different regions and continents. Specifically, it mentions that for NLP, training across four continents results in a 41% drop in throughput compared to the fully local experiment."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This directly answers the question and matches the expected format of providing a numeric value in liters.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context clearly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This directly answers the question and matches the expected format of providing a numeric value in liters."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context clearly supports that using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs. The study by Riccardo Rubei et al. shows that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","1","is_blank","[""rubei2025""]","is_blank","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags. Moreover, we show that the energy consumption of LLMs is highly dependent on the used PETs.","The context clearly supports that using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs. The study by Riccardo Rubei et al. shows that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","According to the context snippet from Luccioni2025a, NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","According to the context snippet from Luccioni2025a, NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context provides a detailed breakdown of the components of amortized hardware CapEx + energy cost for training frontier AI models. According to the text, on average, 44% of this cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context provides a detailed breakdown of the components of amortized hardware CapEx + energy cost for training frontier AI models. According to the text, on average, 44% of this cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The Power Usage Effectiveness (PUE) for Facebook's data centers is mentioned in multiple references as 1.10.","1.1","PUE","[""wu2021a"", ""wu2021b"", ""ebert2024""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.","The Power Usage Effectiveness (PUE) for Facebook's data centers is mentioned in multiple references as 1.10."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","","","is_blank","[]","is_blank","is_blank",""
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The estimated amortized training cost for Google's Gemini Ultra was $30M. This information was found in the context provided, specifically mentioning that among frontier models, the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","30000000","USD","[""cottier2024""]","is_blank","The most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The estimated amortized training cost for Google's Gemini Ultra was $30M. This information was found in the context provided, specifically mentioning that among frontier models, the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems. These measures should consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8. The Act requires providers to include environmental risks in their risk assessments.","1","is_blank","[""ebert2024""]","is_blank","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems. These measures should consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8. The Act requires providers to include environmental risks in their risk assessments."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The EPA's recently tightened primary standard for the annual average limit of PM2.5 is mentioned in the context as 9µg/m³, which is considerably higher than the WHO's recommended level of 5µg/m³.","9","µg/m³","[""han2024""]","is_blank","The EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m³.","The EPA's recently tightened primary standard for the annual average limit of PM2.5 is mentioned in the context as 9µg/m³, which is considerably higher than the WHO's recommended level of 5µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. This information is used to answer the question.","115","books","[""luccioni2025a""]","is_blank","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","A life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. This information is used to answer the question."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context mentions that per-household health impacts in disadvantaged communities can be up to 200 times higher than in less-affected areas. This is stated in multiple sections, emphasizing the disparity in health burdens between different communities.","200","multiplier","[""han2024""]","is_blank","The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context mentions that per-household health impacts in disadvantaged communities can be up to 200 times higher than in less-affected areas. This is stated in multiple sections, emphasizing the disparity in health burdens between different communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is found in the table from the document. The table provides measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown as averages over 10 runs.","702.06","Wh","[""zschache2025""]","is_blank","Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is found in the table from the document. The table provides measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown as averages over 10 runs."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not provide information about a 2022 paper by Dodge et al. or the total number of parameters in the large language model they analyzed. The provided context snippets do not mention Dodge et al. in 2022. Therefore, the answer is 'is_blank'.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide information about a 2022 paper by Dodge et al. or the total number of parameters in the large language model they analyzed. The provided context snippets do not mention Dodge et al. in 2022. Therefore, the answer is 'is_blank'."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described as technological progress improving efficiency, which then results in increased usage and overall resource consumption, is known as Jevons' Paradox. This concept is mentioned in the provided context snippets as a situation where efficiency gains may paradoxically spur increased consumption.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The phenomenon described as technological progress improving efficiency, which then results in increased usage and overall resource consumption, is known as Jevons' Paradox. This concept is mentioned in the provided context snippets as a situation where efficiency gains may paradoxically spur increased consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The energy consumption of GPT-3 and Meena were found in the table from the context. GPT-3 consumed 1287 MWh and Meena consumed 85.7 MWh. Dividing GPT-3's consumption by Meena's gives the multiplier.","15","multiplier","[""patterson2021""]","is_blank","['Table from [ref_id=patterson2021]']","The energy consumption of GPT-3 and Meena were found in the table from the context. GPT-3 consumed 1287 MWh and Meena consumed 85.7 MWh. Dividing GPT-3's consumption by Meena's gives the multiplier."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The 2024 study on AI's power consumption provides a table (Table 2) that lists the mean and standard deviation of energy per 1,000 queries for various tasks. According to this table, the average energy consumption for performing 1,000 image generation inferences is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2 in [ref_id=luccioni2024]: inference energy (kWh) task mean std text classification 0.002 0.001 extractive QA 0.003 0.001 masked language modeling 0.003 0.001 token classification 0.004 0.002 image classification 0.007 0.001 object detection 0.038 0.02 text generation 0.047 0.03 summarization 0.049 0.01 image captioning 0.063 0.02 image generation 2.907 3.31","The 2024 study on AI's power consumption provides a table (Table 2) that lists the mean and standard deviation of energy per 1,000 queries for various tasks. According to this table, the average energy consumption for performing 1,000 image generation inferences is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. The conversion takes into account the relative proportions of different energy sources consumed to produce energy in the United States. Specifically, it is mentioned that CO2e = 0.954pt, where pt is the total power required at a given instance during training.","18560","lbs","[""strubell2019"", ""dodge2022""]","[""https://bit.ly/30sGEbi"", ""https://bit.ly/2LObQhV""]","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. The conversion takes into account the relative proportions of different energy sources consumed to produce energy in the United States. Specifically, it is mentioned that CO2e = 0.954pt, where pt is the total power required at a given instance during training."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a table showing the top-10 counties with the highest per-household health costs. Among these, WV Marion has the highest per-household health cost at $1218.3.","Marion","is_blank","[""han2024""]","is_blank","Table 5c: Top-10 counties by per-household health cost","The context provides a table showing the top-10 counties with the highest per-household health costs. Among these, WV Marion has the highest per-household health cost at $1218.3."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The carbon emissions for Mistral-small before optimization were 0.020 kg CO2 and after optimization were 0.015 kg CO2. To find the multiplier, we divide the after optimization value by the before optimization value: 0.015 / 0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","Table III in the document","The carbon emissions for Mistral-small before optimization were 0.020 kg CO2 and after optimization were 0.015 kg CO2. To find the multiplier, we divide the after optimization value by the before optimization value: 0.015 / 0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","","","kWh","[]","is_blank","is_blank",""
"q125","What is the total number of parameters in the final FLM-101B model?","The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. The total number of parameters in the final FLM-101B model is 101 billion.","101","parameters","[""li2025a""]","is_blank","FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. The total number of parameters in the final FLM-101B model is 101 billion."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The energy required for a full training run of a 6.1B parameter model is estimated to be 103,500 kWh. Using the energy cost of BLOOMz-7B, we find that approximately 592,570,000 inferences are needed to match this training energy cost.","592570000","inferences","[""luccioni2024"", ""dodge2022""]","is_blank","Table 5 in luccioni2024 provides the cost parity for BLOOMz-7B as 592,570,000 inferences. dodge2022 estimates a full training run of a 6.1B parameter model to consume approximately 103,500 kWh.","The energy required for a full training run of a 6.1B parameter model is estimated to be 103,500 kWh. Using the energy cost of BLOOMz-7B, we find that approximately 592,570,000 inferences are needed to match this training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The 2024 study 'Power Hungry Processing' by Luccioni et al. provides detailed information on the energy consumption of various AI models. According to the context, the total amount of energy consumed for all model experimentation and evaluation was 754.66 kWh.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The 2024 study 'Power Hungry Processing' by Luccioni et al. provides detailed information on the energy consumption of various AI models. According to the context, the total amount of energy consumed for all model experimentation and evaluation was 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the BLOOMz models with their training energy cost, fine-tuning energy cost, inference cost, and cost parity as the number of inferences required to sum to the training cost. For the BLOOMz-7B model, the cost parity is listed as 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides a table (Table 5) that lists the BLOOMz models with their training energy cost, fine-tuning energy cost, inference cost, and cost parity as the number of inferences required to sum to the training cost. For the BLOOMz-7B model, the cost parity is listed as 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany. The statements from the population were categorized, processed, and published as the FKTG-dataset.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The text of the submission is given by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains the category of the text.","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany. The statements from the population were categorized, processed, and published as the FKTG-dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","","","liters","[]","is_blank","is_blank",""
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide specific information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. It mentions that an H100 is assumed to be 0.1% rare earth metal by mass and provides estimates of environmental impacts from mining rare earth metals, but does not specify the percentage of recycled materials used in the GPUs.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. It mentions that an H100 is assumed to be 0.1% rare earth metal by mass and provides estimates of environmental impacts from mining rare earth metals, but does not specify the percentage of recycled materials used in the GPUs."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. This directly answers the question.","3","passengers","[""patterson2021""]","is_blank","The CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context provides that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. This directly answers the question."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the May 2025 data from the API platform OpenRouter, 84% of LLM token usage occurred through models with no disclosure of their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to the May 2025 data from the API platform OpenRouter, 84% of LLM token usage occurred through models with no disclosure of their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA-13B, it is mentioned that 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128 TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context provides information about the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA-13B, it is mentioned that 1 A100 80GB GPU is required."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The estimated range of CO2 emissions for a complete training run of a 6.1 billion parameter transformer model is provided in the context snippet from dodge2022. The authors estimate that a full training run would emit 21 to 78 metric tons of CO2, depending on the region it was run in.","[21, 78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The estimated range of CO2 emissions for a complete training run of a 6.1 billion parameter transformer model is provided in the context snippet from dodge2022. The authors estimate that a full training run would emit 21 to 78 metric tons of CO2, depending on the region it was run in."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide specific information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. However, it mentions that pruning, quantization, and efficient coding can improve the energy efficiency of DNNs 3X–7X.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. However, it mentions that pruning, quantization, and efficient coding can improve the energy efficiency of DNNs 3X–7X."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context snippet provides information about a scenario where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only strategy.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context snippet provides information about a scenario where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides a table with specifications for H100, H20, and TPU v6e. The price per hour for an NVIDIA H20 is listed as $4.63/hr.","4.63","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications.","The context provides a table with specifications for H100, H20, and TPU v6e. The price per hour for an NVIDIA H20 is listed as $4.63/hr."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context indicates that the authors had to contact the first author of each paper to gather missing training details regarding their model, achieving an author response rate of 15.4%. This implies that most carbon footprint analyses for AI models do not gather information automatically without needing to contact authors.","0","is_blank","[""luccioni2023""]","is_blank","We then contacted the first author of each of the papers and asked them to provide missing training details regarding their model (See Supplementary Materials A.1 for the email text). We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","The context indicates that the authors had to contact the first author of each paper to gather missing training details regarding their model, achieving an author response rate of 15.4%. This implies that most carbon footprint analyses for AI models do not gather information automatically without needing to contact authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The public health cost of U.S. data centers in 2023 was about $6.7 billion, or $47.5 per household. This is equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","The public health cost of U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers' total electricity cost.","The public health cost of U.S. data centers in 2023 was about $6.7 billion, or $47.5 per household. This is equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA 7B, it is mentioned that only 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context provides information on the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA 7B, it is mentioned that only 1 A100 80GB GPU is required."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context describes a study that explored the integration of energy-efficient optimization techniques in the deployment of large language models (LLMs). The study presented a case study and framework demonstrating how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results revealed that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context describes a study that explored the integration of energy-efficient optimization techniques in the deployment of large language models (LLMs). The study presented a case study and framework demonstrating how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results revealed that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context snippet from luccioni2023 provides information about the author response rate for a carbon footprint analysis survey. The researchers contacted 500 authors and collected information for a total of 95 models from 77 papers, which represents an author response rate of 15.4%.","95","answers","[""luccioni2023""]","is_blank","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","The context snippet from luccioni2023 provides information about the author response rate for a carbon footprint analysis survey. The researchers contacted 500 authors and collected information for a total of 95 models from 77 papers, which represents an author response rate of 15.4%."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The cost per H100 GPU-hour for the JetMoE project is estimated by dividing the total budget by the total GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The cost per H100 GPU-hour for the JetMoE project is estimated by dividing the total budget by the total GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides information on the health impact of training a Llama-3.1 scale model in different locations. Specifically, it mentions that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For Iowa, the total health cost is $2.5 million, and for Oregon, it is $0.23 million. However, the exact percentage for Altoona, Iowa is not directly provided, but it is mentioned that the health cost exceeds 120% of the electricity cost in some locations.","120","percent","[""han2024""]","is_blank","The total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76]. Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context provides information on the health impact of training a Llama-3.1 scale model in different locations. Specifically, it mentions that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For Iowa, the total health cost is $2.5 million, and for Oregon, it is $0.23 million. However, the exact percentage for Altoona, Iowa is not directly provided, but it is mentioned that the health cost exceeds 120% of the electricity cost in some locations."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context snippet from shen2024 provides information about the pre-training of the JetMoE-8B model, stating that it was trained on 1.25T tokens from mixed open-source datasets.","1.25","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context snippet from shen2024 provides information about the pre-training of the JetMoE-8B model, stating that it was trained on 1.25T tokens from mixed open-source datasets."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects and total MW capacity by project location. The United Kingdom is listed with 36 projects.","36","projects","[""amazon2023""]","is_blank","Amazon Renewable Energy Projects* Projects announced as of January 2024. Project Location: United Kingdom, Number of Projects: 36, Total MW Capacity: 901","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects and total MW capacity by project location. The United Kingdom is listed with 36 projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides various statistics about Amazon's workforce, including gender distribution. For 2023, it states that 31.6% of Amazon's U.S. workforce across all levels identified as men, which implies that 68.3% identified as women. However, the question specifically asks for the percentage that identified as men.","31.6","percent","[""amazon2023""]","is_blank","Amazon Representation by the Numbers* † Other Native American and NHOPI+ (2021 and 2022) Latino/e+ Black+ Asian+ White+ Multiracial Native American and Alaskan+ (2023) NHOPI+ (2023) Amazon Workforce (All Levels) 43.1%56.8% 44.1%55.7% 30.8%69.2% 31.4%68.5% 31.5%68.4% 46.9%52.9% 45.5%54.3% 46.6%53.2% 44.3%55.6% 31.1%68.8% 48 31.7%68.3% 31.6%68.3%","The context provides various statistics about Amazon's workforce, including gender distribution. For 2023, it states that 31.6% of Amazon's U.S. workforce across all levels identified as men, which implies that 68.3% identified as women. However, the question specifically asks for the percentage that identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context snippet from li2025b mentions that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context snippet from li2025b mentions that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The total execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84 is not directly provided in the context. However, based on the execution time breakdown provided in Fig. 4, we can infer that the MoE layer is the most time-consuming part of the model. The exact total execution time is not specified, but we can see from Fig. 4 that for a batch size of 84, the execution time breakdown for sparse BlackMamba is approximately 2 seconds.","2.0","seconds","[""xia2024""]","[""https://ref_id=xia2024""]","Fig. 4. Execution time breakdown.","The total execution time of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84 is not directly provided in the context. However, based on the execution time breakdown provided in Fig. 4, we can infer that the MoE layer is the most time-consuming part of the model. The exact total execution time is not specified, but we can see from Fig. 4 that for a batch size of 84, the execution time breakdown for sparse BlackMamba is approximately 2 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity' metric to assess the ratio of computation to communication time when scaling distributed training across continents. This metric helps in predicting performance with different hardware setups and is crucial for determining the suitability of models for distributed training.","granularity","is_blank","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","The ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.","The context introduces the 'granularity' metric to assess the ratio of computation to communication time when scaling distributed training across continents. This metric helps in predicting performance with different hardware setups and is crucial for determining the suitability of models for distributed training."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context snippet from luccioni2025a, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil production by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","According to the context snippet from luccioni2025a, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil production by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context provides a clear definition of water withdrawal, which refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation). This matches the expected format for the answer.","Water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context provides a clear definition of water withdrawal, which refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation). This matches the expected format for the answer."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context provides information about the average number of connected devices per U.S. household. According to the text, every U.S. household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b"", ""amazon2023""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf"", ""https://www.amazon.com/sustainability/report/2023""]","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context provides information about the average number of connected devices per U.S. household. According to the text, every U.S. household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides information on the energy consumption required to pre-train large language models (LLMs). Specifically, it mentions that the energy required to pre-train an LLM spans from as little as 0.8 MWh to 3,500 MWh.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides information on the energy consumption required to pre-train large language models (LLMs). Specifically, it mentions that the energy required to pre-train an LLM spans from as little as 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This is mentioned in multiple references, including [ref_id=stone2022]. Therefore, the statement that IBM's Watson program did NOT beat human contenders in the Jeopardy challenge is false.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This is mentioned in multiple references, including [ref_id=stone2022]. Therefore, the statement that IBM's Watson program did NOT beat human contenders in the Jeopardy challenge is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from [ref_id=luccioni2025a] mentions that one paper suggests 10–50 queries on GPT-3 consumes around half a liter of water.","[10, 50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context snippet from [ref_id=luccioni2025a] mentions that one paper suggests 10–50 queries on GPT-3 consumes around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The MT-Bench score comparison table and text in the context provide the necessary information to answer the question. The JetMoE-8B-chat model achieved a higher MT-Bench score than Llama-2-13b-chat after alignment.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models, JetMoE-8B-chat achieves a higher MT-Bench score than Llama-2-13b-chat after alignment, demonstrating its superior performance.","The MT-Bench score comparison table and text in the context provide the necessary information to answer the question. The JetMoE-8B-chat model achieved a higher MT-Bench score than Llama-2-13b-chat after alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","","","responses","[]","is_blank","is_blank",""
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange reduces deployment costs by up to 77% in conversational settings. This information is found in multiple references throughout the provided context snippets.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The 2024 Griggs et al. paper reports that Mélange reduces deployment costs by up to 77% in conversational settings. This information is found in multiple references throughout the provided context snippets."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context clearly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context clearly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context provides multiple references to the environmental impact of training an AI model of the Llama-3.1 scale. Specifically, it mentions that training such a model can produce air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City.","10000","round trips","[""han2024"", ""rubei2025""]","[""https://shrinkthatfootprint.com/carbon-footprint-of-training-gpt-3-and-large-language-models/""]","Moreover, depending on the location, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City (LA-NYC), resulting in a health cost that even exceeds 120% of the training electricity cost.","The context provides multiple references to the environmental impact of training an AI model of the Llama-3.1 scale. Specifically, it mentions that training such a model can produce air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context snippet from patterson2021, NVIDIA estimated that 80–90% of the ML workload is inference processing.","[80, 90]","percent","[""patterson2021""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","According to the context snippet from patterson2021, NVIDIA estimated that 80–90% of the ML workload is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total amount of CO2 equivalent emissions generated throughout the 'Power Hungry Processing' (2024) study is directly provided in the context.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The total amount of CO2 equivalent emissions generated throughout the 'Power Hungry Processing' (2024) study is directly provided in the context."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method. It is mentioned that estimations using TDP are nearly always an overestimation and can lead to a worst-case overestimation of energy consumption by a factor of 4.1.","0","is_blank","[""chung2025"", ""cottier2024"", ""luccioni2023""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method. It is mentioned that estimations using TDP are nearly always an overestimation and can lead to a worst-case overestimation of energy consumption by a factor of 4.1."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","GPT-4o mini consumes more energy per query than GPT-4o, contrary to the statement. The context provides specific energy consumption values for GPT-4o and GPT-4o mini.","0","is_blank","[""jegham2025""]","is_blank","GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","GPT-4o mini consumes more energy per query than GPT-4o, contrary to the statement. The context provides specific energy consumption values for GPT-4o and GPT-4o mini."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a figure showing the query throughput of Mixtral and BlackMamba. For Mixtral-CS with a batch size of 1, the dense model has a throughput of 0.5 queries/sec.","0.5","queries/sec","[""xia2024""]","is_blank","['Fig. 8. Query throughput of Mixtral and BlackMamba.']","The context provides a figure showing the query throughput of Mixtral and BlackMamba. For Mixtral-CS with a batch size of 1, the dense model has a throughput of 0.5 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context indicates that the direct release of environmental information peaked in 2022 with 10% of notable models releasing some degree of information. However, it also states that 'the introduction of increasingly commercial and proprietary models after 2022... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.' By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category. This suggests that the trend of AI developers directly disclosing environmental information did not continue to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category, as the line between research and commercial deployment became increasingly blurred.","The context indicates that the direct release of environmental information peaked in 2022 with 10% of notable models releasing some degree of information. However, it also states that 'the introduction of increasingly commercial and proprietary models after 2022... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.' By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category. This suggests that the trend of AI developers directly disclosing environmental information did not continue to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The normalized on-demand hourly price for an H100 GPU was calculated based on RunPod's pricing and adjusted to match the pricing structures of major cloud providers.","7.516","USD per hour","[""griggs2024""]","is_blank","The normalized price of $7.516 for H100 was derived by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.","The normalized on-demand hourly price for an H100 GPU was calculated based on RunPod's pricing and adjusted to match the pricing structures of major cloud providers."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides an estimate of water usage for GPT-3 training, which is directly applicable to the question about GPT-4 training run.","700000","liters of water","[""jegham2025""]","is_blank","Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.","The context provides an estimate of water usage for GPT-3 training, which is directly applicable to the question about GPT-4 training run."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the monthly on-demand rental cost of serving Llama2-70b at BF16 precision using 2 NVIDIA A100-80GB GPUs, which is over $5,200. To estimate the hourly cost, we divide this monthly cost by 30 days (assuming a 30-day month).","7.21","USD per hour","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context provides the monthly on-demand rental cost of serving Llama2-70b at BF16 precision using 2 NVIDIA A100-80GB GPUs, which is over $5,200. To estimate the hourly cost, we divide this monthly cost by 30 days (assuming a 30-day month)."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context snippet from wu2021a provides information on the model size increase required to achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task. It states that to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context snippet from wu2021a provides information on the model size increase required to achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task. It states that to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training a Transformer model with neural architecture search. By using the emissions-to-driving-distance ratio implied by the lifetime emissions of a car, we can estimate the equivalent driving distance.","507079","miles","[""luccioni2023"", ""strubell2019""]","is_blank","Table 1 from Strubell et al. (2019) and related text.","The context provides the CO2 emissions for training a Transformer model with neural architecture search. By using the emissions-to-driving-distance ratio implied by the lifetime emissions of a car, we can estimate the equivalent driving distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The inference energy for the BLOOMz-7B model is reported as 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the number of downloads (606,096) by 1 million inferences and the inference energy per inference. The calculation is: 606,096 * 1,000,000 * 1.0 × 10−4 kWh = 60,609.6 kWh. To convert kWh to MWh, we divide by 1,000, resulting in 60.6096 MWh.","60.61","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B inference energy: 1.0 × 10−4 kWh (Table 5 in [luccioni2024])","The inference energy for the BLOOMz-7B model is reported as 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the number of downloads (606,096) by 1 million inferences and the inference energy per inference. The calculation is: 606,096 * 1,000,000 * 1.0 × 10−4 kWh = 60,609.6 kWh. To convert kWh to MWh, we divide by 1,000, resulting in 60.6096 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context snippet from shen2024 provides information about the training of the JetMoE-8B model, specifically mentioning that it was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context snippet from shen2024 provides information about the training of the JetMoE-8B model, specifically mentioning that it was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context provided indicates that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This information is used to answer the question about the cost by 2027.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context provided indicates that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This information is used to answer the question about the cost by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The total number of floating point operations to train GPT-3, as published by OpenAI, is 3.14E+23 FLOPS. This information is directly mentioned in the context snippet from [ref_id=patterson2021].","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The total number of floating point operations to train GPT-3, as published by OpenAI, is 3.14E+23 FLOPS. This information is directly mentioned in the context snippet from [ref_id=patterson2021]."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running LLaMA-65B inference. It specifies that at least 8 V100 GPUs each with 32 GB of RAM are required.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context provides information on the bare minimum hardware requirements for running LLaMA-65B inference. It specifies that at least 8 V100 GPUs each with 32 GB of RAM are required."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The total computational work performed during the final 101B training stage can be estimated using the provided throughput data. The single-GPU throughput for the 101B stage consistently exceeds 160 teraFLOPs/sec. Assuming an average throughput of 165 teraFLOPs/sec (as listed in Table 2), and considering that the 101B model was trained for 21.54 days (or 1859.616 hours, since 24 * 21.54 = 516.96 and 516.96 * 3.6 = 1859.616), we can estimate the total FLOPs performed.","52.76","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size. Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 101 4 4 12 192 2160 165 52.88%","The total computational work performed during the final 101B training stage can be estimated using the provided throughput data. The single-GPU throughput for the 101B stage consistently exceeds 160 teraFLOPs/sec. Assuming an average throughput of 165 teraFLOPs/sec (as listed in Table 2), and considering that the 101B model was trained for 21.54 days (or 1859.616 hours, since 24 * 21.54 = 516.96 and 516.96 * 3.6 = 1859.616), we can estimate the total FLOPs performed."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide a clear top-1 accuracy for AlexNet on ImageNet. While AlexNet is mentioned multiple times in the context, the specific accuracy value is not provided.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide a clear top-1 accuracy for AlexNet on ImageNet. While AlexNet is mentioned multiple times in the context, the specific accuracy value is not provided."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context snippet [ref_id=li2025a] states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. This implies that each server has 8 A800 GPUs, and there are 24 servers in total.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context snippet [ref_id=li2025a] states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. This implies that each server has 8 A800 GPUs, and there are 24 servers in total."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The estimated CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) is 626,155 lbs. This is equivalent to about 5 average American car lifetimes.","5","lifetimes","[""strubell2019""]","is_blank","Table 1: Estimated CO 2 emissions from training com- mon NLP models, compared to familiar consumption.","The estimated CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) is 626,155 lbs. This is equivalent to about 5 average American car lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context directly provides the number of GPU hours required to train FAIR's RoBERTa on 160GB of text.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context directly provides the number of GPU hours required to train FAIR's RoBERTa on 160GB of text."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context provides information about Amazon's on-site solar energy systems, stating that altogether these projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","47500","metric tons","[""amazon2023""]","is_blank","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context provides information about Amazon's on-site solar energy systems, stating that altogether these projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions that large language models were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This library allows for the distribution of model weights across multiple GPUs, enabling parallel computation of activations.","vllm","is_blank","[""zschache2025"", ""samsi2024""]","[""https://github.com/vllm-project/vllm"", ""https://www.ray.io/""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context mentions that large language models were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This library allows for the distribution of model weights across multiple GPUs, enabling parallel computation of activations."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides a table (Table B2) that shows the energy consumption ratio for single-node vs. double-node deployment for various models, including Llama 3.1 70B. For Llama 3.1 70B, the energy consumption ratio is 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2, zschache2025","The context provides a table (Table B2) that shows the energy consumption ratio for single-node vs. double-node deployment for various models, including Llama 3.1 70B. For Llama 3.1 70B, the energy consumption ratio is 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","","","gallons of water","[]","is_blank","is_blank",""
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Therefore, 700 million daily GPT-4o queries would result in annual electricity use comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Therefore, 700 million daily GPT-4o queries would result in annual electricity use comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the context snippet from [ref_id=luccioni2025a], Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons. This information directly answers the question about the percentage increase in Microsoft's global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a"", ""li2025b""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","According to the context snippet from [ref_id=luccioni2025a], Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons. This information directly answers the question about the percentage increase in Microsoft's global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is directly mentioned in the context. The PUE is a measure of datacenter efficiency, representing the ratio of total energy used by the datacenter to the energy used by its computing equipment.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is directly mentioned in the context. The PUE is a measure of datacenter efficiency, representing the ratio of total energy used by the datacenter to the energy used by its computing equipment."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides an estimate of approximately 772 billion GPT-4o queries in 2025. This figure is mentioned multiple times in the document and directly answers the question.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context provides an estimate of approximately 772 billion GPT-4o queries in 2025. This figure is mentioned multiple times in the document and directly answers the question."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is directly provided in the context. The context shows a table with benchmark results for various models, including JetMoE-8B. According to the table, JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. LLaMA2 DeepseekMoE Gemma JetMoE # Total Params 7B 16B 2B 8B # Activate Params 7B 2.8B 2B 2.2B # Training tokens 2T 2T 2T 1.25T ARC-challenge 53.1 53.2 48.4 48.7 Hellaswag 78.6 79.8 71.8 80.5 MMLU 46.9 46.3 41.8 49.2 TruthfulQA 38.8 36.1 33.1 41.7 WinoGrande 74.0 73.7 66.3 70.2 GSM8k 14.5 17.3 16.9 27.8 OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0 MBPP (Pass@1) 20.8 34.0 28.0 34.2 HumanEval (Pass@1) 12.8 25.0 24.4 14.6 All Avg. 45.5 47.3 43.2 47.6","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is directly provided in the context. The context shows a table with benchmark results for various models, including JetMoE-8B. According to the table, JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide a direct answer to the number of AI training runs conducted globally on renewable-only power in 2022. However, it mentions that less than a quarter of the models (34) used low-carbon energy sources like hydroelectricity and nuclear energy, and half of the models in the sample were trained in the United States (48), followed by China (18).","is_blank","training runs","[""luccioni2023""]","is_blank","Table 2. Main Energy Sources for the models analyzed and their carbon intensities [24, 52]","The context does not provide a direct answer to the number of AI training runs conducted globally on renewable-only power in 2022. However, it mentions that less than a quarter of the models (34) used low-carbon energy sources like hydroelectricity and nuclear energy, and half of the models in the sample were trained in the United States (48), followed by China (18)."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The AI Act largely excludes open-source GPAI models from transparency requirements unless they present a systemic risk. However, the Act does not mandate the disclosure of energy consumption for open-source models, which results in a lack of visibility concerning their environmental impact and energy usage.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).","The AI Act largely excludes open-source GPAI models from transparency requirements unless they present a systemic risk. However, the Act does not mandate the disclosure of energy consumption for open-source models, which results in a lack of visibility concerning their environmental impact and energy usage."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The US national datacenter average Power Usage Effectiveness (PUE) in 2020 was 1.59. This information is derived from the context provided, which cites various sources related to data center efficiency and environmental impacts.","1.59","PUE","[""patterson2021"", ""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/ Accessed: 2025-03-18.""]","The US national datacenter average in 2018 was 1.58, which is the value [Str19] used ; In 2020, it was 1.59.","The US national datacenter average Power Usage Effectiveness (PUE) in 2020 was 1.59. This information is derived from the context provided, which cites various sources related to data center efficiency and environmental impacts."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides an analysis of KV Cache size growth for different models in response to increasing batch sizes. Specifically, for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache size grows significantly with the batch size. When the batch size increases to 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","For example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context provides an analysis of KV Cache size growth for different models in response to increasing batch sizes. Specifically, for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache size grows significantly with the batch size. When the batch size increases to 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provides information on the cost breakdown for four notable models studied in-depth by Cottier et al. (2024), which are GPT-3, OPT-175B, GPT-4, and Gemini Ultra. It states that R&D staff costs, including equity, make up between 29% and 49% of the total amortized model development costs, depending on the model.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context provides information on the cost breakdown for four notable models studied in-depth by Cottier et al. (2024), which are GPT-3, OPT-175B, GPT-4, and Gemini Ultra. It states that R&D staff costs, including equity, make up between 29% and 49% of the total amortized model development costs, depending on the model."
"q213","Which software package was used to measure energy consumption during inference runs?","The context clearly mentions that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context clearly mentions that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides an analysis of 100 news articles discussing ChatGPT energy consumption. According to the analysis, 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context provides an analysis of 100 news articles discussing ChatGPT energy consumption. According to the analysis, 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The Compute Time Calibration Function (CTCF) is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The context clearly mentions CTCF as the function that addresses this issue. Therefore, the answer is directly obtained from the context.","CTCF","is_blank","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","The Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The Compute Time Calibration Function (CTCF) is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The context clearly mentions CTCF as the function that addresses this issue. Therefore, the answer is directly obtained from the context."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context provided indicates that increasing the number of shards tends to increase the energy costs of inference per response overall for LLaMA 65B. This is mentioned in Section D, Energy per Response: LLaMA 65B, where it is stated that 'increasing the number of shards still tends to increase the energy costs of inference per response most overall'.","1","is_blank","[""samsi2024""]","is_blank","Section D, Energy per Response: LLaMA 65B","The context provided indicates that increasing the number of shards tends to increase the energy costs of inference per response overall for LLaMA 65B. This is mentioned in Section D, Energy per Response: LLaMA 65B, where it is stated that 'increasing the number of shards still tends to increase the energy costs of inference per response most overall'."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides information on the environmental impact of mining rare earth materials used during manufacturing. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq. Additionally, it mentions that an H100 is 0.1% rare earth metal by mass, and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. We can calculate the water consumption for a single H100 GPU.","0.0022","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context provides information on the environmental impact of mining rare earth materials used during manufacturing. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq. Additionally, it mentions that an H100 is 0.1% rare earth metal by mass, and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. We can calculate the water consumption for a single H100 GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses the need for transparency and reporting of AI's environmental impact, including energy consumption. It mentions that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. However, it does not explicitly state that under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","0","is_blank","[""ebert2024"", ""luccioni2025c""]","is_blank","['The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.']","The context discusses the need for transparency and reporting of AI's environmental impact, including energy consumption. It mentions that open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. However, it does not explicitly state that under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","","","percent","[]","is_blank","is_blank",""
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The total public health cost of U.S. data centers in 2023 was about $6.7 billion. This information is directly provided in the context snippet from the document han2024.","6.7","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers' total electricity cost.","The total public health cost of U.S. data centers in 2023 was about $6.7 billion. This information is directly provided in the context snippet from the document han2024."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh. The energy consumption of the o3 model for a long prompt is 12.222 Wh. To find the factor by which the energy consumption of o3 is greater than GPT-4.1 nano, we divide the energy consumption of o3 by GPT-4.1 nano: 12.222 Wh / 0.827 Wh ≈ 14.78. However, the context does not provide a direct comparison but mentions that DeepSeek-R1 (DS) consumes 29.075 Wh, which is around sixty-five times more than the most efficient model (LLaMA-3.1-8B).","14.78","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). GPT-4.1 nano: 0.827 ± 0.094 Wh for long prompts, o3: 12.222 ± 1.082 Wh for long prompts.","The energy consumption of GPT-4.1 nano for a long prompt is 0.827 Wh. The energy consumption of the o3 model for a long prompt is 12.222 Wh. To find the factor by which the energy consumption of o3 is greater than GPT-4.1 nano, we divide the energy consumption of o3 by GPT-4.1 nano: 12.222 Wh / 0.827 Wh ≈ 14.78. However, the context does not provide a direct comparison but mentions that DeepSeek-R1 (DS) consumes 29.075 Wh, which is around sixty-five times more than the most efficient model (LLaMA-3.1-8B)."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context snippet from griggs2024 provides information about Mélange's cost savings for short-context workloads (Arena dataset) with a 120ms SLO. It states that Mélange achieves 15-77% cost reduction compared to single-GPU-type baselines.","[15, 77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context snippet from griggs2024 provides information about Mélange's cost savings for short-context workloads (Arena dataset) with a 120ms SLO. It states that Mélange achieves 15-77% cost reduction compared to single-GPU-type baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The total estimated net carbon emissions for the pre-training of FLM-101B were found in Table 3 of the document. The value is provided directly in the table.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.","The total estimated net carbon emissions for the pre-training of FLM-101B were found in Table 3 of the document. The value is provided directly in the table."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides a detailed execution time breakdown for Mixtral with sparse fine-tuning and a batch size of 1. The total execution time is 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","The context provides a detailed execution time breakdown for Mixtral with sparse fine-tuning and a batch size of 1. The total execution time is 2.0 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200, indicating a significant disparity. Therefore, the public health costs of AI are not evenly distributed across communities in the U.S.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200, indicating a significant disparity. Therefore, the public health costs of AI are not evenly distributed across communities in the U.S."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet from wu2021b provides information about GPU theoretical performance per watt doubling approximately every 3-4 years as a result of Moore's law scaling and architectural optimization. This is supported by a reference to Sun et al., 2019.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context snippet from wu2021b provides information about GPU theoretical performance per watt doubling approximately every 3-4 years as a result of Moore's law scaling and architectural optimization. This is supported by a reference to Sun et al., 2019."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context specifically mentions using Ollama, an open-source platform, to apply 4-bit quantization for local deployment of large language models in the financial sentiment case study. This is detailed in the section on Local Inference Optimization.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context specifically mentions using Ollama, an open-source platform, to apply 4-bit quantization for local deployment of large language models in the financial sentiment case study. This is detailed in the section on Local Inference Optimization."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that the datasets were accessed on-demand via shards in thetar format with the WebDataset library. However, for a storage service used to shard and stream datasets for spot VMs that could terminate at any time, it specifically mentions Backblaze (B2) as the independent S3 storage provider used.","Backblaze (B2)","is_blank","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","When we run our experiments in a multi-cloud environment on spot instances, we cannot plug in proprietary cloud storage or wait for the dataset to download, as the instances can be terminated any-time. To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent. Additionally, the cost is very manageable at $0.01/GB rate for egress and $0.005/GB/month for storage.","The context mentions that the datasets were accessed on-demand via shards in thetar format with the WebDataset library. However, for a storage service used to shard and stream datasets for spot VMs that could terminate at any time, it specifically mentions Backblaze (B2) as the independent S3 storage provider used."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates a strong correlation between inference energy consumption and model runtime. This suggests that the relationship between runtime and energy consumption is nearly linear, as a strong correlation implies that one variable can serve as a practical proxy for the other.","is_blank","is_blank","[""zschache2025"", ""fernandez2025""]","is_blank","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context indicates a strong correlation between inference energy consumption and model runtime. This suggests that the relationship between runtime and energy consumption is nearly linear, as a strong correlation implies that one variable can serve as a practical proxy for the other."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context snippet mentions that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","The bill primarily mandates studies, stakeholder consultations, and voluntary reporting on AI's environmental impacts without imposing significant regulatory obligations.","The context snippet mentions that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context snippet from Chen et al. (2024) provides a table with specifications of H100, H20, and TPU v6e. The price per chip for H100 is listed as $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","['Table 1: H100, H20, and TPU v6e specifications.']","The context snippet from Chen et al. (2024) provides a table with specifications of H100, H20, and TPU v6e. The price per chip for H100 is listed as $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context mentions that GPUs can theoretically last about five years, but the push for higher performance prompts more frequent upgrades. However, a specific estimated average GPU lifetime before retirement in AI data centers in 2024 is provided in another part of the context.","4","years","[""morrison2025""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","The context mentions that GPUs can theoretically last about five years, but the push for higher performance prompts more frequent upgrades. However, a specific estimated average GPU lifetime before retirement in AI data centers in 2024 is provided in another part of the context."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA-13B, it is mentioned that 2 V100 GPUs each with 32 GB of RAM are required.","2","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context provides information on the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA-13B, it is mentioned that 2 V100 GPUs each with 32 GB of RAM are required."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The reported GHG emissions from the pre-training process for Google's Gemma family of language models are 1247.61 tons CO2e. This is compared to the 'five cars' estimate, which is stated to be around 284 metric tons CO2e (or 626,155 pounds). Therefore, the Gemma family's emissions are over 4 times the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the 'five cars' number.","The reported GHG emissions from the pre-training process for Google's Gemma family of language models are 1247.61 tons CO2e. This is compared to the 'five cars' estimate, which is stated to be around 284 metric tons CO2e (or 626,155 pounds). Therefore, the Gemma family's emissions are over 4 times the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context provides information about the training time of ELMo on 3 NVIDIA GTX 1080 GPUs. According to Peters et al. (2018), ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours.","336","hours","[""strubell2019"", ""luccioni2023""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context provides information about the training time of ELMo on 3 NVIDIA GTX 1080 GPUs. According to Peters et al. (2018), ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The estimated U.S. national average water consumption for electricity generation is provided in the context as 3.1 L/kWh. This value is mentioned in multiple references, including [ref_id=li2025b] and [ref_id=morrison2025].","3.1","L/kWh","[""li2025b"", ""morrison2025""]","is_blank","The U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The estimated U.S. national average water consumption for electricity generation is provided in the context as 3.1 L/kWh. This value is mentioned in multiple references, including [ref_id=li2025b] and [ref_id=morrison2025]."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The reported PUE (Power Usage Effectiveness) of Google's hyperscale data centers in 2021 was found in the context snippet from wu2021b and dodge2022. According to these sources, Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.","1.1","PUE","[""wu2021b"", ""dodge2022"", ""patterson2021""]","is_blank","Google’s datacenter PUE has improved from 1.21 (2008) to 1.10 (2021) [Google, a]. Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.","The reported PUE (Power Usage Effectiveness) of Google's hyperscale data centers in 2021 was found in the context snippet from wu2021b and dodge2022. According to these sources, Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the provided context snippets, specifically from the Amazon 2023 Sustainability Report, AWS states that in North America, it can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the provided context snippets, specifically from the Amazon 2023 Sustainability Report, AWS states that in North America, it can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides a specific cost estimate for fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU. The cost is directly mentioned as $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context provides a specific cost estimate for fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU. The cost is directly mentioned as $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context provides information on the electricity consumption of various hardware components in a datacenter. According to the provided table in [ref_id=dodge2022], when training BERT base on a single NVIDIA TITAN X GPU, the GPU alone accounts for 74% of the total energy consumption due to these components.","74","percent","[""dodge2022""]","is_blank","Table 1 in [ref_id=dodge2022]: Hardwa. GPU CPU0 CPU1 DRAM0 DRAM1 Total Watts 187.1 22.9 9.3 23.0 9.3 251.6 Fraction 74% 9% 4% 9% 4% 100%","The context provides information on the electricity consumption of various hardware components in a datacenter. According to the provided table in [ref_id=dodge2022], when training BERT base on a single NVIDIA TITAN X GPU, the GPU alone accounts for 74% of the total energy consumption due to these components."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context snippet from shen2024 mentions that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s. This directly provides the total number of H100 GPUs used for training.","96","H100 GPUs","[""shen2024""]","is_blank","We con-duct training on a cluster containing 12 nodes and 96 H100s.","The context snippet from shen2024 mentions that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s. This directly provides the total number of H100 GPUs used for training."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context provides information about the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training. It states that when actively training, the average GPU power is over 600W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context provides information about the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training. It states that when actively training, the average GPU power is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","","","lbs","[]","is_blank","is_blank",""
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides a comparison of inference performance between NVIDIA A100 and V100 GPUs for LLaMA-13B. It states that there is a 1.25 times increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context provides a comparison of inference performance between NVIDIA A100 and V100 GPUs for LLaMA-13B. It states that there is a 1.25 times increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The energy consumption of a single short query to GPT-4o is directly stated in the context. According to the text, a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The energy consumption of a single short query to GPT-4o is directly stated in the context. According to the text, a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context provides a comparison of instance selection results by SLO constraints for online inference workloads. In the case of an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. To find the percentage by which Max-Performance's choice was more expensive, we calculate the difference in cost and divide by InferSave's cost, then multiply by 100.","280","percent","[""kim2025""]","is_blank","TABLE V and text in the kim2025 document: 'On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.'","The context provides a comparison of instance selection results by SLO constraints for online inference workloads. In the case of an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. To find the percentage by which Max-Performance's choice was more expensive, we calculate the difference in cost and divide by InferSave's cost, then multiply by 100."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context snippet from zschache2025 provides information about the efficiency of different GPU architectures for models generating a single token per inference.","A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context snippet from zschache2025 provides information about the efficiency of different GPU architectures for models generating a single token per inference."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context mentions that Green AI involves reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. This is a key Green AI practice.","1","is_blank","[""schwartz2019"", ""schwartz2021""]","[""https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext""]","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing transparency, price tags are baselines that other researchers could improve on.","The context mentions that Green AI involves reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. This is a key Green AI practice."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context snippet from luccioni2025a states that 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.' This directly provides the total amount of electronic waste generated worldwide in 2022.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context snippet from luccioni2025a states that 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.' This directly provides the total amount of electronic waste generated worldwide in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The average system power per processor for TPU v2 is 221 Watts, and for V100 GPU, it is 325 Watts. The difference is calculated by subtracting the power of TPU v2 from that of V100 GPU.","104","Watts","[""patterson2021""]","is_blank","Table 3. Average system power per processor and standard deviation for DNNs in this paper.","The average system power per processor for TPU v2 is 221 Watts, and for V100 GPU, it is 325 Watts. The difference is calculated by subtracting the power of TPU v2 from that of V100 GPU."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context snippet from document [ref_id=li2025b] provides information on the water consumption of AI models, including the GPT-3 language model. It states that training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context snippet from document [ref_id=li2025b] provides information on the water consumption of AI models, including the GPT-3 language model. It states that training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context clearly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This information is consistently mentioned across multiple references.","20","multiplier","[""wu2021a"", ""wu2021a"", ""wu2021a""]","is_blank","Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].","The context clearly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This information is consistently mentioned across multiple references."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context provides a detailed analysis of the eco-efficiency of various large language models using Data Envelopment Analysis (DEA). The model that ranked highest in this analysis is o3-mini, which achieved the highest cross-efficiency score of 0.884.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context provides a detailed analysis of the eco-efficiency of various large language models using Data Envelopment Analysis (DEA). The model that ranked highest in this analysis is o3-mini, which achieved the highest cross-efficiency score of 0.884."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context snippet from wu2021b states that smartphones currently have lifetimes of less than 3 years, contributing to e-waste concerns. This information directly supports the answer.","1","is_blank","[""wu2021b""]","[""https://www.fairphone.com/en/""]","less than 3 years for cell phones [Cordella et al., 2020]","The context snippet from wu2021b states that smartphones currently have lifetimes of less than 3 years, contributing to e-waste concerns. This information directly supports the answer."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context provides information on the per-GPU speedup for CV models using T4 GPUs. It mentions that CV's per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup #GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context provides information on the per-GPU speedup for CV models using T4 GPUs. It mentions that CV's per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The FLM-101B model is structured with a context window of 2,048 tokens. This information is directly provided in the context snippet.","2048","tokens","[""li2025a""]","is_blank","FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The FLM-101B model is structured with a context window of 2,048 tokens. This information is directly provided in the context snippet."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context supports the statement that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth. This is stated in multiple references, including chung2025 and fernandez2025, which explain that LLM decoding has low compute-intensity and is bottlenecked by VRAM bandwidth, leading to lower power draw compared to diffusion models.","1","is_blank","[""chung2025"", ""fernandez2025""]","is_blank","Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources... Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context supports the statement that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth. This is stated in multiple references, including chung2025 and fernandez2025, which explain that LLM decoding has low compute-intensity and is bottlenecked by VRAM bandwidth, leading to lower power draw compared to diffusion models."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides a table with gender distribution data for Amazon's workforce, including people managers. For 2023, the global gender distribution for people managers is 46.9% men and 52.9% women.","52.9","percent","[""amazon2023""]","is_blank","Table: Amazon Representation by the Numbers* † Other Native American and NHOPI+ (2021 and 2022) Latino/e+ Black+ Asian+ White+ Multiracial Native American and Alaskan+ (2023) NHOPI+ (2023) Amazon Workforce (All Levels) 43.1%56.8% 44.1%55.7% 30.8%69.2% 31.4%68.5% 31.5%68.4% 46.9%52.9% 45.5%54.3% 46.6%53.2% 44.3%55.6% 31.1%68.8% 48 31.7%68.3% 31.6%68.3%","The context provides a table with gender distribution data for Amazon's workforce, including people managers. For 2023, the global gender distribution for people managers is 46.9% men and 52.9% women."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides a breakdown of the costs for four key models analyzed by Cottier et al. (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). When excluding equity, the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","[61, 76]","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2–6%. However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context provides a breakdown of the costs for four key models analyzed by Cottier et al. (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). When excluding equity, the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context clearly states that 'metrics like accuracy and F1 score are slightly lower after optimization', indicating that not all models experienced improvements in accuracy and F1 scores after optimization. This directly contradicts the statement that accuracy and F1 scores always improved after optimization.","0","is_blank","[""khan2025""]","is_blank","Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.","The context clearly states that 'metrics like accuracy and F1 score are slightly lower after optimization', indicating that not all models experienced improvements in accuracy and F1 scores after optimization. This directly contradicts the statement that accuracy and F1 scores always improved after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour (kWh) for power consumed in the U.S. According to the context, this value is 0.954 pounds per kilowatt-hour (lbs/kWh) or 0.423 kg per kWh.","0.954","lbs/kWh","[""strubell2019"", ""patterson2021""]","is_blank","CO2e = 0 .954pt (2)","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour (kWh) for power consumed in the U.S. According to the context, this value is 0.954 pounds per kilowatt-hour (lbs/kWh) or 0.423 kg per kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a study's projection for the global AI's electricity consumption in 2027. The study suggests that the global AI could consume between 85 and 134 TWh of electricity in 2027.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment.","The context provides a study's projection for the global AI's electricity consumption in 2027. The study suggests that the global AI could consume between 85 and 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context provides information on the number of packages delivered via EVs in Europe in 2023. According to the text, Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","Europe • We delivered 150 million packages via EVs.","The context provides information on the number of packages delivered via EVs in Europe in 2023. According to the text, Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluation involved a pattern of 128 input tokens and a 512 output tokens, simulating a real-time chatbot system. The workload evaluated a total of 3000 requests.","2040000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The online inference workload evaluation involved a pattern of 128 input tokens and a 512 output tokens, simulating a real-time chatbot system. The workload evaluated a total of 3000 requests."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context does not clearly support that the AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration. In fact, it is mentioned that the AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications.","0","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context does not clearly support that the AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration. In fact, it is mentioned that the AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context provides information about the 'Flexible Start' optimization analysis for a short job (DenseNet 201) in the West US region. According to the text, the maximum potential percentage reduction in CO2 emissions for this job can be up to 80% in the West US region.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region. (Fig. 3)","The context provides information about the 'Flexible Start' optimization analysis for a short job (DenseNet 201) in the West US region. According to the text, the maximum potential percentage reduction in CO2 emissions for this job can be up to 80% in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context provides a table with the mean energy consumption for various tasks. The least intensive task is text classification with 0.002 kWh per 1,000 inferences, and the most intensive task is image generation with 2.9 kWh per 1,000 inferences. The factor by which image generation exceeds text classification is over 1450 times.","1450","times","[""luccioni2024""]","is_blank","Table 2 and related text","The context provides a table with the mean energy consumption for various tasks. The least intensive task is text classification with 0.002 kWh per 1,000 inferences, and the most intensive task is image generation with 2.9 kWh per 1,000 inferences. The factor by which image generation exceeds text classification is over 1450 times."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, broken down by project location. For the United States, it lists 244 projects.","244","projects","[""amazon2023""]","is_blank","Amazon Renewable Energy Projects* Projects announced as of January 2024. Project Location, Number of Projects, Total MW Capacity†, ... United States, 244, 17,706","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, broken down by project location. For the United States, it lists 244 projects."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide a specific percentage for Amazon's AWS power usage covered with renewable energy in 2018. However, it mentions that in 2023, Amazon achieved 100% of its electricity consumed by global operations matched with renewable energy sources. Previous years' data is provided as follows: 2022 (90%), 2021 (85%), 2020 (65%), and 2019 (42%).","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide a specific percentage for Amazon's AWS power usage covered with renewable energy in 2018. However, it mentions that in 2023, Amazon achieved 100% of its electricity consumed by global operations matched with renewable energy sources. Previous years' data is provided as follows: 2022 (90%), 2021 (85%), 2020 (65%), and 2019 (42%)."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context suggests that the authors recommend measuring and reporting energy consumption at the cumulative server level. This is mentioned as 'Cumulative server energy reporting' in the policy proposals for AI and environmental impact.","cumulative server level","is_blank","[""ebert2024""]","is_blank","The context mentions 'Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.'","The context suggests that the authors recommend measuring and reporting energy consumption at the cumulative server level. This is mentioned as 'Cumulative server energy reporting' in the policy proposals for AI and environmental impact."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides a table (Table 1) that details the electricity consumption of different hardware components when training a BERT-base model on a single NVIDIA TITAN X GPU. The GPU accounts for 74% of the total electricity consumption.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context provides a table (Table 1) that details the electricity consumption of different hardware components when training a BERT-base model on a single NVIDIA TITAN X GPU. The GPU accounts for 74% of the total electricity consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context provides information about the hardware requirements for serving large language models, specifically mentioning that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs. For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context provides information about the hardware requirements for serving large language models, specifically mentioning that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context provides information on the operational power footprint reduction achieved at Facebook over a two-year period. According to the text, the net effect of iterative optimization is a 28.5% operational power footprint reduction over two years.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B). Despite the significant operational power footprint reduction, we continue to see the overall electricity demand for AI to increase over time — an example of Jevon’s Paradox, where efficiency improvement stimulates additional novel AI use cases.","The context provides information on the operational power footprint reduction achieved at Facebook over a two-year period. According to the text, the net effect of iterative optimization is a 28.5% operational power footprint reduction over two years."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context snippets do not mention the installation of fiber optic cable to support AI workloads in 2023. Therefore, the answer is not supported by the context.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention the installation of fiber optic cable to support AI workloads in 2023. Therefore, the answer is not supported by the context."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The estimated upfront hardware acquisition cost to train GPT-4 is $800M. This is mentioned in the context as the cost of acquiring the hardware used to train GPT-4, which is one to two orders of magnitude larger than the amortized hardware cost of $40M for GPT-4.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The estimated upfront hardware acquisition cost to train GPT-4 is $800M. This is mentioned in the context as the cost of acquiring the hardware used to train GPT-4, which is one to two orders of magnitude larger than the amortized hardware cost of $40M for GPT-4."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","","","is_blank","[]","is_blank","is_blank",""
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides a table and figure showing the maximum batch sizes for fine-tuning Mixtral on different NVIDIA GPUs. For the A100-40GB GPU, the ground truth maximum batch size is shown in a figure.","17","samples","[""xia2024""]","is_blank","TABLE IV and Fig. 13 in xia2024","The context provides a table and figure showing the maximum batch sizes for fine-tuning Mixtral on different NVIDIA GPUs. For the A100-40GB GPU, the ground truth maximum batch size is shown in a figure."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context clearly compares the energy consumption of Recomputation and Swapping preemption mechanisms for LLM inference servers when the server is overloaded. It states that Swapping consistently consumes less energy than Recomputation because it copies data without running computation, and the energy consumption of computation is larger than memory operations.","Swapping","is_blank","[""chung2025""]","is_blank","Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.","The context clearly compares the energy consumption of Recomputation and Swapping preemption mechanisms for LLM inference servers when the server is overloaded. It states that Swapping consistently consumes less energy than Recomputation because it copies data without running computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","","","percent","[]","is_blank","is_blank",""
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context does not provide a specific McKinsey projection for 2030. However, it mentions that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024). For 2028, projections estimate that data center electricity consumption will account for 6.7–12.0% of the national total (Lawrence Berkeley National Laboratory report).","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide a specific McKinsey projection for 2030. However, it mentions that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024). For 2028, projections estimate that data center electricity consumption will account for 6.7–12.0% of the national total (Lawrence Berkeley National Laboratory report)."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The 'Pause and Resume' optimization for the 6B parameter transformer shows a potential emissions saving of up to 25%. This is derived from Figure 4 (b) in the context, which illustrates the emissions decrease for the 6B parameters Transformer when using the Pause and Resume optimization.","25","percent","[""dodge2022""]","is_blank","Figure 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For short experiments, the doubled duration is still relatively short, and thus leads to minimal emissions reduction (see DenseNet 201 in (a)); for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The 'Pause and Resume' optimization for the 6B parameter transformer shows a potential emissions saving of up to 25%. This is derived from Figure 4 (b) in the context, which illustrates the emissions decrease for the 6B parameters Transformer when using the Pause and Resume optimization."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The JetMoE-8B architecture reduces inference computation by about 70% compared to the Llama2-7B model. This information is consistently mentioned across multiple references in the context.","70","percent","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The JetMoE-8B architecture reduces inference computation by about 70% compared to the Llama2-7B model. This information is consistently mentioned across multiple references in the context."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2e emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2e emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The estimated training energy of the full GPT-3 model is directly mentioned in the context snippets. GPT-3 was trained and deployed by OpenAI in Microsoft's data centers with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The estimated training energy of the full GPT-3 model is directly mentioned in the context snippets. GPT-3 was trained and deployed by OpenAI in Microsoft's data centers with an estimated training energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context clearly states that the MoE layer is the costliest and a prime target for optimization to enhance the performance of LLM fine-tuning. This is mentioned multiple times throughout the document, emphasizing its importance.","1","is_blank","[""xia2024""]","is_blank","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning. (2) MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.","The context clearly states that the MoE layer is the costliest and a prime target for optimization to enhance the performance of LLM fine-tuning. This is mentioned multiple times throughout the document, emphasizing its importance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (TABLE III) listing the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. For Mixtral with a dense setup on the CS dataset, the maximum batch size is 2.","2","samples","[""xia2024""]","is_blank","TABLE III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE.","The context provides a table (TABLE III) listing the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. For Mixtral with a dense setup on the CS dataset, the maximum batch size is 2."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context supports the answer by stating that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. This is found in the comparison between C-8 and A-8 experiments.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context supports the answer by stating that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. This is found in the comparison between C-8 and A-8 experiments."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context snippets do not mention the land occupied by new AI data centers globally in 2022. Therefore, the answer is not supported by the given context.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention the land occupied by new AI data centers globally in 2022. Therefore, the answer is not supported by the given context."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides information on the carbon emissions of various machine learning models, including BERT-based models. Specifically, it mentions that the bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","For instance, bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context provides information on the carbon emissions of various machine learning models, including BERT-based models. Specifically, it mentions that the bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context snippet from dodge2022 mentions that the emissions from training BERT on 8 V100s for 36 hours in different regions ranged from approximately 7k grams to 26k grams. This indicates the range of CO2 emissions between the most and least efficient regions.","[7000, 26000]","grams","[""dodge2022""]","is_blank","Fig. 1. Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for 36 hours) in 16 different regions (one region per line) at different times throughout the year.","The context snippet from dodge2022 mentions that the emissions from training BERT on 8 V100s for 36 hours in different regions ranged from approximately 7k grams to 26k grams. This indicates the range of CO2 emissions between the most and least efficient regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context indicates that the direct release of environmental information for notable models peaked in 2022 before declining. This is stated explicitly in multiple sections of the document.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context indicates that the direct release of environmental information for notable models peaked in 2022 before declining. This is stated explicitly in multiple sections of the document."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table with environmental impacts of various models, including OLMo 60M. According to the table, training an OLMo 60M model consumes 1.6 kL of water, which is equivalent to 5 days of water usage for one person in the US.","5","days","[""morrison2025""]","is_blank","Table 2: Power Usage (MWh), Carbon Emissions (tCO2eq), Equiv. to... (energy usage, 1 home, U.S.), Water Consumption (kL), Equiv. to... (water usage, 1 person, U.S.). OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table with environmental impacts of various models, including OLMo 60M. According to the table, training an OLMo 60M model consumes 1.6 kL of water, which is equivalent to 5 days of water usage for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information on Google's DeepMind AlphaFold servers' freshwater consumption in 2023. However, it mentions that one technology company's self-owned data centers alone directly withdrew 29 billion liters and consumed more than 23 billion liters of freshwater for on-site cooling in 2023.","is_blank","liters of freshwater","[""li2025b""]","is_blank","is_blank","The context does not provide specific information on Google's DeepMind AlphaFold servers' freshwater consumption in 2023. However, it mentions that one technology company's self-owned data centers alone directly withdrew 29 billion liters and consumed more than 23 billion liters of freshwater for on-site cooling in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context does not provide a direct statement about adding compute resources to accelerate the MoE layers when fine-tuning LLMs and its effect on costs. However, it mentions that optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning and suggests that adding compute resources could be a way to further reduce costs.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context does not provide a direct statement about adding compute resources to accelerate the MoE layers when fine-tuning LLMs and its effect on costs. However, it mentions that optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning and suggests that adding compute resources could be a way to further reduce costs."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model was not directly provided, but it can be inferred from the information given. The energy consumption for FLM-101B was 28.22 zettaFLOPs. However, the exact energy consumption in kWh was not directly stated, but based on the provided context, we can find it in a related table.","28.22","kWh","[""li2025a""]","is_blank","Table 3: Carbon Footprint Analysis. An important measurement of a model's environmental impact (Schwartz et al. 2020) is the carbon footprints originated from the pre-training process. We estimate carbon emission with the methods provided in (Patterson et al. 2021). We summarize the carbon footprint statistics of FLM-101B and well-known LLMs in Table 3.","The total energy consumption for training the FLM-101B model was not directly provided, but it can be inferred from the information given. The energy consumption for FLM-101B was 28.22 zettaFLOPs. However, the exact energy consumption in kWh was not directly stated, but based on the provided context, we can find it in a related table."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides projections for 2028, not 2030, but mentions that the total public health burden of U.S. data centers could reach more than $20 billion. This is the closest relevant information to answer the question.","20000000000.0","USD","[""han2024""]","is_blank","Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [40]. The overall public health costs could reach more than $20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with ∼35 million registered vehicles [41].","The context provides projections for 2028, not 2030, but mentions that the total public health burden of U.S. data centers could reach more than $20 billion. This is the closest relevant information to answer the question."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU is directly provided in the context. The cost is listed in a table within the document.","32.7","USD","[""xia2024""]","is_blank","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL","The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU is directly provided in the context. The cost is listed in a table within the document."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides information about the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. For sparse Mixtral fine-tuning on the CS dataset, the maximum batch size supported is 8.","8","samples","[""xia2024""]","is_blank","TABLE III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE .","The context provides information about the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. For sparse Mixtral fine-tuning on the CS dataset, the maximum batch size supported is 8."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","","","seconds","[]","is_blank","is_blank",""
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context supports that GPU-level power consumption monitoring is recommended for reporting overall AI energy use. This is because GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter and 74% of the total energy consumption in the experiment described.","1","is_blank","[""dodge2022"", ""chung2025""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context supports that GPU-level power consumption monitoring is recommended for reporting overall AI energy use. This is because GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter and 74% of the total energy consumption in the experiment described."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","According to the context, a 2023 article estimating the carbon footprint of the BLOOM model found that training accounted for only half of the model's overall emissions. This implies that training accounted for 50% of the model's overall emissions.","50","percent","[""luccioni2025c"", ""luccioni2023""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model's overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","According to the context, a 2023 article estimating the carbon footprint of the BLOOM model found that training accounted for only half of the model's overall emissions. This implies that training accounted for 50% of the model's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA 7B, it is stated that 1 V100 32GB GPU is required.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context provides information on the bare minimum hardware requirements for running different sizes of LLaMA models. For LLaMA 7B, it is stated that 1 V100 32GB GPU is required."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The table provides the number of requests for 500ml of water for GPT-3 in different locations. For Arizona, this value is directly provided.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint.","The table provides the number of requests for 500ml of water for GPT-3 in different locations. For Arizona, this value is directly provided."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The estimated CO2 emission for one year of average US home energy use is 8.3 metric tons. This value is mentioned in the context as the total emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","is_blank","The largest training runs (e.g., 6 billion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only trained for about 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest experiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the United States Environmental Protection Agency [46]. The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in §4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The estimated CO2 emission for one year of average US home energy use is 8.3 metric tons. This value is mentioned in the context as the total emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides a table (Table 3) comparing the performance of various models on different benchmarks, including GSM8k, which evaluates grade school math problem-solving. The JetMoE-8B model achieved a score of 27.8 on the GSM8k benchmark.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models.","The context provides a table (Table 3) comparing the performance of various models on different benchmarks, including GSM8k, which evaluates grade school math problem-solving. The JetMoE-8B model achieved a score of 27.8 on the GSM8k benchmark."
