"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet from [ref_id=wu2021b] states that the total energy consumption of the US data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in U.S. data center electricity consumption between 2010 and 2014. The answer_unit is 'percent', which matches the unit provided in the context.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context snippet from [ref_id=wu2021b] states that the total energy consumption of the US data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in U.S. data center electricity consumption between 2010 and 2014. The answer_unit is 'percent', which matches the unit provided in the context."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This information directly answers the question, and the answer_unit is 'cars', so the answer_value should be the number of cars.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This information directly answers the question, and the answer_unit is 'cars', so the answer_value should be the number of cars."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. This implies that 4 more data centers began using recycled water for cooling. The answer_unit is 'data centers', which matches the unit of the answer.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. This implies that 4 more data centers began using recycled water for cooling. The answer_unit is 'data centers', which matches the unit of the answer."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release the embodied carbon emissions of its hardware, so the authors assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU. This value is used as the estimated embodied carbon emissions in kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release the embodied carbon emissions of its hardware, so the authors assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU. This value is used as the estimated embodied carbon emissions in kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 was $40M, while the total training budget for FLM-101B was $100K. To find the factor by which the cost of GPT-4 was greater, we divide the cost of GPT-4 by the budget of FLM-101B. This results in a ratio of 400, indicating that the cost of GPT-4 was 400 times greater than the budget for FLM-101B.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. ... In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.","The estimated amortized training cost of GPT-4 was $40M, while the total training budget for FLM-101B was $100K. To find the factor by which the cost of GPT-4 was greater, we divide the cost of GPT-4 by the budget of FLM-101B. This results in a ratio of 400, indicating that the cost of GPT-4 was 400 times greater than the budget for FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides the CO2e for a single passenger round trip from San Francisco to New York. According to the snippet from [ref_id=patterson2021], Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip. The answer_unit is tCO2e, so the answer_value will be a number in that unit.","1.2","tCO2e","[""patterson2021""]","is_blank","Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.","The context provides the CO2e for a single passenger round trip from San Francisco to New York. According to the snippet from [ref_id=patterson2021], Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip. The answer_unit is tCO2e, so the answer_value will be a number in that unit."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context provides the performance score of FLM-101B on the Open LLM Leaderboard. According to the text, FLM-101B achieves a score of 43.94, which is the average performance score. The answer_unit is 'score', so the answer_value should be the numeric value of the score.","43.94","score","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. ... FLM-101B 28.22 43.94 39.76 66.23 28.30 ∗ 41.47","The context provides the performance score of FLM-101B on the Open LLM Leaderboard. According to the text, FLM-101B achieves a score of 43.94, which is the average performance score. The answer_unit is 'score', so the answer_value should be the numeric value of the score."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that there was a more than 6,750 fold improvement in processor clock speed between the Intel 4004 in 1971 and typical microprocessors in 2021. This directly answers the question, and the answer_unit 'fold' matches the unit given in the context.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that there was a more than 6,750 fold improvement in processor clock speed between the Intel 4004 in 1971 and typical microprocessors in 2021. This directly answers the question, and the answer_unit 'fold' matches the unit given in the context."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context from [ref_id=patterson2021] states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3. This directly answers the question about the training time of GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec. The answer_unit is 'days', so the answer_value will be the number of days.","14.8","days","[""patterson2021""]","is_blank","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context from [ref_id=patterson2021] states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations to train GPT-3. This directly answers the question about the training time of GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec. The answer_unit is 'days', so the answer_value will be the number of days."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The estimated GPU Power Usage can be found in Table 3 of the SGLang benchmarking results. The answer_unit is specified as kWh, so we look for the value in kWh. According to the table, the GPU Power Usage for a Llama 3.2 1B model at an 8 request/s frequency is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates.","The estimated GPU Power Usage can be found in Table 3 of the SGLang benchmarking results. The answer_unit is specified as kWh, so we look for the value in kWh. According to the table, the GPU Power Usage for a Llama 3.2 1B model at an 8 request/s frequency is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context provides the total permitted annual emission limits for data center backup generators in northern Virginia. According to the text, the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx. The answer_unit is 'tons', so the answer_value will be the numeric value of the emission limit in tons.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context provides the total permitted annual emission limits for data center backup generators in northern Virginia. According to the text, the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx. The answer_unit is 'tons', so the answer_value will be the numeric value of the emission limit in tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context mentions that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch. This indicates that the growth strategy saved 72% of the time that would have been required for training the model from scratch. The answer_unit is percent, so the answer_value should be expressed as a percentage.","72","percent","[""[ref_id=li2025a]""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context mentions that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch. This indicates that the growth strategy saved 72% of the time that would have been required for training the model from scratch. The answer_unit is percent, so the answer_value should be expressed as a percentage."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","","","deaths","[]","is_blank","is_blank",""
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) estimate the total energy consumption to train the 6.1 billion parameter model to completion based on the energy consumed during 8 days of training on 256 NVIDIA A100s. The answer is derived by multiplying the total energy consumed during the 8-day training period by the ratio of the total training time to the time it took to train 13% of the model. The result is then converted to days, as specified by the answer_unit field.","60","days","[""dodge2022""]","[""https://www.google.com/about/datacenters/efficiency/""]","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model! a full training run would take 60 days.","Dodge et al. (2022) estimate the total energy consumption to train the 6.1 billion parameter model to completion based on the energy consumed during 8 days of training on 256 NVIDIA A100s. The answer is derived by multiplying the total energy consumed during the 8-day training period by the ratio of the total training time to the time it took to train 13% of the model. The result is then converted to days, as specified by the answer_unit field."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context does not provide a clear comparison of GPU energy consumption for 1,000 queries among the models listed in the appendix of a 2025 study. Therefore, the answer is 'is_blank'.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a clear comparison of GPU energy consumption for 1,000 queries among the models listed in the appendix of a 2025 study. Therefore, the answer is 'is_blank'."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, as stated in the provided context snippets. The answer_unit is 'year', so the answer_value should be a numeric value representing the year. The context clearly supports this answer, and the ref_id and ref_url provide evidence for the claim.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, as stated in the provided context snippets. The answer_unit is 'year', so the answer_value should be a numeric value representing the year. The context clearly supports this answer, and the ref_id and ref_url provide evidence for the claim."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 is mentioned in the context as a source that provides information on e-waste collection and recycling. According to this source, about 22% of e-waste has been formally collected and recycled. The answer_unit is specified as 'percent', so the answer_value should be a numeric value representing this percentage.","22","percent","[""luccioni2025a""]","is_blank","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled","The UN's Global E-Waste Monitor 2024 is mentioned in the context as a source that provides information on e-waste collection and recycling. According to this source, about 22% of e-waste has been formally collected and recycled. The answer_unit is specified as 'percent', so the answer_value should be a numeric value representing this percentage."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The energy consumption for pre-training the BLOOM model is mentioned in the context as 0.8 MWh for the OLMo 20M model and up to 3,500 MWh for the LLaMa 4 Scout model. However, the specific energy consumption for the BLOOM model is not provided in the given context. Therefore, the answer is 'is_blank'.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The energy consumption for pre-training the BLOOM model is mentioned in the context as 0.8 MWh for the OLMo 20M model and up to 3,500 MWh for the LLaMa 4 Scout model. However, the specific energy consumption for the BLOOM model is not provided in the given context. Therefore, the answer is 'is_blank'."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer has 1500 billion parameters but only 0.1% of them are activated per token. This information is found in the table describing the CO2e for NLP models. The answer_unit is percent, so the answer_value should be expressed as a percentage.","0.1","percent","[""patterson2021""]","is_blank","Table 4. CO2e for NLP models","The context states that the Switch Transformer has 1500 billion parameters but only 0.1% of them are activated per token. This information is found in the table describing the CO2e for NLP models. The answer_unit is percent, so the answer_value should be expressed as a percentage."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B has 8 experts in each MoE layer, as shown in Table 1: JetMoE-8B hyperparameters. The answer_unit is 'experts', so the answer_value should be a number representing the count of experts.","8","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters, Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp, 8B 2B 24 2048 8 2 16 128 5632","The context states that JetMoE-8B has 8 experts in each MoE layer, as shown in Table 1: JetMoE-8B hyperparameters. The answer_unit is 'experts', so the answer_value should be a number representing the count of experts."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","","","second","[]","is_blank","is_blank",""
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides the total cost of FLM-101B in zettaFLOPs and breaks it down into costs for English and Chinese portions. The answer_unit is zettaFLOPs, so we need to find the cost for the English portion of FLM-101B in this unit. According to the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context provides the total cost of FLM-101B in zettaFLOPs and breaks it down into costs for English and Chinese portions. The answer_unit is zettaFLOPs, so we need to find the cost for the English portion of FLM-101B in this unit. According to the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context mentions the experimental setup for energy-efficient local inference in financial sentiment classification, which includes the hardware used. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","11th Gen Intel(R) Core(TM) i7-1165G7 processor","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context mentions the experimental setup for energy-efficient local inference in financial sentiment classification, which includes the hardware used. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The 'Power Hungry Processing' (2024) study analyzed a total of 80 finetuned models for ten different tasks. The answer_unit is 'models', so the answer_value should be a number representing the count of models. The context provides a table (Table 6) listing all the models tested for the ten tasks.","80","models","[""luccioni2024""]","[""https://arxiv.org/abs/2311.16863""]","Table 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.","The 'Power Hungry Processing' (2024) study analyzed a total of 80 finetuned models for ten different tasks. The answer_unit is 'models', so the answer_value should be a number representing the count of models. The context provides a table (Table 6) listing all the models tested for the ten tasks."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. This information is used to determine the factor by which the overall carbon footprint decreases. The answer_unit is a multiplier, so the answer_value will be 3, indicating that the carbon footprint decreases to one-third of its original value.","3","multiplier","[""wu2021a""]","is_blank","Figure 9 illustrates that, as GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce. Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. This information is used to determine the factor by which the overall carbon footprint decreases. The answer_unit is a multiplier, so the answer_value will be 3, indicating that the carbon footprint decreases to one-third of its original value."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) estimate the total compute for model development by applying a multiplicative factor to the final training run compute. This factor is sampled from a log-normal distribution with a 90% CI of 1.2x to 4x. The answer_unit is a multiplier, so the answer_value should be a number representing the range of this factor.","[1.2, 4]","multiplier","[""[ref_id=cottier2024]""]","is_blank","A.6 Accounting for compute used throughout model development ... We sampled from the range of plausible values using a log-normal distribution. The distribution was defined by a 90% CI of 1.2x to 4x, leading to a median of 2.2x.","Cottier et al. (2025) estimate the total compute for model development by applying a multiplicative factor to the final training run compute. This factor is sampled from a log-normal distribution with a 90% CI of 1.2x to 4x. The answer_unit is a multiplier, so the answer_value should be a number representing the range of this factor."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides the estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model. The estimate is given as approximately (60/8) ∗ 13.8 = 103.5 MWh. The answer_unit is MWh, so the answer_value should be expressed in MWh.","103.5","MWh","[""dodge2022""]","is_blank","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context provides the estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model. The estimate is given as approximately (60/8) ∗ 13.8 = 103.5 MWh. The answer_unit is MWh, so the answer_value should be expressed in MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context provided discusses the potential environmental impacts of AI and the concept of Jevons' Paradox, which suggests that increased efficiency can lead to increased consumption. The paper by Luccioni et al. in 2025 argues that technical efficiency gains in AI may not lead to net environmental benefits due to this principle. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The context provided discusses the potential environmental impacts of AI and the concept of Jevons' Paradox, which suggests that increased efficiency can lead to increased consumption. The paper by Luccioni et al. in 2025 argues that technical efficiency gains in AI may not lead to net environmental benefits due to this principle. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a clear estimate of the global AI demand's water withdrawal in 2027. According to the text, this demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal. The answer_unit specifies that the answer should be in billion cubic meters, which matches the given estimate.","[4.2, 6.6]","billion cubic meters","[""[li2025b]""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context provides a clear estimate of the global AI demand's water withdrawal in 2027. According to the text, this demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal. The answer_unit specifies that the answer should be in billion cubic meters, which matches the given estimate."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context does not support the claim that Red AI is on the decline. In fact, it states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. This indicates that despite the known diminishing returns, Red AI continues to increase. The answer_unit is '1' for True and '0' for False, and based on the context, the correct answer is '0' because the statement is False.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context does not support the claim that Red AI is on the decline. In fact, it states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. This indicates that despite the known diminishing returns, Red AI continues to increase. The answer_unit is '1' for True and '0' for False, and based on the context, the correct answer is '0' because the statement is False."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context provides the total wall-clock time required to train the FLM-101B model using a growth strategy. According to the text, this time is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch. The answer_unit is 'days', so the answer_value will be the numeric value of the time in days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context provides the total wall-clock time required to train the FLM-101B model using a growth strategy. According to the text, this time is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch. The answer_unit is 'days', so the answer_value will be the numeric value of the time in days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50% capacity, which directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity. Therefore, the answer is False. The answer_unit is not applicable in this case as the question requires a True or False response.","0","is_blank","[""wu2021a""]","is_blank","Figure 10. A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50% capacity, which directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity. Therefore, the answer is False. The answer_unit is not applicable in this case as the question requires a True or False response."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippet from [ref_id=jegham2025] states that 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity'. This directly answers the question about the estimated electricity consumption for training GPT-3. The answer_unit is MWh, which matches the unit provided in the context.","1287","MWh","[""jegham2025""]","is_blank","Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The context snippet from [ref_id=jegham2025] states that 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity'. This directly answers the question about the estimated electricity consumption for training GPT-3. The answer_unit is MWh, which matches the unit provided in the context."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the AI Energy Score as a project aiming to establish a unified approach for comparing the inference efficiency of AI models. This information directly answers the question about the collaborative project's name. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier or name from the context.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the AI Energy Score as a project aiming to establish a unified approach for comparing the inference efficiency of AI models. This information directly answers the question about the collaborative project's name. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier or name from the context."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides information about the execution time breakdown of the MoE layer for different kernels in Mixtral and BlackMamba models. Specifically, Fig. 6 shows the kernel-level MoE time breakdown for BlackMamba with a batch size of 30. The longest kernel is matmul(w1), which takes 1600 microseconds.","1600","microseconds","[""[ref_id=xia2024]""]","is_blank","Fig. 6, Execution breakdown of the MoE layer for different kernels.","The context provides information about the execution time breakdown of the MoE layer for different kernels in Mixtral and BlackMamba models. Specifically, Fig. 6 shows the kernel-level MoE time breakdown for BlackMamba with a batch size of 30. The longest kernel is matmul(w1), which takes 1600 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context from ref_id=shen2024 states that in the JetMoE-8B model, the number of experts selected for activation (top-k) for a given token is set to 2 for every layer. This information is provided in the hyperparameters table and in the text describing the model's architecture. The answer_unit is 'experts', so the answer_value should be the number of experts selected.","2","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters. ... We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context from ref_id=shen2024 states that in the JetMoE-8B model, the number of experts selected for activation (top-k) for a given token is set to 2 for every layer. This information is provided in the hyperparameters table and in the text describing the model's architecture. The answer_unit is 'experts', so the answer_value should be the number of experts selected."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018. This directly contradicts the statement of a 200,000x increase, making the statement false. The answer_unit is not applicable in this case as the question requires a true or false response.","0","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","The context states that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018. This directly contradicts the statement of a 200,000x increase, making the statement false. The answer_unit is not applicable in this case as the question requires a true or false response."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context mentions that the global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic. This information is provided in the snippet with ref_id=wu2021b. The answer_unit is percent, so the answer_value will be a numeric value representing the percentage drop.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context mentions that the global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic. This information is provided in the snippet with ref_id=wu2021b. The answer_unit is percent, so the answer_value will be a numeric value representing the percentage drop."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources. This information directly answers the question and provides the required numeric value. The answer_unit is 'data centers', which in this case is a count of regions, so the answer_value will be a number representing that count.","22","data centers","[""amazon2023""]","is_blank","Data Centers Powered with Renewable Energy Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources. This information directly answers the question and provides the required numeric value. The answer_unit is 'data centers', which in this case is a count of regions, so the answer_value will be a number representing that count."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find the approximate age of the field in 2025, we subtract the founding year from 2025. This calculation yields an age of approximately 69 years. The answer_unit is years, so the answer_value will be a number in years.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To find the approximate age of the field in 2025, we subtract the founding year from 2025. This calculation yields an age of approximately 69 years. The answer_unit is years, so the answer_value will be a number in years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models. The estimate is based on the Neural Architecture Search (NAS) approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter. This process is infrequently performed due to its large-scale nature and the significant resources required.","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c"", ""zschache2025""]","is_blank","According to the text, 'The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.'","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models. The estimate is based on the Neural Architecture Search (NAS) approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter. This process is infrequently performed due to its large-scale nature and the significant resources required."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average Time Per Output Token (TPOT) of 100 ms for the Llama 3.1 8B model results in a 44% reduction in energy consumption per generation compared to the configuration that simply minimizes latency. This information is used to determine the percentage decrease in energy use.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average Time Per Output Token (TPOT) of 100 ms for the Llama 3.1 8B model results in a 44% reduction in energy consumption per generation compared to the configuration that simply minimizes latency. This information is used to determine the percentage decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (TABLE III) that lists the maximum batch size supported by different model and dataset combinations. The table shows that for BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 8. The answer_unit is 'samples', which matches the unit used in the table.","8","samples","[""xia2024""]","is_blank","TABLE III: MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE. BlackMamba-S on GSM8K dataset has a maximum batch size of 8.","The context provides a table (TABLE III) that lists the maximum batch size supported by different model and dataset combinations. The table shows that for BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 8. The answer_unit is 'samples', which matches the unit used in the table."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022. This information is used to determine the answer, and the answer_unit 'GW' specifies that the answer should be expressed in gigawatts.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context states that Amazon now holds 1.3 GW of storage capacity, up from 445 MW in 2022. This information is used to determine the answer, and the answer_unit 'GW' specifies that the answer should be expressed in gigawatts."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context provides a comparison between the annual carbon emissions from GPT-4o inference and the emissions from transatlantic flights. According to the text, the emissions are comparable to those from approximately 272 transatlantic flights between Boston and London. The answer_unit is 'flights', so the answer_value should be a number representing the number of flights.","272","flights","[""jegham2025""]","is_blank","Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context provides a comparison between the annual carbon emissions from GPT-4o inference and the emissions from transatlantic flights. According to the text, the emissions are comparable to those from approximately 272 transatlantic flights between Boston and London. The answer_unit is 'flights', so the answer_value should be a number representing the number of flights."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","","","percent","[]","is_blank","is_blank",""
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The global average power usage effectiveness (PUE) of data centers in 2023 is mentioned in the context snippet [ref_id=ebert2024]. The PUE value is given as 1.58 globally. The answer_unit is PUE, which is a unitless ratio, so the answer_value will be the numeric value of the PUE.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The global average power usage effectiveness (PUE) of data centers in 2023 is mentioned in the context snippet [ref_id=ebert2024]. The PUE value is given as 1.58 globally. The answer_unit is PUE, which is a unitless ratio, so the answer_value will be the numeric value of the PUE."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token. This information is used to determine the number of parameters activated during inference, which matches the 'parameters' unit specified in the answer_unit field.","2000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token. This information is used to determine the number of parameters activated during inference, which matches the 'parameters' unit specified in the answer_unit field."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides information about the GHG emissions associated with pre-training various models, including the Llama 7B model. According to Table 2 in the context, the Llama 2 7B model has carbon emissions of 31 tCO2e. The answer_unit is tCO2e, so the answer_value should be a number in this unit.","31","tCO2e","[""morrison2025""]","is_blank","Table 2: ... Llama 2 7B 81 31 6 yrs, 1 mo - -","The context provides information about the GHG emissions associated with pre-training various models, including the Llama 7B model. According to Table 2 in the context, the Llama 2 7B model has carbon emissions of 31 tCO2e. The answer_unit is tCO2e, so the answer_value should be a number in this unit."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","To find the total number of Amazon electric delivery vans added across 2022 and 2023, we need to look at the differences in the number of vans between these two years for each region and sum them up. The context provides the number of electric delivery vans in the U.S., Europe, and India for both 2022 and 2023. By subtracting the 2022 numbers from the 2023 numbers for each region and then adding those results together, we can find the total number of vans added.","[9410]","electric delivery vans","[""amazon2023""]","is_blank","U.S. 2,600 to 11,800, Europe 1,220 to 3,000+, India 3,800 to 7,200+","To find the total number of Amazon electric delivery vans added across 2022 and 2023, we need to look at the differences in the number of vans between these two years for each region and sum them up. The context provides the number of electric delivery vans in the U.S., Europe, and India for both 2022 and 2023. By subtracting the 2022 numbers from the 2023 numbers for each region and then adding those results together, we can find the total number of vans added."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the statement in question, making it false. The answer_unit is '1' for True and '0' for False.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the statement in question, making it false. The answer_unit is '1' for True and '0' for False."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides a table (Table 4) that lists the energy consumption of various models, including the o3 model, for different prompt sizes. The energy consumption for the o3 model for a long prompt (10k input-1.5k output) is given as 12.222 Wh. The answer_unit is Wh, so the answer_value will be the numeric value of the energy consumption in Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).","The context provides a table (Table 4) that lists the energy consumption of various models, including the o3 model, for different prompt sizes. The energy consumption for the o3 model for a long prompt (10k input-1.5k output) is given as 12.222 Wh. The answer_unit is Wh, so the answer_value will be the numeric value of the energy consumption in Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. This information is used to determine the year when the field of Artificial Intelligence was officially christened. The answer_unit is 'year', so the answer_value should be a number representing the year.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. This information is used to determine the year when the field of Artificial Intelligence was officially christened. The answer_unit is 'year', so the answer_value should be a number representing the year."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. However, it mentions that Google's annualized global on-site water efficiency is 1 L/kWh. The answer_unit field specifies that the expected format/unit for answer_value is WUE.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. However, it mentions that Google's annualized global on-site water efficiency is 1 L/kWh. The answer_unit field specifies that the expected format/unit for answer_value is WUE."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context snippet from [ref_id=wu2021b] states that approximately 770 million people worldwide lack access to a stable supply of electricity, according to the International Energy Agency. This directly supports the question, and since the answer_unit is not specified as a unit, the answer_value should reflect the truth value of the statement. Given the context, the statement is true.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context snippet from [ref_id=wu2021b] states that approximately 770 million people worldwide lack access to a stable supply of electricity, according to the International Energy Agency. This directly supports the question, and since the answer_unit is not specified as a unit, the answer_value should reflect the truth value of the statement. Given the context, the statement is true."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that with a length of 512, it takes about 3-4 Joules for an output token. This information is used to determine the energy per token consumed by LLaMA-65B at a maximum generation length of 512 tokens. The answer_unit is joules per token, so the answer_value will be a range in joules.","[3,4]","joules per token","[""[ref_id=samsi2024]""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that with a length of 512, it takes about 3-4 Joules for an output token. This information is used to determine the energy per token consumed by LLaMA-65B at a maximum generation length of 512 tokens. The answer_unit is joules per token, so the answer_value will be a range in joules."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This directly answers the question about the percentage reduction in model size after quantization. The answer_unit is percent, so the answer_value will be a numeric percentage.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This directly answers the question about the percentage reduction in model size after quantization. The answer_unit is percent, so the answer_value will be a numeric percentage."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context does not provide clear, publicly available calculations and sound scientific grounding to support the claim that AI can reduce global GHG emissions by 5-10%. In fact, it states that 'the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed' and that 'applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding'. Therefore, the answer is False.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.","The context does not provide clear, publicly available calculations and sound scientific grounding to support the claim that AI can reduce global GHG emissions by 5-10%. In fact, it states that 'the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed' and that 'applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding'. Therefore, the answer is False."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This information directly answers the question. The answer_unit is '1' for True and '0' for False, which requires a boolean response.","1","is_blank","[""patterson2021""]","is_blank","We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): ● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This information directly answers the question. The answer_unit is '1' for True and '0' for False, which requires a boolean response."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The estimated cost of training AI2's Grover on 256 TPU chips for two weeks is provided in the context snippet from [ref_id=schwartz2019]. The answer_unit is USD, so the answer_value should be a number in USD. The context states that the estimated cost is $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The estimated cost of training AI2's Grover on 256 TPU chips for two weeks is provided in the context snippet from [ref_id=schwartz2019]. The answer_unit is USD, so the answer_value should be a number in USD. The context states that the estimated cost is $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1. This information directly answers the question about the percentage of running time taken by the optimizer stage in BlackMamba sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1. This information directly answers the question about the percentage of running time taken by the optimizer stage in BlackMamba sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","To estimate the daily energy consumption of a large-scale service like Google Translate, we can use the average energy for a comparable multi-purpose model. The context provides the energy consumption per 1k queries for the Flan-T5-xxl model, which is 0.083 kWh. We can then multiply this value by the number of queries per day (1 billion) and convert it to MWh.","83","MWh","[""is_blank""]","is_blank","is_blank","To estimate the daily energy consumption of a large-scale service like Google Translate, we can use the average energy for a comparable multi-purpose model. The context provides the energy consumption per 1k queries for the Flan-T5-xxl model, which is 0.083 kWh. We can then multiply this value by the number of queries per day (1 billion) and convert it to MWh."
"q067","What was the average global data center PUE in 2023?","The average global data center PUE in 2023 is mentioned in the context snippet from [ref_id=ebert2024] as 1.58. This value is directly provided in the text and matches the expected format for answer_unit 'PUE', which is a unitless ratio.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The average global data center PUE in 2023 is mentioned in the context snippet from [ref_id=ebert2024] as 1.58. This value is directly provided in the text and matches the expected format for answer_unit 'PUE', which is a unitless ratio."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provided states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, which includes equity. This information is used to determine the percentage of the cost of developing Gemini Ultra attributed to R&D staff. The answer_unit is percent, so the answer_value will be a numeric value representing this percentage.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context provided states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, which includes equity. This information is used to determine the percentage of the cost of developing Gemini Ultra attributed to R&D staff. The answer_unit is percent, so the answer_value will be a numeric value representing this percentage."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context states that the seventeen-member Study Panel was launched in mid-fall 2015, which directly answers the question about the number of members in the inaugural 2015 Study Panel. The answer_unit is 'people', so the answer_value should be a number representing the count of people. The ref_id is 'stone2022' as this information is found in the provided context snippets with this reference id.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context states that the seventeen-member Study Panel was launched in mid-fall 2015, which directly answers the question about the number of members in the inaugural 2015 Study Panel. The answer_unit is 'people', so the answer_value should be a number representing the count of people. The ref_id is 'stone2022' as this information is found in the provided context snippets with this reference id."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This information is used to determine the percentage of a client device's total carbon footprint that is accounted for by its manufacturing. The answer_unit is percent, so the answer_value will be a numeric value representing this percentage.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This information is used to determine the percentage of a client device's total carbon footprint that is accounted for by its manufacturing. The answer_unit is percent, so the answer_value will be a numeric value representing this percentage."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context does not support the statement that a model with more parameters will always consume more energy during inference. In fact, it is mentioned that 'Generally, models with more parameters consume more energy, but this is not always the case.' This indicates an exception to the general trend, meaning the statement is false.","0","is_blank","[""chung2025""]","is_blank","Generally, models with more parameters consume more energy, but this is not always the case.","The context does not support the statement that a model with more parameters will always consume more energy during inference. In fact, it is mentioned that 'Generally, models with more parameters consume more energy, but this is not always the case.' This indicates an exception to the general trend, meaning the statement is false."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, which directly answers the question. The answer_unit is not applicable in this case as the question requires a True or False response. The correct answer is False, which is represented numerically as 0.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, which directly answers the question. The answer_unit is not applicable in this case as the question requires a True or False response. The correct answer is False, which is represented numerically as 0."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide information about the CO2 emissions of OpenAI's API requests in January 2024. Therefore, the answer is 'is_blank'.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide information about the CO2 emissions of OpenAI's API requests in January 2024. Therefore, the answer is 'is_blank'."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context from ref_id=luccioni2025c reports the GHG emissions from the pre-training process for Meta's Llama 3 family of models as 11,390 tons CO2e. This is compared to the 'five cars' estimate, which is stated to be around 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions, highlighting that the actual emissions exceed the 'five cars' estimate by over 40 times.","11390","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the “five cars” number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the “five cars” estimate.","The context from ref_id=luccioni2025c reports the GHG emissions from the pre-training process for Meta's Llama 3 family of models as 11,390 tons CO2e. This is compared to the 'five cars' estimate, which is stated to be around 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions, highlighting that the actual emissions exceed the 'five cars' estimate by over 40 times."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years. This information is provided in Figure 2(d) and is further explained in the text. The answer_unit is a multiplier, so the answer_value should be a number representing the factor by which the capacity increased.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) and the corresponding text","The context states that the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years. This information is provided in Figure 2(d) and is further explained in the text. The answer_unit is a multiplier, so the answer_value should be a number representing the factor by which the capacity increased."
"q079","How many miles is the Earth from the Sun?","The context does not provide information about the distance from the Earth to the Sun.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide information about the distance from the Earth to the Sun."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that the AlphaGo program defeated the human Go champion, which directly answers the question. The answer_unit is '1' for True and '0' for False, so the answer_value will be 1. The ref_id will be the list of document ids where this information is found.","1","is_blank","[""stone2022"", ""schwartz2019"", ""wu2021a""]","is_blank","AlphaGo beats human Go champ in milestone for artificial intelligence, AlphaGo, the best version of which required 1,920 CPUs and 280 GPUs to play a single game of Go, AlphaGo: Shedding new light on chess, shogi, and Go","The context clearly states that the AlphaGo program defeated the human Go champion, which directly answers the question. The answer_unit is '1' for True and '0' for False, so the answer_value will be 1. The ref_id will be the list of document ids where this information is found."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions a batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, which is referred to as 'continuous batching'. This strategy is described in the snippet from [ref_id=fernandez2025] as a method to mitigate GPU under-utilization during generation. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions a batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, which is referred to as 'continuous batching'. This strategy is described in the snippet from [ref_id=fernandez2025] as a method to mitigate GPU under-utilization during generation. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context explicitly states that the entire alignment process for JetMoE-8B, including both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This information directly answers the question about the number of H100 GPU hours required for the alignment process.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context explicitly states that the entire alignment process for JetMoE-8B, including both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This information directly answers the question about the number of H100 GPU hours required for the alignment process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The percentage increase in cost can be calculated by finding the difference in costs and dividing by the cost of the InferSave-selected instance, then multiplying by 100. This calculation yields ((2.699 - 2.13) / 2.13) * 100 = 26.7%. The answer_unit is percent, so the answer_value should be expressed as a percentage.","26.7","percent","[""kim2025""]","is_blank","Table VI and the surrounding text describe the instance selection and costs for the offline workload experiment with a 100 TPS SLO.","The context states that with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The percentage increase in cost can be calculated by finding the difference in costs and dividing by the cost of the InferSave-selected instance, then multiplying by 100. This calculation yields ((2.699 - 2.13) / 2.13) * 100 = 26.7%. The answer_unit is percent, so the answer_value should be expressed as a percentage."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context from ref_id=luccioni2024 provides the information about the carbon intensity of the stable-diffusion-xl-base-1.0 model. According to the text, this model generates 1,594 grams of CO2eq for 1,000 inferences. The answer_unit is 'g CO2eq', which matches the unit provided in the context.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context from ref_id=luccioni2024 provides the information about the carbon intensity of the stable-diffusion-xl-base-1.0 model. According to the text, this model generates 1,594 grams of CO2eq for 1,000 inferences. The answer_unit is 'g CO2eq', which matches the unit provided in the context."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides a range of GPU energy usage for 1,000 inference queries in a 2025 study's appendix, specifically in Table 2. The answer_unit is Wh, so the answer_value should be a numeric range in Wh. The range of GPU energy usage is from 0.06 Wh for the bert-tiny model to over 3,426 Wh for the Command-R Plus model.","[0.06, 3426]","Wh","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed)","The context provides a range of GPU energy usage for 1,000 inference queries in a 2025 study's appendix, specifically in Table 2. The answer_unit is Wh, so the answer_value should be a numeric range in Wh. The range of GPU energy usage is from 0.06 Wh for the bert-tiny model to over 3,426 Wh for the Command-R Plus model."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context does not support the idea that researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. In fact, it highlights the complexity and nuance of these issues, suggesting that a more context-specific approach is needed. The answer_unit is '1' for True and '0' for False, but since the statement is False according to the context, the answer_value will be '0'.","0","is_blank","[""luccioni2025b""]","is_blank","There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","The context does not support the idea that researchers believe a universal, one-size-fits-all approach to AI ethics and sustainability can be developed. In fact, it highlights the complexity and nuance of these issues, suggesting that a more context-specific approach is needed. The answer_unit is '1' for True and '0' for False, but since the statement is False according to the context, the answer_value will be '0'."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The gross carbon intensity of energy according to the U.S. average mix is mentioned in the context snippet from [ref_id=patterson2021]. The value is given as 0.429 kg of CO2e/KWh. This value matches the expected format specified in the answer_unit field.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh.","The gross carbon intensity of energy according to the U.S. average mix is mentioned in the context snippet from [ref_id=patterson2021]. The value is given as 0.429 kg of CO2e/KWh. This value matches the expected format specified in the answer_unit field."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions a decentralized PyTorch-based framework called Hivemind, which was used to enable distributed spot instance training across clouds and continents. This framework is specifically designed to handle peers that can stop running at any time, making it suitable for spot instance training. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","Hivemind","is_blank","[""erben2023""]","[""https://github.com/learning-at-home/hivemind""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions a decentralized PyTorch-based framework called Hivemind, which was used to enable distributed spot instance training across clouds and continents. This framework is specifically designed to handle peers that can stop running at any time, making it suitable for spot instance training. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'. This term is mentioned in the context as a way to integrate socio-technical aspects in the description and understanding of AI systems. The answer_unit is 'is_blank', which means the answer_value should be the exact identifier/name from the context that answers the question.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'. This term is mentioned in the context as a way to integrate socio-technical aspects in the description and understanding of AI systems. The answer_unit is 'is_blank', which means the answer_value should be the exact identifier/name from the context that answers the question."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context does not provide a clear answer to the question about the model using sentence embeddings that achieved the highest accuracy in classification experiments on German public administration texts. However, it mentions that the best model for a specific task is a traditional model (Linear Embedding) with moderate energy consumption. Since the question specifically asks about sentence embeddings, the answer cannot be directly determined from the given context.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question about the model using sentence embeddings that achieved the highest accuracy in classification experiments on German public administration texts. However, it mentions that the best model for a specific task is a traditional model (Linear Embedding) with moderate energy consumption. Since the question specifically asks about sentence embeddings, the answer cannot be directly determined from the given context."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions the development of an LLM inference system that incorporates model-attention disaggregation. The system is named Lamina, which is described as a distributed heterogeneous LLM decoding system. This information directly answers the question about the name of the LLM inference system.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions the development of an LLM inference system that incorporates model-attention disaggregation. The system is named Lamina, which is described as a distributed heterogeneous LLM decoding system. This information directly answers the question about the name of the LLM inference system."
"q093","How many parameters does the largest T5 model have?","The context mentions the largest T5 model having 11B parameters. The answer_unit is 'parameters', so the answer_value should be a number representing the number of parameters.","11000000000","parameters","[""patterson2021""]","is_blank","Table 4. CO 2 e for NLP models (see Appendix A) 12 . ... ● T5    is a   pre-trained   language   model   that   casts   all   NLP   problems   in   a   unified   text-to-text   format   to   enable   application   of   transfer   learning   techniques   to   reduce   the   cost   of   training   [Raf19].   The   largest   size   has   11B   parameters,   and   training   used   86   MWh   and   produced   47   tCO 2 e.","The context mentions the largest T5 model having 11B parameters. The answer_unit is 'parameters', so the answer_value should be a number representing the number of parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The total number of parameters in the JetMoE-8B model is mentioned in the context as 8B parameters. This value is directly extracted from the text and matches the expected format of 'parameters'.","8000000000","parameters","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The total number of parameters in the JetMoE-8B model is mentioned in the context as 8B parameters. This value is directly extracted from the text and matches the expected format of 'parameters'."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022, according to corporate reports. This information directly answers the question about the percentage increase in Google's data center water consumption during that period. The answer_unit is 'percent', which matches the format of the answer provided in the context.","20","percent","[""luccioni2025a"", ""li2025b""]","[""https://www.greenpeace.org/usa/reports/oil-in-the-cloud/"", ""https://sustainability.fb.com/2021-sustainability-report/""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022, according to corporate reports. This information directly answers the question about the percentage increase in Google's data center water consumption during that period. The answer_unit is 'percent', which matches the format of the answer provided in the context."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context provides a table outlining common carbon emission metrics in Green AI, including their units and definitions. The 'Carbon Intensity' metric is defined as 'CO₂ emissions per unit of electricity consumed', which directly answers the question. The answer_unit 'is_blank' indicates that the answer should be the exact identifier/name from the context.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","TABLE I: COMMON CARBON EMISSION METRICS IN GREEN AI, Metric 'Carbon Intensity' with unit 'gCO2/kWh'","The context provides a table outlining common carbon emission metrics in Green AI, including their units and definitions. The 'Carbon Intensity' metric is defined as 'CO₂ emissions per unit of electricity consumed', which directly answers the question. The answer_unit 'is_blank' indicates that the answer should be the exact identifier/name from the context."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides information about the FLM-101B training process, including the parallel strategies and throughput for different growth stages. In Table 2, the FLOPs utilization percentage for the final growth stage (101B) is listed as 52.88%. The answer_unit is 'percent', so the answer_value should be expressed as a percentage.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages.","The context provides information about the FLM-101B training process, including the parallel strategies and throughput for different growth stages. In Table 2, the FLOPs utilization percentage for the final growth stage (101B) is listed as 52.88%. The answer_unit is 'percent', so the answer_value should be expressed as a percentage."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context provides the estimated amortized training costs for OpenAI's GPT-4. According to the text, the estimated amortized hardware and energy cost for GPT-4 is $40M. The answer_unit is USD, so the answer_value should be a number in USD.","40","USD","[""cottier2024""]","[""arXiv:2405.21015v2""]","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context provides the estimated amortized training costs for OpenAI's GPT-4. According to the text, the estimated amortized hardware and energy cost for GPT-4 is $40M. The answer_unit is USD, so the answer_value should be a number in USD."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can reduce the operational carbon footprint of a Transformer-based universal translation model by more than 800×. This is mentioned in the snippet from [ref_id=wu2021a]. The answer_unit is a multiplier, so the answer_value should be a number representing the factor by which the operational carbon footprint is reduced.","800","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization .","The context states that full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes, can reduce the operational carbon footprint of a Transformer-based universal translation model by more than 800×. This is mentioned in the snippet from [ref_id=wu2021a]. The answer_unit is a multiplier, so the answer_value should be a number representing the factor by which the operational carbon footprint is reduced."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information about the performance of NLP training when spread across four continents instead of remaining local. According to the text, the NLP task is no longer suitable for distributed training when spread across four continents, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). This indicates that the fraction of local throughput achieved for NLP when training was spread across four continents is 0.59, since 100% - 41% = 59%.","0.59","multiplier","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context provides information about the performance of NLP training when spread across four continents instead of remaining local. According to the text, the NLP task is no longer suitable for distributed training when spread across four continents, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). This indicates that the fraction of local throughput achieved for NLP when training was spread across four continents is 0.59, since 100% - 41% = 59%."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This information directly answers the question about the amount of water returned to communities from Amazon's replenishment projects in 2023. The answer_unit is 'liters', so the answer_value should be a number in liters.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This information directly answers the question about the amount of water returned to communities from Amazon's replenishment projects in 2023. The answer_unit is 'liters', so the answer_value should be a number in liters."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context provided from the document snippets supports the answer through the discussion of how custom tags can improve the energy efficiency of LLMs during source code completion tasks. Specifically, it mentions that custom tags can reduce energy consumption across different prompt engineering techniques such as zero-shot, one-shot, and few-shots. The answer_unit is '1' for True and '0' for False, which requires a boolean response based on the context.","1","is_blank","[""rubei2025""]","is_blank","Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context provided from the document snippets supports the answer through the discussion of how custom tags can improve the energy efficiency of LLMs during source code completion tasks. Specifically, it mentions that custom tags can reduce energy consumption across different prompt engineering techniques such as zero-shot, one-shot, and few-shots. The answer_unit is '1' for True and '0' for False, which requires a boolean response based on the context."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is reported in a 2025 paper. The answer_unit is 'GPUs', so the answer_value should be a number representing the quantity of GPUs shipped. The ref_id and ref_url provide evidence from the cited document.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is reported in a 2025 paper. The answer_unit is 'GPUs', so the answer_value should be a number representing the quantity of GPUs shipped. The ref_id and ref_url provide evidence from the cited document."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context from [ref_id=cottier2024] states that 'on average, 44% goes toward AI accelerator chips' when breaking down the components of amortized hardware CapEx + energy. This directly supports the answer as a percentage, matching the 'percent' unit specified in the additional info.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context from [ref_id=cottier2024] states that 'on average, 44% goes toward AI accelerator chips' when breaking down the components of amortized hardware CapEx + energy. This directly supports the answer as a percentage, matching the 'percent' unit specified in the additional info."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The Power Usage Effectiveness (PUE) for Facebook's data centers is mentioned in the context as being about 1.10. This value is directly provided in the text and matches the expected format for answer_unit.","1.1","PUE","[""wu2021a"", ""wu2021b""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.","The Power Usage Effectiveness (PUE) for Facebook's data centers is mentioned in the context as being about 1.10. This value is directly provided in the text and matches the expected format for answer_unit."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish project ETAIROS, which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym ETAIROS is explicitly mentioned in the text, allowing us to directly answer the question. The answer_unit 'is_blank' indicates that we should provide the exact identifier/name from the context.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","The Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems","The context mentions the Finnish project ETAIROS, which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The acronym ETAIROS is explicitly mentioned in the text, allowing us to directly answer the question. The answer_unit 'is_blank' indicates that we should provide the exact identifier/name from the context."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The estimated amortized training costs for Google's Gemini Ultra can be found in the provided context snippets. According to the text, the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. The answer_unit is specified as USD, so the answer_value should be expressed as a number in USD.","30","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The estimated amortized training costs for Google's Gemini Ultra can be found in the provided context snippets. According to the text, the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. The answer_unit is specified as USD, so the answer_value should be expressed as a number in USD."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, and the authors argue that these measures should also consider environmental risks. However, it does not explicitly state that the AI Act requires providers to conduct risk assessments that include environmental risks. The answer_unit is '1' for True and '0' for False.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, and the authors argue that these measures should also consider environmental risks. However, it does not explicitly state that the AI Act requires providers to conduct risk assessments that include environmental risks. The answer_unit is '1' for True and '0' for False."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context snippet from [ref_id=han2024] states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This value is directly provided in the text and matches the expected unit format specified in the 'answer_unit' field.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context snippet from [ref_id=han2024] states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This value is directly provided in the text and matches the expected unit format specified in the 'answer_unit' field."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context provides information about a life cycle assessment comparing the environmental impacts of print books and e-readers. According to the assessment, 115 books would produce the same amount of CO2 as a single Amazon Kindle device. The answer_unit is 'books', so the answer_value should be a number representing the quantity of books.","115","books","[""luccioni2025a""]","is_blank","a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context provides information about a life cycle assessment comparing the environmental impacts of print books and e-readers. According to the assessment, 115 books would produce the same amount of CO2 as a single Amazon Kindle device. The answer_unit is 'books', so the answer_value should be a number representing the quantity of books."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the per-household health burden in disadvantaged communities could be up to 200 times higher than in less-impacted communities, which directly answers the question. The answer_unit is a multiplier, so the answer_value should be a numeric value representing this factor.","200","multiplier","[""han2024""]","is_blank","Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context states that the per-household health burden in disadvantaged communities could be up to 200 times higher than in less-impacted communities, which directly answers the question. The answer_unit is a multiplier, so the answer_value should be a numeric value representing this factor."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset can be found in Table B1 of the context. The table lists the energy consumed in Wh for different models, including DS Llama 70B. The answer_unit is Wh, so the answer_value should be a number in that unit.","702.06","Wh","[""[ref_id=zschache2025]""]","is_blank","Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset can be found in Table B1 of the context. The table lists the energy consumed in Wh for different models, including DS Llama 70B. The answer_unit is Wh, so the answer_value should be a number in that unit."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The question asks about the total number of parameters in the large language model analyzed by Dodge et al. in their 2022 paper. However, the provided context does not contain information about a 2022 paper by Dodge et al. that analyzes a large language model. The context does mention a paper by Dodge et al. but it is from 2022 and titled 'Measuring the Carbon Intensity of AI in Cloud Instances' and it analyzes the energy consumption of training a large language model comprising over 6.1 billion parameters.","6100000000","parameters","[""dodge2022""]","[""https://arxiv.org/abs/2206.14898""]","Table 2: For the 11 models in our analysis: the type of GPU, the number of GPUs of that type, the number of hours, and the energy used in kWh. ... 6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s.","The question asks about the total number of parameters in the large language model analyzed by Dodge et al. in their 2022 paper. However, the provided context does not contain information about a 2022 paper by Dodge et al. that analyzes a large language model. The context does mention a paper by Dodge et al. but it is from 2022 and titled 'Measuring the Carbon Intensity of AI in Cloud Instances' and it analyzes the energy consumption of training a large language model comprising over 6.1 billion parameters."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency, resulting in increased usage and overall resource consumption. This concept is mentioned in the context as a warning that efficiency gains in AI may lead to increased consumption, rather than reduced environmental impact. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier or name from the context.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""jegham2025"", ""morrison2025""]","[""https://doi.org/10.1145/3715275.3732007"", ""https://arxiv.org/abs/2501.12948"", ""https://time.com/7021709/elon-musk-xai-grok-memphis/""]","Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency, resulting in increased usage and overall resource consumption. This concept is mentioned in the context as a warning that efficiency gains in AI may lead to increased consumption, rather than reduced environmental impact. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier or name from the context."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","To find out how many Meena training runs would use the same total energy as a single full training run of GPT-3, we need to compare the energy consumption of both models. According to the context, the energy consumption for a full training run of GPT-3 is approximately 103,500 kWh, and for Meena, it is 232 MWh or 232,000 kWh. By dividing the total energy consumption of GPT-3 by that of Meena, we can determine the number of Meena training runs equivalent to one GPT-3 training run.","0.446","multiplier","[""patterson2021""]","is_blank","Table 4. CO2e for NLP models","To find out how many Meena training runs would use the same total energy as a single full training run of GPT-3, we need to compare the energy consumption of both models. According to the context, the energy consumption for a full training run of GPT-3 is approximately 103,500 kWh, and for Meena, it is 232 MWh or 232,000 kWh. By dividing the total energy consumption of GPT-3 by that of Meena, we can determine the number of Meena training runs equivalent to one GPT-3 training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The question asks for the average energy consumption for performing 1,000 image generation inferences according to Table 2 in a 2024 study. The answer_unit is specified as kWh, so the answer should be a number in that unit. The context provides Table 2, which lists the mean and standard deviation of energy per 1,000 queries for various tasks, including image generation.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. ... image generation 2.907 3.31","The question asks for the average energy consumption for performing 1,000 image generation inferences according to Table 2 in a 2024 study. The answer_unit is specified as kWh, so the answer should be a number in that unit. The context provides Table 2, which lists the mean and standard deviation of energy per 1,000 queries for various tasks, including image generation."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The question asks for the estimated pounds of CO2e for an average American life in one year. The context provides this information in the snippet from ref_id=dodge2022, which mentions that one average US home energy use is estimated to emit 8.30 metric tons of CO2e. To convert metric tons to pounds, we know that 1 metric ton is approximately 2204.62 pounds. Thus, 8.30 metric tons is equivalent to 8.30 * 2204.62 pounds. However, the exact conversion is not necessary as the question seems to directly reference a value given in pounds in the context.","18300","lbs","[""dodge2022""]","is_blank","One average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The question asks for the estimated pounds of CO2e for an average American life in one year. The context provides this information in the snippet from ref_id=dodge2022, which mentions that one average US home energy use is estimated to emit 8.30 metric tons of CO2e. To convert metric tons to pounds, we know that 1 metric ton is approximately 2204.62 pounds. Thus, 8.30 metric tons is equivalent to 8.30 * 2204.62 pounds. However, the exact conversion is not necessary as the question seems to directly reference a value given in pounds in the context."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context does not provide a clear answer to the question about the county in West Virginia with the highest per-household health cost in 2030. The information provided in the context snippets does not include specific projections for 2030 or detailed data on per-household health costs for counties in West Virginia. Therefore, the answer cannot be determined based on the given context.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a clear answer to the question about the county in West Virginia with the highest per-household health cost in 2030. The information provided in the context snippets does not include specific projections for 2030 or detailed data on per-household health costs for counties in West Virginia. Therefore, the answer cannot be determined based on the given context."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","","","multiplier","[]","is_blank","is_blank",""
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the training and fine-tuning energy costs for the BLOOMz-7B model in Table 5. The training energy cost is 51,686 kWh and the fine-tuning energy cost is 7,571 kWh. To find the combined energy cost, we add these two values together.","59257","kWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides the training and fine-tuning energy costs for the BLOOMz-7B model in Table 5. The training energy cost is 51,686 kWh and the fine-tuning energy cost is 7,571 kWh. To find the combined energy cost, we add these two values together."
"q125","What is the total number of parameters in the final FLM-101B model?","The total number of parameters in the final FLM-101B model is mentioned in the context as 101B parameters. The answer_unit is 'parameters', so the answer_value should be the number of parameters in the same unit. The context provides this information in the section describing the FLM-101B model.","101000000000","parameters","[""li2025a""]","is_blank","FLM-101B model is struc- tured with a hidden state dimension of 10, 240, a layer num- ber of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The total number of parameters in the final FLM-101B model is mentioned in the context as 101B parameters. The answer_unit is 'parameters', so the answer_value should be the number of parameters in the same unit. The context provides this information in the section describing the FLM-101B model."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the energy consumption for training a 6.1B parameter model, which is approximately 103,500 kWh. To find the number of inferences needed to match this training energy cost, we can use the energy cost of a comparable model, BLOOMz-7B, which is 1.0 × 10−4 kWh per inference. By dividing the total training energy cost by the energy cost per inference, we can estimate the number of inferences required.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/"", ""https://www.google.com/about/datacenters/efficiency/""]","Table 2 and the text 'Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!' and 'Inference energy (kWh) 1.0 × 10−4' from Table 5","The context provides the energy consumption for training a 6.1B parameter model, which is approximately 103,500 kWh. To find the number of inferences needed to match this training energy cost, we can use the energy cost of a comparable model, BLOOMz-7B, which is 1.0 × 10−4 kWh per inference. By dividing the total training energy cost by the energy cost per inference, we can estimate the number of inferences required."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The 2024 study 'Power Hungry Processing' mentions the total amount of energy consumed for all model experimentation and evaluation. The answer_unit is 'kWh', so the answer_value should be a number in that unit. According to the context, the total energy consumed was 754.66 kWh.","754.66","kWh","[""luccioni2024""]","[""https://""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The 2024 study 'Power Hungry Processing' mentions the total amount of energy consumed for all model experimentation and evaluation. The answer_unit is 'kWh', so the answer_value should be a number in that unit. According to the context, the total energy consumed was 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the energy costs and cost parity for different BLOOMz models. The cost parity is defined as the number of inferences required to sum to the training cost. For the BLOOMz-7B model, the cost parity is given as 592,570,000 inferences. This value is directly extracted from the table and matches the expected unit of 'inferences'.","592570000","inferences","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","Table 5: The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides a table (Table 5) that lists the energy costs and cost parity for different BLOOMz models. The cost parity is defined as the number of inferences required to sum to the training cost. For the BLOOMz-7B model, the cost parity is given as 592,570,000 inferences. This value is directly extracted from the table and matches the expected unit of 'inferences'."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany, and the statements from the population were categorized, processed, and published as the FKTG-dataset. The answer_unit is 'is_blank', which means the answer should be the exact identifier/name from the context that answers the question.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany, and the statements from the population were categorized, processed, and published as the FKTG-dataset. The answer_unit is 'is_blank', which means the answer should be the exact identifier/name from the context that answers the question."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. Therefore, the answer is 'is_blank'.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. Therefore, the answer is 'is_blank'."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","","","percent","[]","is_blank","is_blank",""
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The actual CO2e for the Evolved Transformer NAS is given as 3.2 tCO2e, and it is stated that a single passenger round trip SF-NY is ~1.2t CO2e. To find the equivalent number of passengers, we can divide the actual CO2e by the CO2e per passenger. The context provides the necessary information to calculate this equivalence.","2.67","passengers","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The actual CO2e for the Evolved Transformer NAS is given as 3.2 tCO2e, and it is stated that a single passenger round trip SF-NY is ~1.2t CO2e. To find the equivalent number of passengers, we can divide the actual CO2e by the CO2e per passenger. The context provides the necessary information to calculate this equivalence."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context from ref_id=luccioni2025c states that according to May 2025 data from OpenRouter28, 84% of LLM usage is through models with no disclosure. This directly answers the question about the percentage of LLM token usage through non-disclosing models. The answer_unit is percent, so the answer_value will be the numeric percentage.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure...","The context from ref_id=luccioni2025c states that according to May 2025 data from OpenRouter28, 84% of LLM usage is through models with no disclosure. This directly answers the question about the percentage of LLM token usage through non-disclosing models. The answer_unit is percent, so the answer_value will be the numeric percentage."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for different LLaMA variants. According to Table II, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 1. The answer_unit is A100_80GB_GPU, which matches the unit of the answer.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","The context provides information about the bare minimum hardware requirements for different LLaMA variants. According to Table II, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 1. The answer_unit is A100_80GB_GPU, which matches the unit of the answer."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The estimated CO2 emissions for a complete training run of a 6.1 billion parameter transformer model, specifically the T5 model, is provided in the context. The answer_unit is specified as metric tons, so the answer_value should be a number in that unit.","47","metric tons","[""patterson2021""]","[""https://arxiv.org/abs/2104.10350""]","Table 4 shows a CO2e calculation for five recent large NLP models, including T5, which has 47 tCO2e emissions.","The estimated CO2 emissions for a complete training run of a 6.1 billion parameter transformer model, specifically the T5 model, is provided in the context. The answer_unit is specified as metric tons, so the answer_value should be a number in that unit."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide a direct answer to the question about the total carbon emissions avoided by pruning and quantizing large language models in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide a direct answer to the question about the total carbon emissions avoided by pruning and quantizing large language models in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context mentions a specific scenario where using 2 A100s and 1 A10G results in a cost saving over A100-only and A10G-only strategies. The answer_unit is percent, so the answer_value should be a percentage. The cost savings over the A100-only strategy is given as 24%.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context mentions a specific scenario where using 2 A100s and 1 A10G results in a cost saving over A100-only and A10G-only strategies. The answer_unit is percent, so the answer_value should be a percentage. The cost savings over the A100-only strategy is given as 24%."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides an estimated price per hour for an NVIDIA H20 in USD, matching the required answer_unit.","4.63","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications. ... Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context provides an estimated price per hour for an NVIDIA H20 in USD, matching the required answer_unit."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses for AI models gather information manually by writing to authors, which directly contradicts the statement that information is gathered automatically without needing to contact authors. This indicates the statement is false. The answer_unit is '1' for True and '0' for False, as per the instructions for True/False questions.","0","is_blank","[""luccioni2025b""]","is_blank","In fact, most carbon foot print analyses gather the information manually by writing to authors.","The context states that most carbon footprint analyses for AI models gather information manually by writing to authors, which directly contradicts the statement that information is gathered automatically without needing to contact authors. This indicates the statement is false. The answer_unit is '1' for True and '0' for False, as per the instructions for True/False questions."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost. This information is used to directly answer the question, and the answer_unit 'percent' is applied by expressing the answer as a percentage value.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost. This information is used to directly answer the question, and the answer_unit 'percent' is applied by expressing the answer as a percentage value."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information about the bare minimum hardware requirements for different LLaMA variants. According to Table II, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization is 1. The answer_unit is A100_80GB_GPU, which specifies the expected format/unit for answer_value.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context provides information about the bare minimum hardware requirements for different LLaMA variants. According to Table II, the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization is 1. The answer_unit is A100_80GB_GPU, which specifies the expected format/unit for answer_value."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context does not provide information about a 45% reduction in carbon emissions after quantization for large language models. Therefore, the answer is is_blank.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about a 45% reduction in carbon emissions after quantization for large language models. Therefore, the answer is is_blank."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that researchers reached out to over 500 authors and were able to collect 95 answers, which directly answers the question. The answer_unit is 'answers', so the answer_value should be a number representing the count of answers collected.","95","answers","[""luccioni2023"", ""luccioni2025b""]","is_blank","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","The context states that researchers reached out to over 500 authors and were able to collect 95 answers, which directly answers the question. The answer_unit is 'answers', so the answer_value should be a number representing the count of answers collected."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The cost per H100 GPU-hour is calculated by dividing the total training budget by the total number of GPU hours used. This information is found in the context provided for ref_id shen2024. The answer_unit USD per hour requires the answer to be in dollars without the unit name.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The cost per H100 GPU-hour is calculated by dividing the total training budget by the total number of GPU hours used. This information is found in the context provided for ref_id shen2024. The answer_unit USD per hour requires the answer to be in dollars without the unit name."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides a table (Table 2) that lists the health cost as a percentage of the electricity cost for training a large AI model in selected U.S. data centers. The table includes the location Altoona, Iowa, and the corresponding health cost percentage. The answer_unit is percent, so the answer_value should be a numeric value representing the percentage.","122","percent","[""han2024""]","is_blank","Table 2 in the context","The context provides a table (Table 2) that lists the health cost as a percentage of the electricity cost for training a large AI model in selected U.S. data centers. The table includes the location Altoona, Iowa, and the corresponding health cost percentage. The answer_unit is percent, so the answer_value should be a numeric value representing the percentage."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. The answer_unit is 'tokens', so the answer_value should be a number representing the number of tokens. The context provides this information directly.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. The answer_unit is 'tokens', so the answer_value should be a number representing the number of tokens. The context provides this information directly."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects by location. The answer can be found by looking up the number of projects in the United Kingdom. The answer_unit is 'projects', so the answer_value should be a number representing the count of projects.","36","projects","[""amazon2023""]","is_blank","Project Location: United Kingdom, Number of Projects: 36","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects by location. The answer can be found by looking up the number of projects in the United Kingdom. The answer_unit is 'projects', so the answer_value should be a number representing the count of projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides the percentage of Amazon's workforce in the United States across all levels who identified as men in 2023. According to the snippet, the percentage of men in Amazon's workforce is given as 43.1% for 2023. The answer_unit is percent, so the answer_value will be the numeric value of the percentage.","43.1","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels) 43.1%56.8% 44.1%55.7% 30.8%69.2% 31.4%68.5% 31.5%68.4% 46.9%52.9% 45.5%54.3% 46.6%53.2% 44.3%55.6% 31.1%68.8% 48 31.7%68.3% 31.6%68.3%","The context provides the percentage of Amazon's workforce in the United States across all levels who identified as men in 2023. According to the snippet, the percentage of men in Amazon's workforce is given as 43.1% for 2023. The answer_unit is percent, so the answer_value will be the numeric value of the percentage."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint. This information is used to determine the percentage of Apple's total water footprint accounted for by its supply chain.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple reports its supply chain accounts for 99% of its total water footprint. This information is used to determine the percentage of Apple's total water footprint accounted for by its supply chain."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","","","seconds","[]","is_blank","is_blank",""
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity metric' as a way to assess the ratio of computation to communication time when scaling distributed training across continents. This metric is used to evaluate the scalability potential of different models on the same hardware. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","granularity metric","is_blank","[""erben2023""]","is_blank","We found that the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.","The context introduces the 'granularity metric' as a way to assess the ratio of computation to communication time when scaling distributed training across continents. This metric is used to evaluate the scalability potential of different models on the same hardware. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil could add up to 640 percent more carbon emissions than the company's carbon removal targets for the year. This directly answers the question, and the answer_unit 'times' matches the context's percentage increase, which can be interpreted as a multiplier.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year","The context states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil could add up to 640 percent more carbon emissions than the company's carbon removal targets for the year. This directly answers the question, and the answer_unit 'times' matches the context's percentage increase, which can be interpreted as a multiplier."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines water withdrawal as freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses. This definition directly answers the question, and according to the 'answer_unit' field, the answer should be the exact identifier/name from the context that answers the question. Therefore, the term for freshwater taken from ground or surface sources for various uses is 'water withdrawal'.","water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context defines water withdrawal as freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses. This definition directly answers the question, and according to the 'answer_unit' field, the answer should be the exact identifier/name from the context that answers the question. Therefore, the term for freshwater taken from ground or surface sources for various uses is 'water withdrawal'."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The Standing Committee forms a Study Panel every five years to assess the current state of AI, as stated in the context. This information is used to determine the frequency at which the Study Panel is formed. The answer_unit is 'years', so the answer_value will be a number representing the frequency in years.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The Standing Committee forms a Study Panel every five years to assess the current state of AI, as stated in the context. This information is used to determine the frequency at which the Study Panel is formed. The answer_unit is 'years', so the answer_value will be a number representing the frequency in years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The average number of connected devices per U.S. household is reported in the context as 25 devices, which includes smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc. This value is directly stated in the text and matches the expected unit of 'devices'. The ref_id 'wu2021b' is used as evidence for this answer.","25","devices","[""wu2021b""]","[""https://www.iscaconf.org/isca2021/program/""]","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, ﬁtness trackers, and connected ﬁtness machines [Deloitte, 2021].","The average number of connected devices per U.S. household is reported in the context as 25 devices, which includes smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc. This value is directly stated in the text and matches the expected unit of 'devices'. The ref_id 'wu2021b' is used as evidence for this answer."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training a large language model (LLM) in MWh. According to the snippet from ref_id=luccioni2025c, the energy required to pre-train an LLM spans from as little as 0.8 MWh to 3,500 MWh. The answer_unit is MWh, so the answer_value will be a range in this unit.","[0.8, 3500]","MWh","[""[luccioni2025c]""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides a range of energy consumption for pre-training a large language model (LLM) in MWh. According to the snippet from ref_id=luccioni2025c, the energy required to pre-train an LLM spans from as little as 0.8 MWh to 3,500 MWh. The answer_unit is MWh, so the answer_value will be a range in this unit."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the statement in the question. Therefore, the answer is False. The answer_unit is not applicable in this case as the question requires a True or False response.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the statement in the question. Therefore, the answer is False. The answer_unit is not applicable in this case as the question requires a True or False response."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from [ref_id=luccioni2025a] mentions that one paper suggests 10–50 queries on GPT-3 consumes around half a liter of water. This directly answers the question about the number of queries to the GPT-3 model that consume approximately half a liter of water. The answer_unit is 'queries', so the answer_value should be a range of queries.","[10, 50]","queries","[""[luccioni2025a]""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context snippet from [ref_id=luccioni2025a] mentions that one paper suggests 10–50 queries on GPT-3 consumes around half a liter of water. This directly answers the question about the number of queries to the GPT-3 model that consume approximately half a liter of water. The answer_unit is 'queries', so the answer_value should be a range of queries."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context provides a table comparing MT-Bench scores of various models, including JetMoE-8B-Chat and Llama-2-13b-Chat. According to the table, JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. The answer_unit is 'score', so the answer_value should be the numeric score.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models","The context provides a table comparing MT-Bench scores of various models, including JetMoE-8B-Chat and Llama-2-13b-Chat. According to the table, JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. The answer_unit is 'score', so the answer_value should be the numeric score."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context provides information about the water consumption of GPT-3, specifically that it needs to 'drink' a 500ml bottle of water for roughly 10-50 medium-length responses. This information is used to calculate the number of medium-length completions that could be produced with the water required to fill a single 500 mL bottle. The answer_unit is 'responses', which means the answer_value should be a number representing the number of responses.","[10,50]","responses","[""[ref_id=li2025b]""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context provides information about the water consumption of GPT-3, specifically that it needs to 'drink' a 500ml bottle of water for roughly 10-50 medium-length responses. This information is used to calculate the number of medium-length completions that could be produced with the water required to fill a single 500 mL bottle. The answer_unit is 'responses', which means the answer_value should be a number representing the number of responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings. This is supported by the context, which explicitly states the cost reduction percentage. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings. This is supported by the context, which explicitly states the cost reduction percentage. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that for any meaningful inferences with the 65B LLaMA model, at a minimum, 4 A100 GPUs each with 80GB of memory are required. This information is used to determine the bare minimum number of A100 80GB GPUs required to run LLaMA-65B inference without compression or quantization.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that for any meaningful inferences with the 65B LLaMA model, at a minimum, 4 A100 GPUs each with 80GB of memory are required. This information is used to determine the bare minimum number of A100 80GB GPUs required to run LLaMA-65B inference without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context provided states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This information is used to determine the answer value in the specified unit of 'round trips'.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City (LA-NYC), resulting in a health cost that even exceeds 120% of the training electricity cost.","The context provided states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This information is used to determine the answer value in the specified unit of 'round trips'."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from [ref_id=patterson2021] states that NVIDIA estimated 80–90% of the ML workload to be inference processing. This estimate is directly provided in the context and matches the requested answer_unit of percent.","[80,90]","percent","[""[patterson2021, chung2025, luccioni2024, fernandez2025]""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context from [ref_id=patterson2021] states that NVIDIA estimated 80–90% of the ML workload to be inference processing. This estimate is directly provided in the context and matches the requested answer_unit of percent."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is found in the context snippet with ref_id 'luccioni2024'. The snippet mentions that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞'. The answer_unit is 'kg CO2eq', so we directly use the value 178.97 as the answer_value.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is found in the context snippet with ref_id 'luccioni2024'. The snippet mentions that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞'. The answer_unit is 'kg CO2eq', so we directly use the value 178.97 as the answer_value."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context suggests that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method. This is because TDP is the maximum power a GPU can draw, but in reality, it rarely reaches this maximum power due to other resource constraints. Therefore, using TDP for estimation can lead to overestimation.","0","is_blank","[""chung2025"", ""cottier2024"", ""ebert2024""]","is_blank","The dimmed bars behind each solid bar in Figure 3 show the estimated energy consumption based on the GPU’s Thermal Design Power (TDP) instead of measuring the real GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74].","The context suggests that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method. This is because TDP is the maximum power a GPU can draw, but in reality, it rarely reaches this maximum power due to other resource constraints. Therefore, using TDP for estimation can lead to overestimation."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. This information directly answers the question about the energy consumption comparison between GPT-4o mini and GPT-4o. The answer_unit is 'is_blank' for True or False questions, but since the question asks for a True or False answer, we will provide a 1 for True or 0 for False in the answer_value field.","0","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. This information directly answers the question about the energy consumption comparison between GPT-4o mini and GPT-4o. The answer_unit is 'is_blank' for True or False questions, but since the question asks for a True or False answer, we will provide a 1 for True or 0 for False in the answer_value field."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","","","queries/sec","[]","is_blank","is_blank",""
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context does not support the claim that the trend of AI developers directly disclosing environmental information for notable models continued to increase after 2022. In fact, it states that the introduction of increasingly commercial and proprietary models after 2022 triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","0","is_blank","[""luccioni2025c""]","is_blank","However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context does not support the claim that the trend of AI developers directly disclosing environmental information for notable models continued to increase after 2022. In fact, it states that the introduction of increasingly commercial and proprietary models after 2022 triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The Griggs et al. (2024) evaluation mentions the normalized on-demand hourly price for an H100 GPU, which was calculated by comparing RunPod's H100 cost to RunPod's A100-80G cost and then adjusting relative to the A100's price on major clouds. The answer_unit is USD per hour, so the answer_value should be a number in that unit.","7.516","USD per hour","[""griggs2024""]","[""https://example.com/griggs2024""]","Table 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100. On-demand Price ($/h) 0.7 1.01 3.67 7.5164","The Griggs et al. (2024) evaluation mentions the normalized on-demand hourly price for an H100 GPU, which was calculated by comparing RunPod's H100 cost to RunPod's A100-80G cost and then adjusting relative to the A100's price on major clouds. The answer_unit is USD per hour, so the answer_value should be a number in that unit."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","","","liters of water","[]","is_blank","is_blank",""
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the monthly on-demand rental cost of serving Llama2-70b at BF16 precision using 2 NVIDIA A100-80GB GPUs, which is over $5,200 per month. To estimate the hourly cost, we need to divide the monthly cost by the number of hours in a month, assuming 30 days. The answer_unit specifies that the answer should be in USD per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context provides the monthly on-demand rental cost of serving Llama2-70b at BF16 precision using 2 NVIDIA A100-80GB GPUs, which is over $5,200 per month. To estimate the hourly cost, we need to divide the monthly cost by the number of hours in a month, assuming 30 days. The answer_unit specifies that the answer should be in USD per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This directly answers the question and provides the necessary multiplier as per the answer_unit specification.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This directly answers the question and provides the necessary multiplier as per the answer_unit specification."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context does not provide the emissions-to-driving-distance ratio from a recent study.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide the emissions-to-driving-distance ratio from a recent study."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","To estimate the total energy consumed, we need to know the inference energy per query for the BLOOMz-7B model and the number of inferences per download. The context provides the inference energy for BLOOMz-7B as 1.0 × 10−4 kWh per query. However, it does not mention the number of downloads or the exact number of inferences per download. Given the information that every download resulted in 1 million inferences and assuming 606,096 downloads, we can estimate the total energy consumption.","60.6096","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","BLOOMz-7B inference energy: 1.0 × 10−4 kWh per query, Table 5.","To estimate the total energy consumed, we need to know the inference energy per query for the BLOOMz-7B model and the number of inferences per download. The context provides the inference energy for BLOOMz-7B as 1.0 × 10−4 kWh per query. However, it does not mention the number of downloads or the exact number of inferences per download. Given the information that every download resulted in 1 million inferences and assuming 606,096 downloads, we can estimate the total energy consumption."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context snippet from [ref_id=shen2024] mentions that JetMoE-8B was trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. This information directly answers the question about the number of H100 GPU hours consumed during the pre-training of the JetMoE-8B model.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context snippet from [ref_id=shen2024] mentions that JetMoE-8B was trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. This information directly answers the question about the number of H100 GPU hours consumed during the pre-training of the JetMoE-8B model."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This directly answers the question about the cost that the largest training runs will exceed by 2027. The answer_unit is USD, so the answer_value should be a number in USD.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This directly answers the question about the cost that the largest training runs will exceed by 2027. The answer_unit is USD, so the answer_value should be a number in USD."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context from [ref_id=patterson2021] states that OpenAI published the total number of floating point operations to train their model as 3.14E+23. This value is directly provided in the context and matches the expected format of FLOPS.","3.14e+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context from [ref_id=patterson2021] states that OpenAI published the total number of floating point operations to train their model as 3.14E+23. This value is directly provided in the context and matches the expected format of FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This information is used to determine the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This information is used to determine the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The total computational work performed during the final 101B training stage is estimated using the throughput data provided for this stage. The table shows that the single-GPU throughput for the 101B stage consistently exceeds 160 teraFLOPs/sec, with a utilization rate of at least 51.3%. Given that the training time for the 101B model is 26.54B tokens and assuming a constant throughput, we can estimate the total computational work.","4.23","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages.","The total computational work performed during the final 101B training stage is estimated using the throughput data provided for this stage. The table shows that the single-GPU throughput for the 101B stage consistently exceeds 160 teraFLOPs/sec, with a utilization rate of at least 51.3%. Given that the training time for the 101B model is 26.54B tokens and assuming a constant throughput, we can estimate the total computational work."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","","","percent","[]","is_blank","is_blank",""
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. This implies that each server has 8 A800 GPUs. To find the total number of GPUs, we multiply the number of servers by the number of GPUs per server. The answer_unit is 'GPUs', so we express the answer as a number of GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. This implies that each server has 8 A800 GPUs. To find the total number of GPUs, we multiply the number of servers by the number of GPUs per server. The answer_unit is 'GPUs', so we express the answer as a number of GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, which is 284,019 kg (626,155 lbs) of CO2. To find the equivalent in average American lifetimes, we can use the information from Table 1 in ref_id=strubell2019, which states that the CO2 emissions from an average American life is 36,156 lbs. Therefore, we can calculate the equivalent number of lifetimes by dividing the estimated CO2 emissions by the CO2 emissions of an average American life.","17","lifetimes","[""strubell2019"", ""luccioni2023""]","is_blank","Table 1 in ref_id=strubell2019 and the text in ref_id=luccioni2023","The context provides the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, which is 284,019 kg (626,155 lbs) of CO2. To find the equivalent in average American lifetimes, we can use the information from Table 1 in ref_id=strubell2019, which states that the CO2 emissions from an average American life is 36,156 lbs. Therefore, we can calculate the equivalent number of lifetimes by dividing the estimated CO2 emissions by the CO2 emissions of an average American life."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train. The answer_unit is 'hours', so we express the answer_value as a number in hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours to train. The answer_unit is 'hours', so we express the answer_value as a number in hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources. This information is used to determine the answer value in metric tons as specified by the answer_unit field.","47500","metric tons","[""amazon2023""]","[""https://www.amazon.com/""]","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources. This information is used to determine the answer value in metric tons as specified by the answer_unit field."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This information is used to determine the framework used to deploy large language models across multiple GPUs and nodes.","vllm library","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm""]","LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations.","The context mentions that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This information is used to determine the framework used to deploy large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides information about the energy consumption of various models, including the Llama 3.1 70B model, when deployed on single and double nodes. According to Table B2, the energy consumption for the Llama 3.1 70B model increases by a factor of 1.95 when deployed on two nodes instead of one. This value is obtained by dividing the energy consumed in the double-node configuration by the energy consumed in the single-node configuration.","1.95","multiplier","[""[ref_id=zschache2025]""]","is_blank","Table B2","The context provides information about the energy consumption of various models, including the Llama 3.1 70B model, when deployed on single and double nodes. According to Table B2, the energy consumption for the Llama 3.1 70B model increases by a factor of 1.95 when deployed on two nodes instead of one. This value is obtained by dividing the energy consumed in the double-node configuration by the energy consumed in the single-node configuration."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide information about the water consumption per ChatGPT user session in 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide information about the water consumption per ChatGPT user session in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes. This information is used to directly answer the question about the annual electricity use comparable to the number of U.S. homes for 700 million daily GPT-4o queries.","35000","homes","[""jegham2025""]","is_blank","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes.","The context states that even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes. This information is used to directly answer the question about the annual electricity use comparable to the number of U.S. homes for 700 million daily GPT-4o queries."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context provides information about Microsoft's water consumption increase between 2021 and 2022. According to the text, Microsoft reported a 34% increase in global water consumption during this period. The answer_unit is 'percent', so the answer_value should be a numeric value representing this percentage.","34","percent","[""luccioni2025a"", ""li2025b""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].","The context provides information about Microsoft's water consumption increase between 2021 and 2022. According to the text, Microsoft reported a 34% increase in global water consumption during this period. The answer_unit is 'percent', so the answer_value should be a numeric value representing this percentage."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which directly answers the question. The answer_unit is True or False, so the answer_value should be 0 for False. The ref_id is provided as evidence from the context.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which directly answers the question. The answer_unit is True or False, so the answer_value should be 0 for False. The ref_id is provided as evidence from the context."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is given in the context as 1.11. This value represents the ratio of the total energy needed by the datacenter to the energy used solely by computational devices, indicating the energy efficiency of the datacenter.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is given in the context as 1.11. This value represents the ratio of the total energy needed by the datacenter to the energy used solely by computational devices, indicating the energy efficiency of the datacenter."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides an estimate of the total number of GPT-4o queries in 2025, which is approximately 772 billion. This value is directly stated in the text and matches the expected unit of 'queries'.","772000000000","queries","[""jegham2025""]","is_blank","GPT-4o. To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20% from January to May 2025, reflecting the documented increase in ChatGPT’s weekly active user base from 300 million to 800 million between December 2024 and April 2025 [72]. This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context provides an estimate of the total number of GPT-4o queries in 2025, which is approximately 772 billion. This value is directly stated in the text and matches the expected unit of 'queries'."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite can be found in Table 3 of the provided context. The average score is listed as 53.0. The answer_unit is 'score', so the answer_value will be the numeric value of the score.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite can be found in Table 3 of the provided context. The average score is listed as 53.0. The answer_unit is 'score', so the answer_value will be the numeric value of the score."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide information on the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). However, it does not explicitly state they are fully exempt from reporting energy consumption. The AI Act's requirements and exemptions for open-source models need to be carefully considered. Given the information provided, the answer to whether open-source general-purpose AI models are fully exempt from reporting their energy consumption unless they pose systemic risk is not definitively supported by the context.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2).","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). However, it does not explicitly state they are fully exempt from reporting energy consumption. The AI Act's requirements and exemptions for open-source models need to be carefully considered. Given the information provided, the answer to whether open-source general-purpose AI models are fully exempt from reporting their energy consumption unless they pose systemic risk is not definitively supported by the context."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The US national datacenter average Power Usage Effectiveness (PUE) in 2020 is mentioned in the context as 1.59. This value is directly provided in the text and matches the expected format for answer_unit.","1.59","PUE","[""patterson2021""]","is_blank","The US national datacenter average in 2020 was 1.59.","The US national datacenter average Power Usage Effectiveness (PUE) in 2020 is mentioned in the context as 1.59. This value is directly provided in the text and matches the expected format for answer_unit."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides information about the KV Cache size growth for the OPT-2.7B model in response to increasing batch sizes. According to the text, when the batch size increases to 32, the KV Cache expands to 5.312GB. The answer_unit is specified as GB, so the answer_value should be a number in GB.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB.","The context provides information about the KV Cache size growth for the OPT-2.7B model in response to increasing batch sizes. According to the text, when the batch size increases to 32, the KV Cache expands to 5.312GB. The answer_unit is specified as GB, so the answer_value should be a number in GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provided by Cottier et al. (2024) states that for the four notable models studied in-depth, R&D staff costs (including equity) accounted for a certain percentage range of the total amortized cost. The answer_unit is specified as percent, so the answer_value should be a numeric range in percent. According to the context, the percentage range is 29% to 49%.","[29, 49]","percent","[""[cottier2024]""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context provided by Cottier et al. (2024) states that for the four notable models studied in-depth, R&D staff costs (including equity) accounted for a certain percentage range of the total amortized cost. The answer_unit is specified as percent, so the answer_value should be a numeric range in percent. According to the context, the percentage range is 29% to 49%."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the energy consumption and the runtime of the inference phase were measured by the CodeCarbon package. This information is used to determine the software package used for measuring energy consumption during inference runs.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","[""https://github.com/mlco2/codecarbon"", ""https://mlco2.github.io/codecarbon/methodology.html#power-usage""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context mentions that the energy consumption and the runtime of the inference phase were measured by the CodeCarbon package. This information is used to determine the software package used for measuring energy consumption during inference runs."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides information about an analysis of 100 news articles discussing ChatGPT's energy consumption. According to this analysis, 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. The answer_unit is 'percent', so the answer_value should be expressed as a percentage.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context provides information about an analysis of 100 news articles discussing ChatGPT's energy consumption. According to this analysis, 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. The answer_unit is 'percent', so the answer_value should be expressed as a percentage."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions a function that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is specifically named in the text, allowing for an exact match. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier or name from the context.","Compute Time Calibration Function","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context mentions a function that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is specifically named in the text, allowing for an exact match. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier or name from the context."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards tends to increase the energy costs of inference per response most overall for LLaMA-65B. This is evident in Figures 8 and 9, which show energy metrics in terms of responses from the 65B model. The answer_unit is '1' for True and '0' for False, so the answer_value will be 1 since the statement is true.","1","is_blank","[""samsi2024""]","is_blank","Figures 8 and 9 show energy metrics in terms of responses from the 65B model. Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall...","The context states that increasing the number of shards tends to increase the energy costs of inference per response most overall for LLaMA-65B. This is evident in Figures 8 and 9, which show energy metrics in terms of responses from the 65B model. The answer_unit is '1' for True and '0' for False, so the answer_value will be 1 since the statement is true."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Since the H100 GPU is 0.1% rare earth metal by mass and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s, we can estimate the water consumption for a single H100 GPU. Assuming an H100 is 0.1% rare earth metal by mass, the amount of rare earth material in an H100 is 0.00125 kg (0.1% of 125g). Therefore, the estimated water consumption for a single H100 GPU is 0.011 kL (11 kL/kg * 0.00125 kg * (1/63)).","0.011","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Since the H100 GPU is 0.1% rare earth metal by mass and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s, we can estimate the water consumption for a single H100 GPU. Assuming an H100 is 0.1% rare earth metal by mass, the amount of rare earth material in an H100 is 0.00125 kg (0.1% of 125g). Therefore, the estimated water consumption for a single H100 GPU is 0.011 kL (11 kL/kg * 0.00125 kg * (1/63))."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context does not clearly support an answer to whether under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities. However, it does mention that there is an proposal to remove the open-source exemption from reporting obligations, implying that currently they might not have to report. Given the lack of clear information, the answer defaults to is_blank.","0","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support an answer to whether under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities. However, it does mention that there is an proposal to remove the open-source exemption from reporting obligations, implying that currently they might not have to report. Given the lack of clear information, the answer defaults to is_blank."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippet from [ref_id=luccioni2025a] mentions that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This information directly answers the question about the percentage of PPAs purchased by these corporations. The answer_unit is percent, so the answer_value will be a numeric value representing the percentage.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]","The context snippet from [ref_id=luccioni2025a] mentions that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This information directly answers the question about the percentage of PPAs purchased by these corporations. The answer_unit is percent, so the answer_value will be a numeric value representing the percentage."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The total public health cost of U.S. data centers in 2023 can be found in the context, which mentions that the U.S. data centers have already resulted in a total public health cost of about $6.7 billion in 2023. The answer_unit is USD, so the answer_value should be expressed as a number in USD. The context provides the necessary information to determine the total public health cost.","6.7","USD","[""han2024""]","is_blank","Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 as a reference. Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","The total public health cost of U.S. data centers in 2023 can be found in the context, which mentions that the U.S. data centers have already resulted in a total public health cost of about $6.7 billion in 2023. The answer_unit is USD, so the answer_value should be expressed as a number in USD. The context provides the necessary information to determine the total public health cost."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","To find the factor by which the energy consumption of the o3 model is greater than that of GPT-4.1 nano for a long prompt, we need to compare their respective energy consumptions for long prompts. The energy consumption values for long prompts are given in the context as 12.222 Wh for o3 and 0.827 Wh for GPT-4.1 nano. We calculate the factor by dividing the energy consumption of o3 by that of GPT-4.1 nano.","14.7","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).","To find the factor by which the energy consumption of the o3 model is greater than that of GPT-4.1 nano for a long prompt, we need to compare their respective energy consumptions for long prompts. The energy consumption values for long prompts are given in the context as 12.222 Wh for o3 and 0.827 Wh for GPT-4.1 nano. We calculate the factor by dividing the energy consumption of o3 by that of GPT-4.1 nano."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that Mélange achieves 15-77% cost reduction for short-context tasks with a 120ms SLO. The answer_unit is percent, so the answer_value should be a range in percent. The evaluation of short-context workloads is based on the Arena dataset.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that Mélange achieves 15-77% cost reduction for short-context tasks with a 120ms SLO. The answer_unit is percent, so the answer_value should be a range in percent. The evaluation of short-context workloads is based on the Arena dataset."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the carbon footprint statistics of FLM-101B and other LLMs in Table 3. According to this table, the net carbon emissions for FLM-101B are 26 tCO2e. The answer_unit is tCO2e, which matches the unit provided in the context.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.","The context provides the carbon footprint statistics of FLM-101B and other LLMs in Table 3. According to this table, the net carbon emissions for FLM-101B are 26 tCO2e. The answer_unit is tCO2e, which matches the unit provided in the context."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU can be found in Figure 4.","6","seconds","[""xia2024""]","is_blank","Figure 4","The total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU can be found in Figure 4."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health costs of AI are not evenly distributed across communities in the U.S., with disadvantaged communities bearing a disproportionate share. This information is used to determine the answer value as 0, indicating False. The answer_unit is not applicable in this case as the question requires a True or False response.","0","is_blank","[""han2024""]","[""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html"", ""https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/""]","Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context clearly states that the public health costs of AI are not evenly distributed across communities in the U.S., with disadvantaged communities bearing a disproportionate share. This information is used to determine the answer value as 0, indicating False. The answer_unit is not applicable in this case as the question requires a True or False response."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context supports the answer through the information provided in the snippet from ref_id=wu2021b, which states that GPU theoretical performance per watt doubles every 3-4 years due to Moore’s law scaling and architectural optimization. The answer_unit is is_blank, but since the question is True or False, the answer_value should be 1 for True or 0 for False.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architec- tural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context supports the answer through the information provided in the snippet from ref_id=wu2021b, which states that GPU theoretical performance per watt doubles every 3-4 years due to Moore’s law scaling and architectural optimization. The answer_unit is is_blank, but since the question is True or False, the answer_value should be 1 for True or 0 for False."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama was used for local AI model deployment and supports a variety of pre-trained and fine-tuned models. It also specifically states that quantization was applied through Ollama, which aligns with the requirement for 4-bit quantization in the financial sentiment case study. Therefore, Ollama is the open-source tool that matches the description provided in the question.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context mentions that Ollama was used for local AI model deployment and supports a variety of pre-trained and fine-tuned models. It also specifically states that quantization was applied through Ollama, which aligns with the requirement for 4-bit quantization in the financial sentiment case study. Therefore, Ollama is the open-source tool that matches the description provided in the question."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that the WebDataset library was used to access datasets on-demand via shards in the tar format, which allows for streaming decompression, streaming preprocessing, and automatic local caching. This is suitable for spot VMs that can terminate at any time. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","WebDataset","is_blank","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","We access the datasets on-demand via shards in the tar format with the WebDataset library [10].","The context mentions that the WebDataset library was used to access datasets on-demand via shards in the tar format, which allows for streaming decompression, streaming preprocessing, and automatic local caching. This is suitable for spot VMs that can terminate at any time. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context provides information about the relationship between runtime and energy consumption in experiments with large language models. According to the text, 'the energy consumption during inference is shown to highly correlate with the model’s runtime,' which suggests a strong linear relationship. This information is used to determine the answer.","1","is_blank","[""zschache2025""]","is_blank","Additionally, the energy consumption during inference is shown to highly correlate with the model’s runtime.","The context provides information about the relationship between runtime and energy consumption in experiments with large language models. According to the text, 'the energy consumption during inference is shown to highly correlate with the model’s runtime,' which suggests a strong linear relationship. This information is used to determine the answer."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context mentions that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. The answer_unit is 'is_blank', which means the answer_value should be the exact identifier/name from the context that answers the question.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf""]","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [78].","The context mentions that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. The answer_unit is 'is_blank', which means the answer_value should be the exact identifier/name from the context that answers the question."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context does not provide information about Chen et al. (2025) or the price per hour for an NVIDIA H100 from this source. However, it does mention the price per hour for an NVIDIA H100 in the context of Chen et al. (2024) as $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications.","The context does not provide information about Chen et al. (2025) or the price per hour for an NVIDIA H100 from this source. However, it does mention the price per hour for an NVIDIA H100 in the context of Chen et al. (2024) as $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context does not provide a clear estimate of the average GPU lifetime in AI data centers in 2024. However, it mentions that internally, some assume a 4-year lifespan for GPUs, which leads to an embodied emissions calculation. This information is not directly related to the average GPU lifetime before retirement but is the closest relevant detail provided.","4","years","[""morrison2025""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","The context does not provide a clear estimate of the average GPU lifetime in AI data centers in 2024. However, it mentions that internally, some assume a 4-year lifespan for GPUs, which leads to an embodied emissions calculation. This information is not directly related to the average GPU lifetime before retirement but is the closest relevant detail provided."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides a table (TABLE II) that lists the bare minimum hardware requirements for different LLaMA models, including LLaMA-13B. According to this table, LLaMA-13B requires 2 NVIDIA V100 32GB GPUs to run inference without compression or quantization. The answer_unit is 'V100_32GB_GPUs', which matches the unit of the answer.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B","The context provides a table (TABLE II) that lists the bare minimum hardware requirements for different LLaMA models, including LLaMA-13B. According to this table, LLaMA-13B requires 2 NVIDIA V100 32GB GPUs to run inference without compression or quantization. The answer_unit is 'V100_32GB_GPUs', which matches the unit of the answer."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context provides the reported GHG emissions from Google's Gemma family of language models as 1247.61 tons CO2e, which exceeds the 'five cars' estimate. The 'five cars' estimate is mentioned as a basis for comparison, but its exact value is not provided in this specific snippet. However, it is stated that the reported emissions are over 4x the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number","The context provides the reported GHG emissions from Google's Gemma family of language models as 1247.61 tons CO2e, which exceeds the 'five cars' estimate. The 'five cars' estimate is mentioned as a basis for comparison, but its exact value is not provided in this specific snippet. However, it is stated that the reported emissions are over 4x the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. Since the question asks for the training time on 3 NVIDIA GTX 1080 Ti GPUs, we can assume that the training time would be similar. The answer_unit is specified as hours, so we express the answer_value as a number in hours.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. Since the question asks for the training time on 3 NVIDIA GTX 1080 Ti GPUs, we can assume that the training time would be similar. The answer_unit is specified as hours, so we express the answer_value as a number in hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The estimated U.S. national average water consumption for electricity generation is provided in the context as 3.1 L/kWh and also mentioned as 3.14 L/kWh in another part of the text. Given the 'answer_unit' is L/kWh, the answer should be a number in that unit. The context supports the answer by directly stating the values.","3.14","L/kWh","[""li2025b"", ""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively. ... our value of 3.14 L/kWh for the U.S. average water consumption factor is lower than 4.35 L/kWh reported by [1], as well as lower than Meta’s global electricity water consumption intensity factor of 3.70 L/kWh in 2024","The estimated U.S. national average water consumption for electricity generation is provided in the context as 3.1 L/kWh and also mentioned as 3.14 L/kWh in another part of the text. Given the 'answer_unit' is L/kWh, the answer should be a number in that unit. The context supports the answer by directly stating the values."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context provides the reported PUE of Google's hyperscale data centers in 2021 as 1.10. This value is directly mentioned in the snippet from ref_id=wu2021b. The answer_unit is PUE, which matches the unit of the provided value.","1.1","PUE","[""wu2021b"", ""dodge2022""]","[""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]","The context provides the reported PUE of Google's hyperscale data centers in 2021 as 1.10. This value is directly mentioned in the snippet from ref_id=wu2021b. The answer_unit is PUE, which matches the unit of the provided value."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This information directly answers the question about the expected reduction in carbon footprint. The answer_unit is 'percent', so the answer_value will be a numeric percentage.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that research shows AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This information directly answers the question about the expected reduction in carbon footprint. The answer_unit is 'percent', so the answer_value will be a numeric percentage."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. This information directly answers the question about the net cost, and the answer_unit is USD, which matches the unit provided in the context.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. This information directly answers the question about the net cost, and the answer_unit is USD, which matches the unit provided in the context."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context from [ref_id=dodge2022] and [ref_id=chung2025] supports the answer by stating that GPUs account for 50-70% of the total provisioned power in a datacenter. Applying the answer_unit 'percent', we find that the answer_value should be a percentage. However, since the context provides a range, we will use the lower and upper bounds of this range.","[50,70]","percent","[""dodge2022"", ""chung2025""]","is_blank","According to the U.S. Department of Energy and as mentioned in [ref_id=dodge2022] and [ref_id=chung2025], GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter.","The context from [ref_id=dodge2022] and [ref_id=chung2025] supports the answer by stating that GPUs account for 50-70% of the total provisioned power in a datacenter. Applying the answer_unit 'percent', we find that the answer_value should be a percentage. However, since the context provides a range, we will use the lower and upper bounds of this range."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that the training infrastructure for JetMoE-8B consisted of a cluster with 12 nodes and that inside each node, GPUs are connected via NVLinks and Infiniband is used for fast communication between nodes. It also mentions that the training was conducted on a cluster containing 12 nodes and 96 H100s. The answer_unit is H100 GPUs, so we need to express the answer as a number in that unit.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that the training infrastructure for JetMoE-8B consisted of a cluster with 12 nodes and that inside each node, GPUs are connected via NVLinks and Infiniband is used for fast communication between nodes. It also mentions that the training was conducted on a cluster containing 12 nodes and 96 H100s. The answer_unit is H100 GPUs, so we need to express the answer as a number in that unit."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context snippet from [ref_id=morrison2025] provides the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training. The answer_unit is specified as Watts, so the answer_value should be a number in Watts. According to the context, when actively training, the average GPU power is over 600W.","600","Watts","[""morrison2025""]","is_blank","Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B train- ing. The first spike is the beginning of training, and each drop happens when a model checkpoint is saved. When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context snippet from [ref_id=morrison2025] provides the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training. The answer_unit is specified as Watts, so the answer_value should be a number in Watts. According to the context, when actively training, the average GPU power is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","","","lbs","[]","is_blank","is_blank",""
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. This implies a speedup in inference throughput. The answer_unit is a multiplier, so we express the answer as a multiplier.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for LLaMA 13B, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. This implies a speedup in inference throughput. The answer_unit is a multiplier, so we express the answer as a multiplier."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides the energy consumption of a single short query to GPT-4o as 0.42 Wh, which matches the expected unit of Wh. This value is mentioned in multiple places in the context, including in the sections discussing the energy cost of a single GPT-4o user session and the estimated 2025 annual energy consumption of GPT-4o inference.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context provides the energy consumption of a single short query to GPT-4o as 0.42 Wh, which matches the expected unit of Wh. This value is mentioned in multiple places in the context, including in the sections discussing the energy cost of a single GPT-4o user session and the estimated 2025 annual energy consumption of GPT-4o inference."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, offering the lowest cost of $0.71 while providing 620.17 TPS, and Max-Performance selected g6e.xlarge, which provides the highest performance but at a cost of $2.699. This cost difference is used to calculate the percentage by which Max-Performance's choice is more expensive than InferSave's top choice.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, offering the lowest cost of $0.71 while providing 620.17 TPS, and Max-Performance selected g6e.xlarge, which provides the highest performance but at a cost of $2.699. This cost difference is used to calculate the percentage by which Max-Performance's choice is more expensive than InferSave's top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context mentions that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This information directly answers the question about the most energy-efficient GPU architecture for single-token classification models.","V100 or A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context mentions that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This information directly answers the question about the most energy-efficient GPU architecture for single-token classification models."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context mentions that Green AI involves reporting the computational price tag of finding, training, and running models, which includes the amount of compute used, but does not explicitly state that it involves providing the financial cost. However, it does mention that reporting the computational price tag is a key Green AI practice, and that price tags are baselines that other researchers could improve on. The answer_unit is '1' for True and '0' for False.","0","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context mentions that Green AI involves reporting the computational price tag of finding, training, and running models, which includes the amount of compute used, but does not explicitly state that it involves providing the financial cost. However, it does mention that reporting the computational price tag is a key Green AI practice, and that price tags are baselines that other researchers could improve on. The answer_unit is '1' for True and '0' for False."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that in 2022, the total amount of electronic waste generated worldwide was 62 million tonnes. The answer_unit is 'metric tons', so we need to convert tonnes to metric tons, but since 1 tonne is equal to 1 metric ton, the value remains the same.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context states that in 2022, the total amount of electronic waste generated worldwide was 62 million tonnes. The answer_unit is 'metric tons', so we need to convert tonnes to metric tons, but since 1 tonne is equal to 1 metric ton, the value remains the same."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The difference in power consumption is calculated based on the provided average power values for TPU v2 and V100 GPU in the context.","19","Watts","[""patterson2021""]","is_blank","The average power was 208 Watts for TPU v2 and 189 Watts for V100 GPU.","The difference in power consumption is calculated based on the provided average power values for TPU v2 and V100 GPU in the context."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is provided in the text and matches the expected unit of 'liters' specified in the answer_unit field.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context states that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is provided in the text and matches the expected unit of 'liters' specified in the answer_unit field."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that Facebook's recommendation and ranking model sizes have increased by 20× between 2019 and 2021. This information is provided in multiple snippets, including Figure 2(c) and the text accompanying it. The answer_unit is specified as a multiplier, so the answer_value should be a numeric multiplier.","20","multiplier","[""wu2021a""]","is_blank","Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].","The context states that Facebook's recommendation and ranking model sizes have increased by 20× between 2019 and 2021. This information is provided in multiple snippets, including Figure 2(c) and the text accompanying it. The answer_unit is specified as a multiplier, so the answer_value should be a numeric multiplier."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context provides information about a recent eco-efficiency analysis using DEA, where the model o3-mini achieved the highest cross-efficiency score. This suggests that o3-mini ranked highest in the analysis. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884)","The context provides information about a recent eco-efficiency analysis using DEA, where the model o3-mini achieved the highest cross-efficiency score. This suggests that o3-mini ranked highest in the analysis. The answer_unit is 'is_blank', indicating that the answer should be the exact identifier/name from the context that answers the question."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context mentions that smartphones currently average lifetimes of less than 3 years, which contributes to e-waste concerns. This information is found in the snippet with ref_id=wu2021b, where it states 'For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]'. The answer_unit is '1' for True or '0' for False, so the answer_value will be 1 since the statement is true.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]","The context mentions that smartphones currently average lifetimes of less than 3 years, which contributes to e-waste concerns. This information is found in the snippet with ref_id=wu2021b, where it states 'For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]'. The answer_unit is '1' for True or '0' for False, so the answer_value will be 1 since the statement is true."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context does not support the claim of nearly linear per-GPU speedup for CV models in intra-zone scaling with T4 GPUs. According to Figure 7, CV's per-GPU speedup is almost linear, but the actual values are 0.43, 0.42, 0.43, 0.41, and 0.41 for 2, 3, 4, 6, and 8 GPUs, respectively, which indicates a slight drop-off.","0","is_blank","[""erben2023""]","is_blank","Figure 7: (A) Intra-zone performance for CV and NLP.","The context does not support the claim of nearly linear per-GPU speedup for CV models in intra-zone scaling with T4 GPUs. According to Figure 7, CV's per-GPU speedup is almost linear, but the actual values are 0.43, 0.42, 0.43, 0.41, and 0.41 for 2, 3, 4, 6, and 8 GPUs, respectively, which indicates a slight drop-off."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context window size for the FLM-101B model is specified in the text as 2,048 tokens. This value is directly mentioned in the description of the FLM-101B model's structure. The answer_unit is 'tokens', which matches the unit provided in the context.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context window size for the FLM-101B model is specified in the text as 2,048 tokens. This value is directly mentioned in the description of the FLM-101B model's structure. The answer_unit is 'tokens', which matches the unit provided in the context."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth, which results in lower power draw during inference compared to diffusion models. This information is used to determine the answer to the question. The answer_unit is not applicable in this case as the question requires a True or False response.","1","is_blank","[""chung2025"", ""zschache2025""]","is_blank","This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58].","The context states that LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth, which results in lower power draw during inference compared to diffusion models. This information is used to determine the answer to the question. The answer_unit is not applicable in this case as the question requires a True or False response."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","","","percent","[]","is_blank","is_blank",""
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. when excluding equity. According to the text, the fraction of hardware cost rises to 61–76% when equity is excluded from R&D costs. The answer_unit is percent, so the answer_value will be expressed as a percentage range.","[61, 76]","percent","[""[ref_id=cottier2024]""]","is_blank","On the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while energy comprises only 2–6%. With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.","The context provides the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. when excluding equity. According to the text, the fraction of hardware cost rises to 61–76% when equity is excluded from R&D costs. The answer_unit is percent, so the answer_value will be expressed as a percentage range."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context does not support the claim that accuracy and F1 scores always improved after optimization. In fact, it states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context does not support the claim that accuracy and F1 scores always improved after optimization. In fact, it states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. The answer_unit is lbs/kWh, so we need to find the value in pounds per kilowatt-hour. According to the context, the EPA provides this value, but it is given in kg per KWh in some references and needs to be converted or directly stated in lbs/kWh in others.","0.936","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., and in another reference, it is stated that Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018.","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. The answer_unit is lbs/kWh, so we need to find the value in pounds per kilowatt-hour. According to the context, the EPA provides this value, but it is given in kg per KWh in some references and needs to be converted or directly stated in lbs/kWh in others."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a range of projected electricity consumption by the global AI in 2027. According to the study, this range is between 85 and 134 TWh. The answer_unit is specified as TWh, so the answer_value will be expressed in this unit.","[85,134]","TWh","[""[li2025b]""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7].","The context provides a range of projected electricity consumption by the global AI in 2027. According to the study, this range is between 85 and 134 TWh. The answer_unit is specified as TWh, so the answer_value will be expressed in this unit."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe. This information is found in the section 'Europe' under the discussion of electric delivery vehicles. The answer_unit is 'packages', which matches the unit provided in the context.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe. This information is found in the section 'Europe' under the discussion of electric delivery vehicles. The answer_unit is 'packages', which matches the unit provided in the context."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluation processed a total of 3000 requests, each with 128 input tokens and 512 output tokens. To find the total number of tokens processed, we need to multiply the number of requests by the total number of tokens per request. The answer_unit is 'tokens', so we will express the answer as a number in that unit.","1920000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","Workload Definition: For a holistic evaluation of InferSave’s ability to select the optimal instance in a variety of scenarios, we perform two contrasting inference workloads. • Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The online inference workload evaluation processed a total of 3000 requests, each with 128 input tokens and 512 output tokens. To find the total number of tokens processed, we need to multiply the number of requests by the total number of tokens per request. The answer_unit is 'tokens', so we will express the answer as a number in that unit."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context does not clearly support an answer to the question, as it discusses the AI Act's shortcomings in addressing greenhouse gas emissions from AI applications, but does not explicitly state that it mandates disclosure of such emissions. Therefore, the answer is '0' for False, as the context suggests the opposite.","0","is_blank","[""ebert2024""]","is_blank","5) The AI Act fails to address the greenhouse gas (GHG) emis- sions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context does not clearly support an answer to the question, as it discusses the AI Act's shortcomings in addressing greenhouse gas emissions from AI applications, but does not explicitly state that it mandates disclosure of such emissions. Therefore, the answer is '0' for False, as the context suggests the opposite."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context from the 2022 Dodge et al. paper provides the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region through the 'Flexible Start' optimization analysis. The answer_unit is specified as percent, so the answer_value should be a numeric percentage. According to the context, for very short experiments like DenseNet 201, which ran for less than half an hour, significant reduction greater than 30% in multiple regions and up to 80% in West US can be found.","80","percent","[""dodge2022""]","is_blank","Fig. 3. What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context from the 2022 Dodge et al. paper provides the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region through the 'Flexible Start' optimization analysis. The answer_unit is specified as percent, so the answer_value should be a numeric percentage. According to the context, for very short experiments like DenseNet 201, which ran for less than half an hour, significant reduction greater than 30% in multiple regions and up to 80% in West US can be found."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context from ref_id=luccioni2024 provides a comparison of energy consumption for different tasks, stating that image generation uses, on average, over 60 times more energy than text generation, and also comparing image generation to text classification. Given that text classification is the least energy-intensive task and image generation is the most energy-intensive, we can infer the factor by which image generation exceeds text classification based on the provided data. The mean consumption values are 2.9kWh for image generation and 0.002kWh for text classification, which also supports the calculation of this factor.","1450","times","[""luccioni2024""]","is_blank","We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context from ref_id=luccioni2024 provides a comparison of energy consumption for different tasks, stating that image generation uses, on average, over 60 times more energy than text generation, and also comparing image generation to text classification. Given that text classification is the least energy-intensive task and image generation is the most energy-intensive, we can infer the factor by which image generation exceeds text classification based on the provided data. The mean consumption values are 2.9kWh for image generation and 0.002kWh for text classification, which also supports the calculation of this factor."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which directly answers the question. The answer_unit is a boolean value (True or False), so the answer_value should be either 1 for True or 0 for False. Based on the context, traditional models did not achieve accuracy comparable to large language models, so the answer is False, which corresponds to an answer_value of 0.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which directly answers the question. The answer_unit is a boolean value (True or False), so the answer_value should be either 1 for True or 0 for False. Based on the context, traditional models did not achieve accuracy comparable to large language models, so the answer is False, which corresponds to an answer_value of 0."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects by location. The answer_unit is 'projects', so the answer_value should be the number of projects in the United States. According to the table, there are 244 projects in the United States.","244","projects","[""amazon2023""]","is_blank","Project Location: United States, Number of Projects: 244","The context provides a table of Amazon Renewable Energy Projects announced as of January 2024, which includes the number of projects by location. The answer_unit is 'projects', so the answer_value should be the number of projects in the United States. According to the table, there are 244 projects in the United States."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","","","percent","[]","is_blank","is_blank",""
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend that AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility. This is stated in the context as 'Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.' The answer_unit is 'is_blank' because the question asks for the measurement level, which is an identifier/name from the context.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.","The authors recommend that AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility. This is stated in the context as 'Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.' The answer_unit is 'is_blank' because the question asks for the measurement level, which is an identifier/name from the context."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context provides information about an experiment to train a BERT-base model, where the electricity consumption of different components was measured. According to Table 1, the GPU accounted for 74% of the total electricity consumption. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context provides information about an experiment to train a BERT-base model, where the electricity consumption of different components was measured. According to Table 1, the GPU accounted for 74% of the total electricity consumption. The answer_unit is 'percent', so the answer_value should be a numeric value representing the percentage."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from [ref_id=griggs2024] states that serving a Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit is 'NVIDIA A100-80GB GPUs', which is a unit, so the answer_value should be a number in that unit.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context from [ref_id=griggs2024] states that serving a Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit is 'NVIDIA A100-80GB GPUs', which is a unit, so the answer_value should be a number in that unit."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context mentions that Facebook achieved a 28.5% operational energy footprint reduction over a two-year period due to iterative hardware-software optimization. This is stated in Figure 8 and supported by the discussion on optimization across AI model development and system stack over time. The answer_unit is percent, so the answer_value will be the numeric value of the reduction.","28.5","percent","[""wu2021a""]","is_blank","Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context mentions that Facebook achieved a 28.5% operational energy footprint reduction over a two-year period due to iterative hardware-software optimization. This is stated in Figure 8 and supported by the discussion on optimization across AI model development and system stack over time. The answer_unit is percent, so the answer_value will be the numeric value of the reduction."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide information about the installation of fiber optic cables to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not provide information about the installation of fiber optic cables to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides the estimated upfront hardware acquisition cost to train GPT-4 as $800M. This value is directly mentioned in the text and matches the expected unit of USD. The answer is supported by the ref_id 'cottier2024'.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context provides the estimated upfront hardware acquisition cost to train GPT-4 as $800M. This value is directly mentioned in the text and matches the expected unit of USD. The answer is supported by the ref_id 'cottier2024'."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context does not clearly support the idea that 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications. In fact, the term seems to be used more broadly to discuss the environmental implications, challenges, and opportunities of AI. Therefore, the answer to the question is False, but since the question asks for a True or False answer and the context does not explicitly define 'Sustainable AI' in the proposed manner, the answer_value should reflect the False outcome as '0'.","0","is_blank","[""luccioni2025a""]","is_blank","is_blank","The context does not clearly support the idea that 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications. In fact, the term seems to be used more broadly to discuss the environmental implications, challenges, and opportunities of AI. Therefore, the answer to the question is False, but since the question asks for a True or False answer and the context does not explicitly define 'Sustainable AI' in the proposed manner, the answer_value should reflect the False outcome as '0'."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","","","samples","[]","is_blank","is_blank",""
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context from chung2025 states that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests, whereas Swapping copies data without running computation. The answer_unit is 'is_blank', indicating the answer should be the exact identifier/name from the context.","Swapping","is_blank","[""chung2025""]","is_blank","Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.","The context from chung2025 states that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests, whereas Swapping copies data without running computation. The answer_unit is 'is_blank', indicating the answer should be the exact identifier/name from the context."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context mentions Google's 2024 environmental report, which states that the company reported a 48% increase in GHG emissions since 2019. This information is used to answer the question, and the answer_unit is 'percent', so the answer_value will be the numeric value of the percentage increase.","48","percent","[""luccioni2025a"", ""luccioni2025b""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf"", ""https://sustainability.google/reports/""]","Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption”","The context mentions Google's 2024 environmental report, which states that the company reported a 48% increase in GHG emissions since 2019. This information is used to answer the question, and the answer_unit is 'percent', so the answer_value will be the numeric value of the percentage increase."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context does not directly provide McKinsey's projections for 2030, but it mentions data center energy consumption projections from other sources.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not directly provide McKinsey's projections for 2030, but it mentions data center energy consumption projections from other sources."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides information about the 'Pause and Resume' optimization for training of the 6B parameter transformer. According to Figure 4, the maximum potential emissions saving is up to 25% when doubling the duration. The answer_unit is 'percent', so the answer_value should be a number in that unit.","25","percent","[""dodge2022""]","is_blank","Figure 4: What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For short experiments, the doubled duration is still relatively short, and thus leads to minimal emissions reduction (see DenseNet 201 in (a)); for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context provides information about the 'Pause and Resume' optimization for training of the 6B parameter transformer. According to Figure 4, the maximum potential emissions saving is up to 25% when doubling the duration. The answer_unit is 'percent', so the answer_value should be a number in that unit."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This information is used directly to answer the question, and the answer_unit 'percent' indicates that the answer should be expressed as a percentage.","70","percent","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This information is used directly to answer the question, and the answer_unit 'percent' indicates that the answer should be expressed as a percentage."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The carbon footprint of training the BERT large language model is quantified in the provided context snippets. According to the snippet from [ref_id=luccioni2025b], the carbon footprint of training BERT was quantified as reaching 626,155 pounds of emissions. This value matches the expected unit of lbs CO2e specified in the additional info.","626155","lbs CO2e","[""luccioni2025b"", ""strubell2019""]","is_blank","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.","The carbon footprint of training the BERT large language model is quantified in the provided context snippets. According to the snippet from [ref_id=luccioni2025b], the carbon footprint of training BERT was quantified as reaching 626,155 pounds of emissions. This value matches the expected unit of lbs CO2e specified in the additional info."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The estimated training energy of the full GPT-3 model is provided in the context snippet from ref_id=li2025b. The answer_unit is specified as MWh, which matches the unit given in the context. Therefore, the answer_value can be directly extracted from the context.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The estimated training energy of the full GPT-3 model is provided in the context snippet from ref_id=li2025b. The answer_unit is specified as MWh, which matches the unit given in the context. Therefore, the answer_value can be directly extracted from the context."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context supports the answer by stating that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and that optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning. This indicates that the MoE layer is a targeted layer when enhancing performance of LLM fine-tuning. The answer_unit is '1' for True and '0' for False.","1","is_blank","[""xia2024""]","is_blank","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context supports the answer by stating that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and that optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning. This indicates that the MoE layer is a targeted layer when enhancing performance of LLM fine-tuning. The answer_unit is '1' for True and '0' for False."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (TABLE III) that lists the maximum batch size supported by different model and dataset combinations. For Mixtral with a dense setup on the Hellaswag dataset, the maximum batch size is given as 2. The answer_unit is specified as ""samples"", which matches the unit used in the table.","2","samples","[""[ref_id=xia2024]""]","is_blank","TABLE III","The context provides a table (TABLE III) that lists the maximum batch size supported by different model and dataset combinations. For Mixtral with a dense setup on the Hellaswag dataset, the maximum batch size is given as 2. The answer_unit is specified as ""samples"", which matches the unit used in the table."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training. This information is found in the section discussing intercontinental scalability. The answer_unit is not applicable in this case as the question requires a True or False response.","1","is_blank","[""[ref_id=erben2023]""]","is_blank","While these experiments were specifically designed to be a worst-case scenario, what about a more balanced GPU distribution with at least two GPUs in each region? ... However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). Finally, each additional region introduces a constant penalty that is not amortized by adding local hardware, which should be considered when running geo-distributed training setups.","The context states that for CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training. This information is found in the section discussing intercontinental scalability. The answer_unit is not applicable in this case as the question requires a True or False response."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","","","hectares","[]","is_blank","is_blank",""
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides a comparison of CO2eq emissions between task-specific and general-purpose models. The answer_unit is 'g CO2eq', which requires the answer_value to be a number in grams of CO2eq. The context states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries","The context provides a comparison of CO2eq emissions between task-specific and general-purpose models. The answer_unit is 'g CO2eq', which requires the answer_value to be a number in grams of CO2eq. The context states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context from [ref_id=dodge2022] mentions that there is a large variation between the least carbon-intensive regions and the most carbon-intensive regions, indicating that choosing the region in which experiments run can be very impactful. The range of emissions is given as 7k grams vs. 26k grams for the most efficient vs. least efficient regions. Applying the answer_unit 'grams', we find the range is approximately between 7,000 and 26,000 grams.","[7000, 26000]","grams","[""dodge2022""]","is_blank","Fig. 1 and text: '...indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).'","The context from [ref_id=dodge2022] mentions that there is a large variation between the least carbon-intensive regions and the most carbon-intensive regions, indicating that choosing the region in which experiments run can be very impactful. The range of emissions is given as 7k grams vs. 26k grams for the most efficient vs. least efficient regions. Applying the answer_unit 'grams', we find the range is approximately between 7,000 and 26,000 grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This information is used to determine the answer. The answer_unit is 'year', so the answer_value will be the year in which the peak occurred.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This information is used to determine the answer. The answer_unit is 'year', so the answer_value will be the year in which the peak occurred."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table with estimated power usage, carbon emissions, and water consumption from training various models, including the OLMo 60M model. According to the table, the water consumption for the OLMo 60M model is 1.6 kL, which is equivalent to 5 days of water usage for one person in the US. The answer_unit is 'days', so we need to express the answer as a number in days.","5","days","[""morrison2025""]","[""https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator"", ""https://www.epa.gov/watersense/statistics-and-facts""]","Table 2: ... OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table with estimated power usage, carbon emissions, and water consumption from training various models, including the OLMo 60M model. According to the table, the water consumption for the OLMo 60M model is 1.6 kL, which is equivalent to 5 days of water usage for one person in the US. The answer_unit is 'days', so we need to express the answer as a number in days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","","","liters of freshwater","[]","is_blank","is_blank",""
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can further reduce cost based on the study, which implies that it does not increase costs but rather decreases them. The answer_unit is '1' for True and '0' for False, so the answer_value should be '0' since the statement is False.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate the MoE layers can further reduce cost based on the study, which implies that it does not increase costs but rather decreases them. The answer_unit is '1' for True and '0' for False, so the answer_value should be '0' since the statement is False."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context provides the total energy consumption for training the FLM-101B model in the section discussing the carbon footprint analysis. The answer_unit is specified as kWh, so the answer_value should be a number in that unit. According to the context, the total energy consumption for training the FLM-101B model is 40 MkWh, which is equivalent to 40,000 kWh.","40000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. ... Energy (MkWh) ... FLM-101B ... 40","The context provides the total energy consumption for training the FLM-101B model in the section discussing the carbon footprint analysis. The answer_unit is specified as kWh, so the answer_value should be a number in that unit. According to the context, the total energy consumption for training the FLM-101B model is 40 MkWh, which is equivalent to 40,000 kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The question asks for the projected total public health burden of U.S. data centers in 2030. However, the provided context does not contain information about projections for 2030. It does mention projections for 2028, stating that the total public health burden could reach more than $20 billion. Since there is no information for 2030, the answer cannot be directly determined from the given context.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The question asks for the projected total public health burden of U.S. data centers in 2030. However, the provided context does not contain information about projections for 2030. It does mention projections for 2028, stating that the total public health burden could reach more than $20 billion. Since there is no information for 2030, the answer cannot be directly determined from the given context."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU can be found in Table IV of the provided context. The table lists the estimated cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate. According to the table, the estimated cost for fine-tuning Mixtral on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU is $32.7. The answer_unit is USD, so the answer_value should be expressed as a number in USD.","32.7","USD","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","TABLE IV: ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL","The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU can be found in Table IV of the provided context. The table lists the estimated cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate. According to the table, the estimated cost for fine-tuning Mixtral on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU is $32.7. The answer_unit is USD, so the answer_value should be expressed as a number in USD."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context does not provide explicit information about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB. However, it does mention that the maximum batch size supported by Mixtral on A40 GPU is 4, as shown in Table IV.","4","samples","[""[ref_id=xia2024]""]","is_blank","TABLE IV ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7","The context does not provide explicit information about the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB. However, it does mention that the maximum batch size supported by Mixtral on A40 GPU is 4, as shown in Table IV."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","","","seconds","[]","is_blank","is_blank",""
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context suggests that GPU-level power consumption monitoring is a common and recommended method for reporting AI energy use, as it is a significant contributor to overall energy consumption. The answer_unit is '1' for True and '0' for False, indicating a boolean response. Based on the context, the correct answer is True, as GPU-level power consumption monitoring is indeed recommended.","1","is_blank","[""dodge2022"", ""schwartz2019"", ""strubell2019""]","[""https://bit.ly/30sGEbi"", ""https://bit.ly/2LObQhV"", ""https://bit.ly/2JTbGnI""]","Table 1, 'The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB)'","The context suggests that GPU-level power consumption monitoring is a common and recommended method for reporting AI energy use, as it is a significant contributor to overall energy consumption. The answer_unit is '1' for True and '0' for False, indicating a boolean response. Based on the context, the correct answer is True, as GPU-level power consumption monitoring is indeed recommended."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in a 2023 article estimating the carbon footprint of the BLOOM model, Luccioni et al. proposed using a Life Cycle Assessment approach and found that training accounted for only half of the model's overall emissions. This information directly answers the question about the percentage of overall emissions attributed to training. The answer_unit is percent, so the answer_value should be a numeric percentage.","50","percent","[""luccioni2025b""]","is_blank","meaning that training accounted for only half of the model’s overall emissions [121]","The context states that in a 2023 article estimating the carbon footprint of the BLOOM model, Luccioni et al. proposed using a Life Cycle Assessment approach and found that training accounted for only half of the model's overall emissions. This information directly answers the question about the percentage of overall emissions attributed to training. The answer_unit is percent, so the answer_value should be a numeric percentage."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides the bare minimum hardware requirements for different LLaMA variants in Table II. According to this table, LLaMA-7B requires at least 1 NVIDIA V100 32GB GPU to run inference without compression or quantization. The answer_unit is 'V100_32GB_GPU', which specifies that the answer should be the number of V100 32GB GPUs required.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B","The context provides the bare minimum hardware requirements for different LLaMA variants in Table II. According to this table, LLaMA-7B requires at least 1 NVIDIA V100 32GB GPU to run inference without compression or quantization. The answer_unit is 'V100_32GB_GPU', which specifies that the answer should be the number of V100 32GB GPUs required."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides information about the water consumption of GPT-3 in different locations, including Arizona. According to Table 1 in ref_id=li2025b, the water consumption per request for GPT-3 in Arizona is approximately 6.52 mL. To find out how many requests it would take to consume a 500ml bottle of water, we can divide 500ml by the water consumption per request.","76.6","requests","[""[li2025b]""]","is_blank","Table 1 in ref_id=li2025b","The context provides information about the water consumption of GPT-3 in different locations, including Arizona. According to Table 1 in ref_id=li2025b, the water consumption per request for GPT-3 in Arizona is approximately 6.52 mL. To find out how many requests it would take to consume a 500ml bottle of water, we can divide 500ml by the water consumption per request."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The estimated CO2 emission in metric tons for one year of average US home energy use is provided in the context snippet from [ref_id=dodge2022]. The answer_unit is 'metric tons', so the answer_value should be a number in that unit. The context states that the average US home emits 8.3 metric tons CO2 per year.","8.3","metric tons","[""dodge2022""]","is_blank","which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year.","The estimated CO2 emission in metric tons for one year of average US home energy use is provided in the context snippet from [ref_id=dodge2022]. The answer_unit is 'metric tons', so the answer_value should be a number in that unit. The context states that the average US home emits 8.3 metric tons CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides the score achieved by the JetMoE-8B model on the GSM8k benchmark in Table 3. The answer_unit is 'score', so the answer_value should be the numeric score. The score is listed as 27.8 for JetMoE-8B.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... GSM8k 14.5 17.3 16.9 27.8","The context provides the score achieved by the JetMoE-8B model on the GSM8k benchmark in Table 3. The answer_unit is 'score', so the answer_value should be the numeric score. The score is listed as 27.8 for JetMoE-8B."
