"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet from ref_id=wu2021b states that the total energy consumption of US data centers increased by about 4% from 2010-2014. The answer_unit is 'percent', so the answer_value is 4.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context snippet from ref_id=wu2021b states that the total energy consumption of US data centers increased by about 4% from 2010-2014. The answer_unit is 'percent', so the answer_value is 4."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context snippet [ref_id=amazon2023] states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, equivalent to taking more than 13,900 cars off the road. The answer_unit is 'cars', so the numeric value 13,900 is extracted directly from the context.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context snippet [ref_id=amazon2023] states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, equivalent to taking more than 13,900 cars off the road. The answer_unit is 'cars', so the numeric value 13,900 is extracted directly from the context."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. The answer_unit is 'data centers', so the answer_value is the numeric difference (24 - 20 = 4). However, the question asks how many AWS began using in 2023, which is the increase of 4 data centers. The context explicitly mentions this increase.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. The answer_unit is 'data centers', so the answer_value is the numeric difference (24 - 20 = 4). However, the question asks how many AWS began using in 2023, which is the increase of 4 data centers. The context explicitly mentions this increase."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context from morrison2025 states that they assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equating to 463 kg per GPU. This directly provides the numeric value in kg per GPU as required by the answer_unit.","463","kg/GPU","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/""]","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","The context from morrison2025 states that they assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equating to 463 kg per GPU. This directly provides the numeric value in kg per GPU as required by the answer_unit."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that FLM-101B was trained within a $100,000 budget (li2025a). The estimated amortized hardware CapEx + energy cost for GPT-4 is $40M (cottier2024). The ratio is $40,000,000 / $100,000 = 400.","400","ratio","[""li2025a"", ""cottier2024""]","is_blank","['FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.', 'For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.']","The context states that FLM-101B was trained within a $100,000 budget (li2025a). The estimated amortized hardware CapEx + energy cost for GPT-4 is $40M (cottier2024). The ratio is $40,000,000 / $100,000 = 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context explicitly states multiple times that a single passenger round trip SF-NY is approximately 1.2t CO2e. For example, one snippet mentions 'a single passenger round trip SF-NY is ~1.2t CO2e (Table 2)' and another provides the calculation details leading to this value. The answer_unit 'tCO2e' matches the unit provided in the context, so the numeric value is directly used.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context explicitly states multiple times that a single passenger round trip SF-NY is approximately 1.2t CO2e. For example, one snippet mentions 'a single passenger round trip SF-NY is ~1.2t CO2e (Table 2)' and another provides the calculation details leading to this value. The answer_unit 'tCO2e' matches the unit provided in the context, so the numeric value is directly used."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context snippets from ref_id=li2025a mention that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. The answer_unit 'score' requires the numeric value, which is directly provided in the context.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context snippets from ref_id=li2025a mention that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. The answer_unit 'score' requires the numeric value, which is directly provided in the context."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context explicitly states that there was a 'more than 6,750 fold improvement in processor clock speed' between 1971 and 2021 microprocessors. The answer_unit 'fold' matches the required format, so we directly extract the numeric value from the context.","6750","fold","[""wu2021b""]","is_blank","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context explicitly states that there was a 'more than 6,750 fold improvement in processor clock speed' between 1971 and 2021 microprocessors. The answer_unit 'fold' matches the required format, so we directly extract the numeric value from the context."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context snippet from ref_id=patterson2021 explicitly states that it took ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute the required FLOPS for GPT-3. The answer_unit is 'days', so the numeric value is provided.","14.8","days","[""patterson2021""]","is_blank","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS. For the CO2e calculation, it doesn’t actually matter whether it takes 2 weeks on 10,000 GPUs or 20 weeks on 1,000 GPUs, but we need one number for Table 4, so we used NVIDIA’s suggestion of 10,000 GPUs.","The context snippet from ref_id=patterson2021 explicitly states that it took ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute the required FLOPS for GPT-3. The answer_unit is 'days', so the numeric value is provided."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context snippet from ref_id=morrison2025 includes a table (Table 4) that lists GPU Power Usage in kWh for various models at different request frequencies. For Llama 3.2 1B at 8 req/s, the GPU Power Usage is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: [...] Llama 3.2 1B [...] 8 [...] 0.036 [...]","The context snippet from ref_id=morrison2025 includes a table (Table 4) that lists GPU Power Usage in kWh for various models at different request frequencies. For Llama 3.2 1B at 8 req/s, the GPU Power Usage is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. The answer_unit is 'tons', so the numeric value is provided.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. The answer_unit is 'tons', so the numeric value is provided."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that using the growth strategy resulted in a 72% time-saving compared to training from scratch. The answer_unit 'percent' requires a numerical value without the percentage sign, so 72 is the correct answer_value.","72","percent","[""li2025a""]","[""https://arxiv.org/abs/2309.03852v3""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that using the growth strategy resulted in a 72% time-saving compared to training from scratch. The answer_unit 'percent' requires a numerical value without the percentage sign, so 72 is the correct answer_value."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context mentions that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-1 and scope-2 pollutants. However, there is no specific projection provided for 2030 in the provided context snippets.","is_blank","deaths","[""han2024""]","is_blank","our analysis demonstrates that driven by the growing demand for AI, the U.S. data centers could contribute to, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths in 2028, exceeding 1/3 of asthma deaths in the U.S. each year [40].","The context mentions that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-1 and scope-2 pollutants. However, there is no specific projection provided for 2030 in the provided context snippets."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context snippet from ref_id=dodge2022 states that the 6 billion parameter model training run took 8 days and was only 13% complete. The full training run is estimated by multiplying 8 days by (100/13), which results in approximately 61.5 days. However, the context directly mentions that a full training run would take 60 days, aligning with the answer_unit of days.","60","days","[""dodge2022""]","[""https://doi.org/10.1145/3531146.3533234""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. [...] we estimate a full training run would consume approximately 103,593 kWh. [...] we estimate a full training run would take 60 days.","The context snippet from ref_id=dodge2022 states that the 6 billion parameter model training run took 8 days and was only 13% complete. The full training run is estimated by multiplying 8 days by (100/13), which results in approximately 61.5 days. However, the context directly mentions that a full training run would take 60 days, aligning with the answer_unit of days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context snippets from ref_id=luccioni2025c's Table 2 list models with their GPU energy consumption for 1k queries. The highest value shown is 3,426.38 Wh for Command-R Plus by Cohere.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","Command-R Plus Cohere 3,426.38 Text Generation","The context snippets from ref_id=luccioni2025c's Table 2 list models with their GPU energy consumption for 1k queries. The highest value shown is 3,426.38 Wh for Command-R Plus by Cohere."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. The answer_unit 'year' requires extracting the numeric year from the date mentioned.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. The answer_unit 'year' requires extracting the numeric year from the date mentioned."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context explicitly states that the UN's Global E-Waste Monitor 2024 reported 22% of e-waste being formally collected and recycled. The answer_unit 'percent' requires a numerical value without the percentage symbol.","22","percent","[""luccioni2025a""]","is_blank","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context explicitly states that the UN's Global E-Waste Monitor 2024 reported 22% of e-waste being formally collected and recycled. The answer_unit 'percent' requires a numerical value without the percentage symbol."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context mentions that the final training run for the 176B model used 37.24% of the energy of the BLOOM project, which was 3,500 MWh according to [ref_id=luccioni2025c]. This value is converted to MWh as required.","3500","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context mentions that the final training run for the 176B model used 37.24% of the energy of the BLOOM project, which was 3,500 MWh according to [ref_id=luccioni2025c]. This value is converted to MWh as required."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context snippet from ref_id=patterson2021 states that the Switch Transformer model has 1500B parameters with only 0.1% activated per token. The answer_unit 'percent' requires converting 0.1% to a percentage value.","0.1","percent","[""patterson2021""]","is_blank","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context snippet from ref_id=patterson2021 states that the Switch Transformer model has 1500B parameters with only 0.1% activated per token. The answer_unit 'percent' requires converting 0.1% to a percentage value."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context snippet from ref_id=shen2024 states that JetMoE-8B uses 8 experts in each MoE layer, as shown in the hyperparameters table (Nexperts=8). The answer_unit 'experts' requires a numeric value, which is directly provided in the context.","8","experts","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632","The context snippet from ref_id=shen2024 states that JetMoE-8B uses 8 experts in each MoE layer, as shown in the hyperparameters table (Nexperts=8). The answer_unit 'experts' requires a numeric value, which is directly provided in the context."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context mentions Figure 4's execution time breakdown for Mixtral and BlackMamba. For BlackMamba with a batch size of 30 (dense setup), the execution time is shown as approximately 1.5 seconds in the 'Dense(bsz=30)' bar under the Mamba section of Figure 4.","1.5","second","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)
0.0
0.5
1.0
1.5
2.0
    Execution Time
Breakdown (seconds)
Mixtral
Mamba
Fig. 4. Execution time breakdown.","The context mentions Figure 4's execution time breakdown for Mixtral and BlackMamba. For BlackMamba with a batch size of 30 (dense setup), the execution time is shown as approximately 1.5 seconds in the 'Dense(bsz=30)' bar under the Mamba section of Figure 4."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context explicitly states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated to the English portion. The answer_unit 'zettaFLOPs' requires a numerical value, which is directly provided in the context.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context explicitly states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated to the English portion. The answer_unit 'zettaFLOPs' requires a numerical value, which is directly provided in the context."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The question asks for the hardware processor used in the experimental setup for energy-efficient local inference in financial sentiment classification. The context from ref_id=khan2025 states that the hardware used includes an '11th Gen Intel(R) Core(TM) i7-1165G7 processor' in their experimental setup. This matches the required answer format (exact identifier/name from context).","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The question asks for the hardware processor used in the experimental setup for energy-efficient local inference in financial sentiment classification. The context from ref_id=khan2025 states that the hardware used includes an '11th Gen Intel(R) Core(TM) i7-1165G7 processor' in their experimental setup. This matches the required answer format (exact identifier/name from context)."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context snippet from ref_id=luccioni2024 lists 80 finetuned models tested across ten tasks in Table 6, which directly answers the question about the number of models analyzed.","80","models","[""luccioni2024""]","is_blank","Table 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.","The context snippet from ref_id=luccioni2024 lists 80 finetuned models tested across ten tasks in Table 6, which directly answers the question about the number of models analyzed."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context snippet from wu2021a explicitly states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. The answer_unit 'multiplier' requires expressing the reduction factor as a numeric value, which is 3 in this case.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context snippet from wu2021a explicitly states that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. The answer_unit 'multiplier' requires expressing the reduction factor as a numeric value, which is 3 in this case."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context from Cottier et al. (2024) states that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. The answer_unit 'multiplier' indicates the answer should be a numeric range based on the provided context.","[1.2, 4]","multiplier","[""cottier2024""]","is_blank","we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.","The context from Cottier et al. (2024) states that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. The answer_unit 'multiplier' indicates the answer should be a numeric range based on the provided context."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that the 6 billion parameter transformer model's full training run is estimated to consume approximately 103.5 MWh, as calculated from (60/8) * 13.8 MWh. The answer_unit is MWh, so the numeric value 103.5 is provided.","103.5","MWh","[""dodge2022""]","is_blank","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that the 6 billion parameter transformer model's full training run is estimated to consume approximately 103.5 MWh, as calculated from (60/8) * 13.8 MWh. The answer_unit is MWh, so the numeric value 103.5 is provided."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context snippets from luccioni2025a repeatedly mention Jevons' Paradox as the economic principle explaining why AI efficiency gains may lead to increased consumption rather than environmental benefits. The paper's title and abstract explicitly frame their argument around this principle.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","is_blank","This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The context snippets from luccioni2025a repeatedly mention Jevons' Paradox as the economic principle explaining why AI efficiency gains may lead to increased consumption rather than environmental benefits. The paper's title and abstract explicitly frame their argument around this principle."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. The answer_unit specifies 'billion cubic meters', so the answer_value is formatted as a numeric range matching this unit.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. The answer_unit specifies 'billion cubic meters', so the answer_value is formatted as a numeric range matching this unit."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the assertion in the question that Red AI is on the decline. The answer_value must be '0' (False) as per the True/False format.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the assertion in the question that Red AI is on the decline. The answer_value must be '0' (False) as per the True/False format."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This directly answers the question about the wall-clock time required, and the answer_unit 'days' matches the provided value.","21.54","days","[""li2025a""]","[""https://arxiv.org/abs/2309.03852v3""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This directly answers the question about the wall-clock time required, and the answer_unit 'days' matches the provided value."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, which directly contradicts the claim of over 80% utilization.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements. [ref_id=wu2021a]","The context states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, which directly contradicts the claim of over 80% utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippets from ref_ids jegham2025 and li2025b both state that the training of GPT-3 is estimated to consume 1,287 MWh of electricity. The answer_unit is MWh, so the numeric value is directly provided.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","[ref_id=jegham2025] Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity... [ref_id=li2025b] GPT-3 was trained... with an estimated training energy of 1287 MWh [29].","The context snippets from ref_ids jegham2025 and li2025b both state that the training of GPT-3 is estimated to consume 1,287 MWh of electricity. The answer_unit is MWh, so the numeric value is directly provided."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score 21' project multiple times, explicitly stating it aims to establish a unified approach for comparing inference efficiency of AI models. The question asks for the name, which matches the answer_unit 'is_blank' requiring the exact identifier from context.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the 'AI Energy Score 21' project multiple times, explicitly stating it aims to establish a unified approach for comparing inference efficiency of AI models. The question asks for the name, which matches the answer_unit 'is_blank' requiring the exact identifier from context."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context includes Figure 6 from ref_id=xia2024, which shows the execution breakdown of the MoE layer for different kernels. For BlackMamba (Mamba) with a batch size of 30 (Sparse(bsz=30)), the longest kernel execution time is approximately 2000 microseconds for matmul(w2).","2000","microseconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0
400
800
1200
1600
2000
matmul(w1)
gelu
matmul(w2)
elementwise_mult
top_k
sigmoid
matmul(router)
    Execution Time Breakdown (μs)
Mixtral
Mamba
Fig. 6. Execution breakdown of the MoE layer for different kernels.","The context includes Figure 6 from ref_id=xia2024, which shows the execution breakdown of the MoE layer for different kernels. For BlackMamba (Mamba) with a batch size of 30 (Sparse(bsz=30)), the longest kernel execution time is approximately 2000 microseconds for matmul(w2)."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context snippet from Shen2024 explicitly states in Table 1 that the 'Top-k' value for JetMoE-8B is 2. Additionally, the text mentions setting 'top-k to 2 for every layer', which directly answers the question about the number of experts selected per token.","2","experts","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Table 1: JetMoE-8B hyperparameters.","The context snippet from Shen2024 explicitly states in Table 1 that the 'Top-k' value for JetMoE-8B is 2. Additionally, the text mentions setting 'top-k to 2 for every layer', which directly answers the question about the number of experts selected per token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the computations for deep learning models increased 300,000x from 2012 to 2018, which directly addresses the question's claim of a 200,000x increase. The answer_value must be 0 (False) as the actual increase is higher than stated.","0","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","The context states that the computations for deep learning models increased 300,000x from 2012 to 2018, which directly addresses the question's claim of a 200,000x increase. The answer_value must be 0 (False) as the actual increase is higher than stated."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from ref_id=wu2021b states that global carbon emissions in 2020 dropped by 6.4% during the COVID-19 pandemic. The answer_unit is 'percent', so the answer_value is the numeric value 6.4.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context snippet from ref_id=wu2021b states that global carbon emissions in 2020 dropped by 6.4% during the COVID-19 pandemic. The answer_unit is 'percent', so the answer_value is the numeric value 6.4."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources. The answer_unit 'data centers' requires the numeric value of data centers, which is 22.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources. The answer_unit 'data centers' requires the numeric value of data centers, which is 22."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of AI was officially born in 1956. To calculate the age in 2025, subtract 1956 from 2025, resulting in 69 years.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The context states that the field of AI was officially born in 1956. To calculate the age in 2025, subtract 1956 from 2025, resulting in 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context states that the 'five cars' estimate comes from the neural architecture search (NAS) approach used in the 2019 Strubell et al. study. The NAS training workload is described as a large-scale procedure performed infrequently, which matches the question's focus on a specific and infrequently performed AI process.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach [...] could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions [...] or about five times the emissions of a car during its lifetime [...] the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.","The context states that the 'five cars' estimate comes from the neural architecture search (NAS) approach used in the 2019 Strubell et al. study. The NAS training workload is described as a large-scale procedure performed infrequently, which matches the question's focus on a specific and infrequently performed AI process."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption by 44% compared to the configuration that minimizes latency. The answer_unit 'percent' requires a numeric value, so 44% is converted to 44.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption by 44% compared to the configuration that minimizes latency. The answer_unit 'percent' requires a numeric value, so 44% is converted to 44."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context snippet [ref_id=xia2024] from Table III lists the maximum batch sizes for different models and datasets. For BlackMamba-S (sparse) on the MATH dataset, the maximum batch size is 8. However, the question asks about the GSM8K dataset. The context mentions that BlackMamba is inadequate for fine-tuning GS (GSM8K) and thus results for it are not shown. Therefore, the answer is not available in the provided context.","is_blank","samples","[""xia2024""]","is_blank","The BlackMamba model is inadequate for fine-tuning GS. This is likely attributed to the complexity of mathematical tasks and the smaller model size of BlackMamba. Additionally, Mamba is specifically engineered for long sequence modeling, potentially resulting in unsatisfactory arithmetic reasoning ability [25]. Thus, in our characterization study in later sections, we will not show the results for BlackMamba fine-tuned on MATH.","The context snippet [ref_id=xia2024] from Table III lists the maximum batch sizes for different models and datasets. For BlackMamba-S (sparse) on the MATH dataset, the maximum batch size is 8. However, the question asks about the GSM8K dataset. The context mentions that BlackMamba is inadequate for fine-tuning GS (GSM8K) and thus results for it are not shown. Therefore, the answer is not available in the provided context."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context explicitly states that in 2023, Amazon's energy storage capacity was 1.3 GW, up from 445 MW in 2022. The answer_unit is 'GW', so the numeric value is provided in gigawatts as required.","1.3","GW","[""amazon2023""]","is_blank","In 2023, we added seven solar projects paired with battery energy storage systems to our portfolio in the U.S. We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context explicitly states that in 2023, Amazon's energy storage capacity was 1.3 GW, up from 445 MW in 2022. The answer_unit is 'GW', so the numeric value is provided in gigawatts as required."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o's annual carbon emissions are comparable to approximately 272 transatlantic flights between Boston and London. The answer_unit 'flights' matches the required format, so the numeric value is extracted directly from the context.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that GPT-4o's annual carbon emissions are comparable to approximately 272 transatlantic flights between Boston and London. The answer_unit 'flights' matches the required format, so the numeric value is extracted directly from the context."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The provided context snippets do not mention any specific data about the percentage of AI inference workloads in Asia powered by coal in 2023. The documents discuss global or US energy consumption, but no details about Asia's coal usage for AI inference in 2023 are present.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention any specific data about the percentage of AI inference workloads in Asia powered by coal in 2023. The documents discuss global or US energy consumption, but no details about Asia's coal usage for AI inference in 2023 are present."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context snippet from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally. The answer_unit 'PUE' requires a numeric value, which is directly provided in the context.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context snippet from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally. The answer_unit 'PUE' requires a numeric value, which is directly provided in the context."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters but only activates 2B parameters per input token during inference. The answer_unit is 'parameters', so the numeric value 2B is provided as the answer_value.","2B","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters but only activates 2B parameters per input token during inference. The answer_unit is 'parameters', so the numeric value 2B is provided as the answer_value."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context snippet from [ref_id=luccioni2025c] includes a table titled 'Range of Pre-Training Environmental Impacts (Representative Models Displayed)' which lists the Llama 7B model with GHG emissions of 14 tCO2e. The answer_unit 'tCO2e' matches the required unit, so the answer_value is 14.","14","tCO2e","[""luccioni2025c""]","is_blank","Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","The context snippet from [ref_id=luccioni2025c] includes a table titled 'Range of Pre-Training Environmental Impacts (Representative Models Displayed)' which lists the Llama 7B model with GHG emissions of 14 tCO2e. The answer_unit 'tCO2e' matches the required unit, so the answer_value is 14."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in 2022 and 2023 for each region. For the U.S., 2022 had 2,600 and 2023 had 11,800. Europe had 1,220 in 2022 and 3,000+ in 2023. India had 3,800 in 2022 and 7,200+ in 2023. Summing these gives 2,600 + 11,800 + 1,220 + 3,000 + 3,800 + 7,200 = 29,620. However, the exact numbers for Europe and India in 2023 are approximate (3,000+ and 7,200+), so the total is expressed as a range.","[29620, 30820]","electric delivery vans","[""amazon2023""]","is_blank","Last Mile Electric Delivery Vehicles by Region 7
Region 2022 2023
U.S. 2,600 11,800
Europe 1,220 3,000+
India 3,800 7,200+","The context provides the number of electric delivery vans in 2022 and 2023 for each region. For the U.S., 2022 had 2,600 and 2023 had 11,800. Europe had 1,220 in 2022 and 3,000+ in 2023. India had 3,800 in 2022 and 7,200+ in 2023. Summing these gives 2,600 + 11,800 + 1,220 + 3,000 + 3,800 + 7,200 = 29,620. However, the exact numbers for Europe and India in 2023 are approximate (3,000+ and 7,200+), so the total is expressed as a range."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context snippets indicate that operational environmental impacts of LLMs include GHG emissions from energy consumption, which involves data center operations including cooling. For example, [ref_id=luccioni2023] mentions Power Usage Effectiveness (PUE) related to cooling, and [ref_id=wu2021a] discusses operational carbon footprint considering data center PUE. Therefore, the statement is false because cooling contributes to GHG emissions.","0","is_blank","[""luccioni2023"", ""wu2021a""]","is_blank","['We also recognize that there are several factors that we are missing in order to be more precise in our estimation the carbon footprint of ML models: for instance, we do not have the necessary information regarding the Power Usage Effectiveness (PUE) of the data centers used for model training (i.e. the overhead used for heating, cooling, Internet etc.)', 'To quantify the emissions of Facebook’s models we measure the total energy consumed, assume location-based carbon intensities for energy mixes, 5 and use a data center Power Usage Effectiveness (PUE) of 1.1.']","The context snippets indicate that operational environmental impacts of LLMs include GHG emissions from energy consumption, which involves data center operations including cooling. For example, [ref_id=luccioni2023] mentions Power Usage Effectiveness (PUE) related to cooling, and [ref_id=wu2021a] discusses operational carbon footprint considering data center PUE. Therefore, the statement is false because cooling contributes to GHG emissions."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context snippet from ref_id=jegham2025's Table 4 lists the energy consumption for the o3 model under the 'Energy Consumption(10k input-1.5k output)(Wh)' column as 12.222 ± 1.082 Wh. The answer_unit is 'Wh', so the answer_value is the numeric value provided.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). Model [...] o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082 [...]","The context snippet from ref_id=jegham2025's Table 4 lists the energy consumption for the o3 model under the 'Energy Consumption(10k input-1.5k output)(Wh)' column as 12.222 ± 1.082 Wh. The answer_unit is 'Wh', so the answer_value is the numeric value provided."
"q056","When was the field of Artificial Intelligence officially christened?","The context snippets from ref_id=stone2022 explicitly state that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. The answer_unit is 'year', so the answer_value is the numeric year 1956.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context snippets from ref_id=stone2022 explicitly state that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. The answer_unit is 'year', so the answer_value is the numeric year 1956."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context snippets mention that Google's annualized global on-site water efficiency (WUE) is 1.0 L/kWh for scope-1 water consumption. This value is derived from their latest sustainability reports and is used in estimating global AI's water usage for 2027.","1.0","WUE","[""li2025b""]","is_blank","assuming 1.0 L/kWh for global scope-1 water consumption efficiency, we obtain a total on-site scope-1 water consumption of 0.09 – 0.14 billion cubic meters. Note that Google and Equinix both operate data centers globally, but represent two distinct categories of data centers: hyperscale data centers (Google) and multi-tenant colocation data centers (Equinix).","The context snippets mention that Google's annualized global on-site water efficiency (WUE) is 1.0 L/kWh for scope-1 water consumption. This value is derived from their latest sustainability reports and is used in estimating global AI's water usage for 2027."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context snippet [ref_id=wu2021b] states 'approximately 770 million people... do not have access to a stable supply of electricity', which directly supports the statement as true.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context snippet [ref_id=wu2021b] states 'approximately 770 million people... do not have access to a stable supply of electricity', which directly supports the statement as true."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for LLaMA 65B with a maximum generation length of 512 tokens, the energy per output token is about 3-4 Joules. This directly answers the question and matches the required unit of joules per token.","[3, 4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for LLaMA 65B with a maximum generation length of 512 tokens, the energy per output token is about 3-4 Joules. This directly answers the question and matches the required unit of joules per token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context snippet from wu2021a explicitly states that converting the 32-bit floating-point representation to 16-bit reduced the overall RM2 model size by 15%. The answer_unit 'percent' matches the required format, so the answer_value is 15.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context snippet from wu2021a explicitly states that converting the 32-bit floating-point representation to 16-bit reduced the overall RM2 model size by 15%. The answer_unit 'percent' matches the required format, so the answer_value is 15."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context states that the 5-10% GHG reduction claim originates from BCG reports but explicitly mentions that the reasoning and calculations are unclear and lack scientific grounding. The BCG reports do not provide detailed calculations or scientific validation, and the application of individual project observations to global emissions is criticized as unscientific.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. [...] Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding.","The context states that the 5-10% GHG reduction claim originates from BCG reports but explicitly mentions that the reasoning and calculations are unclear and lack scientific grounding. The BCG reports do not provide detailed calculations or scientific validation, and the application of individual project observations to global emissions is criticized as unscientific."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from patterson2021 states that sparsely activated models provide more than 10X reductions in computation and energy costs while offering higher accuracy than dense models. This directly supports the assertion that they consume less than 1/10th the energy without sacrificing accuracy.","1","is_blank","[""patterson2021""]","is_blank","Sparsely activated mixture-of-expert-style models can provide more than 10X reductions in computation requirements and energy costs for both training and inference while providing significantly higher accuracy than dense Transformer or LSTM-based models of equivalent computational cost per token [Sha17,Lep20,Fed21].","The context from patterson2021 states that sparsely activated models provide more than 10X reductions in computation and energy costs while offering higher accuracy than dense models. This directly supports the assertion that they consume less than 1/10th the energy without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context snippet [ref_id=schwartz2019] explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. The answer_unit is USD, so the answer_value is the numeric value provided in the context.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context snippet [ref_id=schwartz2019] explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. The answer_unit is USD, so the answer_value is the numeric value provided in the context."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. The answer_unit is 'percent', so we directly report the numeric value from the context.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. The answer_unit is 'percent', so we directly report the numeric value from the context."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the energy consumption rate for Flan-T5-xxl as 0.083 kWh per 1,000 queries. For 1 billion queries, the calculation is (1,000,000,000 queries / 1,000) * 0.083 kWh = 83,000 kWh, which converts to 83 MWh.","83","MWh","[""luccioni2024""]","[""https://blog.google/products/translate/ten-years-of-google-translate/""]","Flan-T5-xxl at 0.083 kWh/1k queries","The context provides the energy consumption rate for Flan-T5-xxl as 0.083 kWh per 1,000 queries. For 1 billion queries, the calculation is (1,000,000,000 queries / 1,000) * 0.083 kWh = 83,000 kWh, which converts to 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context snippet from ref_id=ebert2024 explicitly states 'The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].' This directly provides the average global PUE value for 2023 as 1.58, matching the answer_unit 'PUE' which expects a numeric value without the unit name.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context snippet from ref_id=ebert2024 explicitly states 'The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].' This directly provides the average global PUE value for 2023 as 1.58, matching the answer_unit 'PUE' which expects a numeric value without the unit name."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context snippets do not mention Microsoft contracting wind turbines for Azure AI clusters in 2023. The documents focus on Amazon's renewable energy projects and other companies' sustainability efforts without specific details about Microsoft's wind turbine contracts for that year.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention Microsoft contracting wind turbines for Azure AI clusters in 2023. The documents focus on Amazon's renewable energy projects and other companies' sustainability efforts without specific details about Microsoft's wind turbine contracts for that year."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49% when equity is included. This is directly supported by multiple references to the same figure in the Cottier et al. (2024) document.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49% when equity is included. This is directly supported by multiple references to the same figure in the Cottier et al. (2024) document."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the inaugural 2015 Study Panel had seventeen members. The answer_unit 'people' requires a numeric value, which is provided directly in the context.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context explicitly states that the inaugural 2015 Study Panel had seventeen members. The answer_unit 'people' requires a numeric value, which is provided directly in the context."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context snippet [ref_id=wu2021a] states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.' This directly answers the question about the percentage of a client device's total carbon footprint from manufacturing. The answer_unit 'percent' requires a numeric value, which is provided as 74.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context snippet [ref_id=wu2021a] states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.' This directly answers the question about the percentage of a client device's total carbon footprint from manufacturing. The answer_unit 'percent' requires a numeric value, which is provided as 74."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context from ref_id=luccioni2024 states that BLOOMz-7B (7B parameters) consumes 1.0 × 10⁻⁴ kWh per inference, while BLOOMz-560M (560M parameters) uses 5.4 × 10⁻⁵ kWh. However, ref_id=fernandez2025 mentions that energy use depends on factors like workload geometry and optimizations, implying that parameter count alone does not determine energy consumption. The answer_unit is 'is_blank', so answer_value must be '0' (False) based on evidence.","0","is_blank","[""fernandez2025"", ""luccioni2024""]","is_blank","Our analysis reveals that while idealized estimations of hardware utilization substantially underestimate the energy use of language model inference, proper application of inference efficiency optimizations can substantially reduce the energy requirements of inference by up to 73% from unoptimized baselines (fernandez2025). [...] the amount of energy required per inference varies from 5.4×10⁻⁵ kWh for BLOOMz-560M to 1.0×10⁻⁴ kWh for BLOOMz-7B (luccioni2024).","The context from ref_id=luccioni2024 states that BLOOMz-7B (7B parameters) consumes 1.0 × 10⁻⁴ kWh per inference, while BLOOMz-560M (560M parameters) uses 5.4 × 10⁻⁵ kWh. However, ref_id=fernandez2025 mentions that energy use depends on factors like workload geometry and optimizations, implying that parameter count alone does not determine energy consumption. The answer_unit is 'is_blank', so answer_value must be '0' (False) based on evidence."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context from ref_id=stone2022 states that the Study Panel does not consider it likely that near-term AI systems will autonomously choose to inflict harm on people. This directly addresses the concern about AI being an imminent threat, indicating the panel's position is that it is not an immediate threat.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","While the Study Panel does not consider it likely that near-term AI systems will autonomously choose to inflict harm on people, it will be possible for people to use AI-based systems for harmful as well as helpful purposes.","The context from ref_id=stone2022 states that the Study Panel does not consider it likely that near-term AI systems will autonomously choose to inflict harm on people. This directly addresses the concern about AI being an imminent threat, indicating the panel's position is that it is not an immediate threat."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The provided context snippets do not mention any specific data about OpenAI's API requests or their CO2 emissions in January 2024. The documents discuss various studies and reports on AI's energy consumption and carbon footprint but do not provide exact figures for OpenAI's API requests during that period.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention any specific data about OpenAI's API requests or their CO2 emissions in January 2024. The documents discuss various studies and reports on AI's energy consumption and carbon footprint but do not provide exact figures for OpenAI's API requests during that period."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. The 'five cars' estimate is based on 284 metric tons CO2e (from Strubell et al.), and the answer_unit 'tCO2e' matches the reported value.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context states that Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. The 'five cars' estimate is based on 284 metric tons CO2e (from Strubell et al.), and the answer_unit 'tCO2e' matches the reported value."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period. This is directly mentioned in multiple snippets from ref_id=wu2021a, including Figure 2(d) and the accompanying text.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period. This is directly mentioned in multiple snippets from ref_id=wu2021a, including Figure 2(d) and the accompanying text."
"q079","How many miles is the Earth from the Sun?","The provided context snippets do not contain any information about the distance between the Earth and the Sun. All references discuss topics related to AI, machine learning, energy consumption, and environmental impacts, but none mention astronomical distances.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context snippets do not contain any information about the distance between the Earth and the Sun. All references discuss topics related to AI, machine learning, energy consumption, and environmental impacts, but none mention astronomical distances."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context snippets from stone2022 explicitly state that AlphaGo defeated the human Go champion. For example, one snippet mentions 'AlphaGo beats human Go champ in milestone for artificial intelligence' and another states 'the AlphaGo program... defeated the current human champion at the game of Go'. The answer_unit is 'is_blank', so answer_value should be the exact identifier from the context. However, since the question is a True/False, the answer_value must be 1 (True) according to the system instructions.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","AlphaGo beats human Go champ in milestone for artificial intelligence (ref_id=stone2022)","The context snippets from stone2022 explicitly state that AlphaGo defeated the human Go champion. For example, one snippet mentions 'AlphaGo beats human Go champ in milestone for artificial intelligence' and another states 'the AlphaGo program... defeated the current human champion at the game of Go'. The answer_unit is 'is_blank', so answer_value should be the exact identifier from the context. However, since the question is a True/False, the answer_value must be 1 (True) according to the system instructions."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly mentions 'Continuous batching' as the strategy that dynamically replaces completed requests with new ones to reduce idle GPU time. The answer_unit is 'is_blank', requiring the exact name from the context.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context explicitly mentions 'Continuous batching' as the strategy that dynamically replaces completed requests with new ones to reduce idle GPU time. The answer_unit is 'is_blank', requiring the exact name from the context."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context explicitly states that the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. This is directly mentioned in the provided snippets from shen2024.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context explicitly states that the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. This is directly mentioned in the provided snippets from shen2024."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge costing $2.699. The percentage increase is calculated as (2.699 - 2.13)/2.13 * 100 ≈ 26.7%, which matches the 26.7% mentioned in the context.","26.7","percent","[""kim2025""]","is_blank","On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge costing $2.699. The percentage increase is calculated as (2.699 - 2.13)/2.13 * 100 ≈ 26.7%, which matches the 26.7% mentioned in the context."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context snippet from luccioni2024 explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. The answer_unit 'g CO2eq' matches the unit provided in the context, so the answer_value is directly taken from the text.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51], whereas the least carbon-intensive text generation model (distilbert-base-uncased) generates as much carbon as 0.0006 miles driven by a similar vehicle, i.e.","The context snippet from luccioni2024 explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. The answer_unit 'g CO2eq' matches the unit provided in the context, so the answer_value is directly taken from the text."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context snippets from luccioni2025c's Table 2 list GPU energy usage for 1k queries across multiple models, with the lowest being 0.06 Wh (bert-tiny) and highest being 3,426.38 Wh (Command-R Plus). The answer_unit specifies Wh, so the range is formatted as [0.06, 3426.38] using these exact values from the table.","[0.06, 3426.38]","Wh","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed) [...] Command-R Plus Cohere 3,426.38 Text Generation [...] Max/Min Variance: 57,106","The context snippets from luccioni2025c's Table 2 list GPU energy usage for 1k queries across multiple models, with the lowest being 0.06 Wh (bert-tiny) and highest being 3,426.38 Wh (Command-R Plus). The answer_unit specifies Wh, so the range is formatted as [0.06, 3426.38] using these exact values from the table."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states 'There is no one-size-fits-all solution for either ethics or sustainability' and mentions that current ethical charters lack depth for practical applications, indicating researchers do not believe a universal approach is feasible.","0","is_blank","[""luccioni2025b""]","is_blank","There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","The context explicitly states 'There is no one-size-fits-all solution for either ethics or sustainability' and mentions that current ethical charters lack depth for practical applications, indicating researchers do not believe a universal approach is feasible."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context snippet from patterson2021 states that 'the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21]'. This directly provides the value in the required unit of kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context snippet from patterson2021 states that 'the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21]'. This directly provides the value in the required unit of kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as a PyTorch-based framework developed to enable collaborative DL training in a decentralized fashion. It specifically states that Hivemind was used for distributed spot training across multiple clouds and continents, which directly answers the question.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context mentions Hivemind as a PyTorch-based framework developed to enable collaborative DL training in a decentralized fashion. It specifically states that Hivemind was used for distributed spot training across multiple clouds and continents, which directly answers the question."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context mentions that Ehsan et al. proposed expanding transparency in AI to include socio-technical aspects, specifically using the term 'social transparency'. This directly answers the question about the proposed term.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The context mentions that Ehsan et al. proposed expanding transparency in AI to include socio-technical aspects, specifically using the term 'social transparency'. This directly answers the question about the proposed term."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that in the experiments on German public administration texts (FKTG dataset), the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. The answer_unit is 'is_blank', requiring the exact model name from the context.","traditional linear model using pre-trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context mentions that in the experiments on German public administration texts (FKTG dataset), the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. The answer_unit is 'is_blank', requiring the exact model name from the context."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that the authors developed and deployed an LLM inference system called Lamina, which incorporates model-attention disaggregation. The answer_unit is 'is_blank', requiring the exact name from the context.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions that the authors developed and deployed an LLM inference system called Lamina, which incorporates model-attention disaggregation. The answer_unit is 'is_blank', requiring the exact name from the context."
"q093","How many parameters does the largest T5 model have?","The context snippets from ref_id=patterson2021 mention the T5 model in Table 4, which lists the number of parameters as 11 billion. The answer_unit is 'parameters', so the answer_value is the numeric value provided in the context.","11","parameters","[""patterson2021""]","is_blank","Number of Parameters (B) ... T5 ... 11","The context snippets from ref_id=patterson2021 mention the T5 model in Table 4, which lists the number of parameters as 11 billion. The answer_unit is 'parameters', so the answer_value is the numeric value provided in the context."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states in multiple snippets that JetMoE-8B has 8B parameters. For example, Table 1 in the hyperparameters section lists 'Ptotal' as 8B, and the text mentions 'JetMoE-8B has 8B parameters'. The answer_unit 'parameters' requires a numeric value in parameters, which matches the 8B value provided.","8000000000","parameters","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Table 1: JetMoE-8B hyperparameters.","The context explicitly states in multiple snippets that JetMoE-8B has 8B parameters. For example, Table 1 in the hyperparameters section lists 'Ptotal' as 8B, and the text mentions 'JetMoE-8B has 8B parameters'. The answer_unit 'parameters' requires a numeric value in parameters, which matches the 8B value provided."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context snippets from ref_id=li2025b and ref_id=luccioni2025a both explicitly state that Google's data center water consumption increased by 20% from 2021 to 2022. The answer_unit 'percent' requires the numeric value without the percentage sign, which matches the provided data.","20","percent","[""li2025b"", ""luccioni2025a""]","[""https://arxiv.org/abs/2304.03271v5"", ""https://doi.org/10.1145/XXXXXX.XXXXXX""]","Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4], and another technology company’s data center water consumption saw∼34% and ∼22% increases over the same periods, respectively [6] (li2025b). Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78] (luccioni2025a).","The context snippets from ref_id=li2025b and ref_id=luccioni2025a both explicitly state that Google's data center water consumption increased by 20% from 2021 to 2022. The answer_unit 'percent' requires the numeric value without the percentage sign, which matches the provided data."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context snippet from ref_id=khan2025's Table I explicitly lists 'Carbon Intensity' with the unit 'gCO2/kWh' and defines it as 'CO2 emissions per unit of electricity consumed'. This matches the question's description of the metric.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context snippet from ref_id=khan2025's Table I explicitly lists 'Carbon Intensity' with the unit 'gCO2/kWh' and defines it as 'CO2 emissions per unit of electricity consumed'. This matches the question's description of the metric."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context snippet from Table 2 in ref_id=li2025a shows that in the 101B parameter stage, the FLOPs utilization percentage is 52.88%. The answer_unit 'percent' requires the numeric value without the percentage sign.","52.88","percent","[""li2025a""]","is_blank","Table 2: ... Params (billion) ... FLOPs Utilization ... 101 ... 52.88%","The context snippet from Table 2 in ref_id=li2025a shows that in the 101B parameter stage, the FLOPs utilization percentage is 52.88%. The answer_unit 'percent' requires the numeric value without the percentage sign."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context snippet [ref_id=cottier2024] explicitly states that the amortized hardware CapEx + energy cost for GPT-4 is $40M. The answer_unit is USD, so the numeric value 40000000 is provided.","40000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context snippet [ref_id=cottier2024] explicitly states that the amortized hardware CapEx + energy cost for GPT-4 is $40M. The answer_unit is USD, so the numeric value 40000000 is provided."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that for the Transformer-based universal translation model (LM), full-stack optimizations including platform-level caching, GPU acceleration, and algorithmic changes reduced operational energy footprint by over 800×. The answer_unit 'multiplier' requires expressing this as a numerical factor.","810","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.","The context states that for the Transformer-based universal translation model (LM), full-stack optimizations including platform-level caching, GPU acceleration, and algorithmic changes reduced operational energy footprint by over 800×. The answer_unit 'multiplier' requires expressing this as a numerical factor."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that for NLP tasks, intercontinental training (C-8) results in a 41% performance drop compared to the local experiment (A-8). This implies the achieved throughput is 59% of the local throughput, which is a multiplier of 0.59.","0.59","multiplier","[""erben2023""]","is_blank","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for NLP tasks, intercontinental training (C-8) results in a 41% performance drop compared to the local experiment (A-8). This implies the achieved throughput is 59% of the local throughput, which is a multiplier of 0.59."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023. The answer_unit 'liters' requires a numeric value in liters, which matches the provided data.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023. The answer_unit 'liters' requires a numeric value in liters, which matches the provided data."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context explicitly states in multiple instances that using custom tags with the mentioned techniques reduces energy consumption. For example, the Answer to RQ1 and the Abstract both confirm this finding. The answer_unit 'is_blank' requires the answer_value to be a boolean (1 for True, 0 for False) as per the user's instruction for True/False questions.","1","is_blank","[""rubei2025""]","is_blank","Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context explicitly states in multiple instances that using custom tags with the mentioned techniques reduces energy consumption. For example, the Answer to RQ1 and the Abstract both confirm this finding. The answer_unit 'is_blank' requires the answer_value to be a boolean (1 for True, 0 for False) as per the user's instruction for True/False questions."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context snippet from ref_id=luccioni2025a explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which is a million more than in 2023. The answer_unit is 'GPUs', so the numeric value is provided without the unit name.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context snippet from ref_id=luccioni2025a explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which is a million more than in 2023. The answer_unit is 'GPUs', so the numeric value is provided without the unit name."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of amortized hardware CapEx + energy cost goes toward AI accelerator chips. The answer_unit is 'percent', so the numeric value 44 is returned.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of amortized hardware CapEx + energy cost goes toward AI accelerator chips. The answer_unit is 'percent', so the numeric value 44 is returned."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context snippets from multiple references [wu2021a, wu2021b] explicitly state that Facebook's data centers have a PUE of 1.10. The answer_unit 'PUE' indicates the value should be a number in PUE units, which matches the provided data.","1.1","PUE","[""wu2021a"", ""wu2021b""]","is_blank","[ref_id=wu2021a] Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers. --- [ref_id=wu2021b] Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context snippets from multiple references [wu2021a, wu2021b] explicitly state that Facebook's data centers have a PUE of 1.10. The answer_unit 'PUE' indicates the value should be a number in PUE units, which matches the provided data."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project, which stands for Ethical AI for the Governance of the Society. The acronym ETAIROS is explicitly provided in the context snippets, directly answering the question about the project's acronym.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The context mentions the Finnish ETAIROS project, which stands for Ethical AI for the Governance of the Society. The acronym ETAIROS is explicitly provided in the context snippets, directly answering the question about the project's acronym."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context explicitly states that the most expensive publicly-announced training runs include Google's Gemini Ultra at $30M. The answer_unit 'USD' requires a numeric value, which matches the $30M figure provided.","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context explicitly states that the most expensive publicly-announced training runs include Google's Gemini Ultra at $30M. The answer_unit 'USD' requires a numeric value, which matches the $30M figure provided."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, and the authors argue that environmental risks should be included. However, the Act's provisions lack sufficient emphasis on environmental factors and no detailed reporting on environmental risk mitigation is required. The question asks if the Act *requires* inclusion of environmental risks, but the context indicates this is proposed but not clearly mandated by the current Act.","0","is_blank","[""ebert2024""]","is_blank","Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors. Although environmental protection is included in the Act’s objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, and the authors argue that environmental risks should be included. However, the Act's provisions lack sufficient emphasis on environmental factors and no detailed reporting on environmental risk mitigation is required. The question asks if the Act *requires* inclusion of environmental risks, but the context indicates this is proposed but not clearly mandated by the current Act."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context mentions that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which directly answers the question. The answer_unit 'µg/m³' matches the unit provided in the context.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context mentions that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which directly answers the question. The answer_unit 'µg/m³' matches the unit provided in the context."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context snippet from luccioni2025a explicitly states that a life cycle assessment comparing print books to e-readers found that 115 books produce the same amount of CO2 as a single Amazon Kindle device. The answer_unit 'books' requires a numeric value, which is directly provided in the context.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context snippet from luccioni2025a explicitly states that a life cycle assessment comparing print books to e-readers found that 115 books produce the same amount of CO2 as a single Amazon Kindle device. The answer_unit 'books' requires a numeric value, which is directly provided in the context."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that 'some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.' This directly answers the question about the multiplier factor between most affected and less-impacted communities.","200","multiplier","[""han2024""]","is_blank","In particular, some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.","The context states that 'some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.' This directly answers the question about the multiplier factor between most affected and less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context snippet from ref_id=zschache2025's Table B1 lists the DS Llama 70B model with 702.06 Wh energy consumption for inference on the FKTG dataset, which matches the required unit 'Wh'.","702.06","Wh","[""zschache2025""]","is_blank","DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context snippet from ref_id=zschache2025's Table B1 lists the DS Llama 70B model with 702.06 Wh energy consumption for inference on the FKTG dataset, which matches the required unit 'Wh'."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context snippets provided do not mention a 2022 paper by Dodge et al. or any specific large language model analyzed by them. Therefore, the answer is not supported by the given context.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context snippets provided do not mention a 2022 paper by Dodge et al. or any specific large language model analyzed by them. Therefore, the answer is not supported by the given context."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context snippets from multiple references (luccioni2025a, jegham2025, morrison2025) explicitly mention Jevons' Paradox as the phenomenon where improved efficiency leads to increased overall resource consumption. The answer_unit is 'is_blank', requiring the exact identifier from the context, which is 'Jevons Paradox'.","Jevons Paradox","is_blank","[""luccioni2025a"", ""jegham2025"", ""morrison2025""]","is_blank","While much attention has been given to AI improving productivity and resource efficiency, these gains can result in higher overall consumption due to effects such as Jevons Paradox. [ref_id=luccioni2025a]","The context snippets from multiple references (luccioni2025a, jegham2025, morrison2025) explicitly mention Jevons' Paradox as the phenomenon where improved efficiency leads to increased overall resource consumption. The answer_unit is 'is_blank', requiring the exact identifier from the context, which is 'Jevons Paradox'."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides energy consumption for GPT-3 training as 1,287 MWh (from ref_id=patterson2021) and Meena training as 85.7 MWh (from ref_id=patterson2021). To find how many Meena runs equal one GPT-3 run: 1287 / 85.7 ≈ 15.02.","15","multiplier","[""patterson2021""]","is_blank","GPT-3 training energy: 1,287 MWh; Meena training energy: 85.7 MWh (Table 4 in patterson2021)","The context provides energy consumption for GPT-3 training as 1,287 MWh (from ref_id=patterson2021) and Meena training as 85.7 MWh (from ref_id=patterson2021). To find how many Meena runs equal one GPT-3 run: 1287 / 85.7 ≈ 15.02."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context includes Table 2 from the 2024 study (ref_id=luccioni2024) which lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences. The answer_unit 'kWh' requires the numeric value from the table.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
...
image generation 2.907 3.31
Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","The context includes Table 2 from the 2024 study (ref_id=luccioni2024) which lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences. The answer_unit 'kWh' requires the numeric value from the table."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context snippet from ref_id=strubell2019 explicitly states 'American life, avg, 1 year 36,156' under the 'Consumption CO2e (lbs)' table. The answer_unit is 'lbs', so the numeric value 36,156 is directly provided.","36156","lbs","[""strubell2019""]","is_blank","American life, avg, 1 year 36,156","The context snippet from ref_id=strubell2019 explicitly states 'American life, avg, 1 year 36,156' under the 'Consumption CO2e (lbs)' table. The answer_unit is 'lbs', so the numeric value 36,156 is directly provided."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context mentions that several counties in West Virginia are among the most affected by data center air pollution. Specifically, Figure 6(c) lists the top-10 counties by per-household health cost, with Marion County, WV having the highest at $1218.3. The question asks for the county in West Virginia with the highest projected per-household health cost, which is directly provided in the context.","Marion","is_blank","[""han2024""]","is_blank","State County Per-household Health Cost($) IR
WV Marion 1218.3(978.0, 1458.5) 0.80
WV Mason 1139.0(897.6, 1380.4) 0.71
OH Meigs 1123.1(840.8, 1405.5) 0.62
OH Gallia 1107.6(828.1, 1387.2) 0.74
WV Marshall 1083.8(831.2, 1336.5) 0.77
WV Taylor 1052.5(853.1, 1252.0) 0.70
PA Fayette 992.6(782.0, 1203.3) 0.74
PA Greene 944.9(770.3, 1119.5) 0.88
WV Brooke 918.8(693.2, 1144.4) 0.69
WV Jackson 871.9(703.8, 1039.9) 0.73","The context mentions that several counties in West Virginia are among the most affected by data center air pollution. Specifically, Figure 6(c) lists the top-10 counties by per-household health cost, with Marion County, WV having the highest at $1218.3. The question asks for the county in West Virginia with the highest projected per-household health cost, which is directly provided in the context."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context from ref_id=khan2025's Table III shows Mistral-small's CO2 emissions decreased from 0.020 kg before optimization to 0.015 kg after optimization. To find the multiplier, divide the post-optimization value by the pre-optimization value: 0.015 / 0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","Table III shows Mistral-small's CO2 emissions before optimization as 0.020 kg and after optimization as 0.015 kg.","The context from ref_id=khan2025's Table III shows Mistral-small's CO2 emissions decreased from 0.020 kg before optimization to 0.015 kg after optimization. To find the multiplier, divide the post-optimization value by the pre-optimization value: 0.015 / 0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context snippets from ref_id=ebert2024 and ref_id=luccioni2024 report that the BLOOMz-7B model's training energy was 51,686 kWh and fine-tuning energy was 7,571 kWh. Adding these gives the combined total as per the question's requirement in kWh.","59257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","Training energy (kWh) 51,686 [...] Finetuning energy (kWh) 7,571 [...] (from Table 5 in luccioni2024 and ebert2024 context)","The context snippets from ref_id=ebert2024 and ref_id=luccioni2024 report that the BLOOMz-7B model's training energy was 51,686 kWh and fine-tuning energy was 7,571 kWh. Adding these gives the combined total as per the question's requirement in kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The context snippets from ref_id=li2025a explicitly state the parameters of the FLM-101B model. The model is described with a hidden state dimension, layer number, context window, attention heads, and vocabulary size. The question asks for the total number of parameters, which is directly provided in the context as 101B.","101B","parameters","[""li2025a""]","is_blank","Params 175B 280B 540B 130B 70B 101B (from Table 3 in li2025a)","The context snippets from ref_id=li2025a explicitly state the parameters of the FLM-101B model. The model is described with a hidden state dimension, layer number, context window, attention heads, and vocabulary size. The question asks for the total number of parameters, which is directly provided in the context as 101B."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the full training energy for a 6.1B parameter model as 103,500 kWh from Dodge et al. (2022). For BLOOMz-7B, Table 5 in Luccioni et al. (2024) shows each inference uses 1.0e-4 kWh. Dividing 103,500 kWh by 1.0e-4 kWh/inference gives 1,035,000,000 inferences.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","From dodge2022: 'the total energy consumption to train this model to completion would be approximately [...] 103,500 kWh'. From luccioni2024 Table 5: 'Inference energy (kWh) 1.0 × 10−4' for BLOOMz-7B.","The context provides the full training energy for a 6.1B parameter model as 103,500 kWh from Dodge et al. (2022). For BLOOMz-7B, Table 5 in Luccioni et al. (2024) shows each inference uses 1.0e-4 kWh. Dividing 103,500 kWh by 1.0e-4 kWh/inference gives 1,035,000,000 inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context snippet from ref_id=luccioni2024 explicitly states that the total energy consumed for all model experimentation and evaluation was 754.66 kWh. The answer_unit is 'kWh', so the answer_value is provided as a number in that unit.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context snippet from ref_id=luccioni2024 explicitly states that the total energy consumed for all model experimentation and evaluation was 754.66 kWh. The answer_unit is 'kWh', so the answer_value is provided as a number in that unit."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides Table 5 which lists the 'Cost parity (# inferences)' for BLOOMz-7B as 592,570,000. This directly answers the question about the number of inferences needed for deployment energy to equal training and fine-tuning costs.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B ... Cost parity (# inferences) 592,570,000 (Table 5)","The context provides Table 5 which lists the 'Cost parity (# inferences)' for BLOOMz-7B as 592,570,000. This directly answers the question about the number of inferences needed for deployment energy to equal training and fine-tuning costs."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the dataset used for the German nuclear waste site objection texts is called the FKTG-dataset. The answer_unit is 'is_blank', requiring the exact name from the context.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the dataset used for the German nuclear waste site objection texts is called the FKTG-dataset. The answer_unit is 'is_blank', requiring the exact name from the context."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context snippets provided do not mention Meta's Llama 3 inference serving clusters or their freshwater consumption in 2024. While several documents discuss water consumption related to AI models and data centers, none specifically reference Llama 3's 2024 figures.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context snippets provided do not mention Meta's Llama 3 inference serving clusters or their freshwater consumption in 2024. While several documents discuss water consumption related to AI models and data centers, none specifically reference Llama 3's 2024 figures."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context snippets do not provide any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While there are mentions of environmental impact from mining rare earth metals and some recycling efforts by companies like Amazon, there is no specific data on NVIDIA H100's use of recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context snippets do not provide any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While there are mentions of environmental impact from mining rare earth metals and some recycling efforts by companies like Amazon, there is no specific data on NVIDIA H100's use of recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. The 3.2 tCO2e mentioned aligns with this equivalence, directly answering the question in the required 'passengers' unit.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. The 3.2 tCO2e mentioned aligns with this equivalence, directly answering the question in the required 'passengers' unit."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that in May 2025 data from OpenRouter, 84% of LLM token usage was through models with no disclosure of environmental impact. The answer_unit 'percent' requires a numerical value without the unit symbol, which matches the provided data.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context states that in May 2025 data from OpenRouter, 84% of LLM token usage was through models with no disclosure of environmental impact. The answer_unit 'percent' requires a numerical value without the unit symbol, which matches the provided data."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context snippets from ref_id=samsi2024 in Table II list the bare minimum hardware requirements for LLaMA variants. For the 13B model, under the A100 80GB column, the 'Count' is 1, indicating one A100 GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The context snippets from ref_id=samsi2024 in Table II list the bare minimum hardware requirements for LLaMA variants. For the 13B model, under the A100 80GB column, the 'Count' is 1, indicating one A100 GPU is required."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context from dodge2022 states that a 6 billion parameter model trained to completion would emit 21 to 78 metric tons of CO2. The answer_unit specifies metric tons, so the answer_value is the numeric range [21,78].","[21, 78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context from dodge2022 states that a 6 billion parameter model trained to completion would emit 21 to 78 metric tons of CO2. The answer_unit specifies metric tons, so the answer_value is the numeric range [21,78]."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The provided context snippets do not mention specific data about carbon emissions avoided by pruning and quantizing large language models in 2023. While several studies discuss energy efficiency and carbon emissions related to model training and inference, none provide numerical values for emissions avoided through pruning and quantization in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention specific data about carbon emissions avoided by pruning and quantizing large language models in 2023. While several studies discuss energy efficiency and carbon emissions related to model training and inference, none provide numerical values for emissions avoided through pruning and quantization in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context mentions that using a mix of 2 A100s and 1 A10G results in a 24% cost saving over A100-only. The answer_unit is 'percent', so answer_value is 24.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context mentions that using a mix of 2 A100s and 1 A10G results in a 24% cost saving over A100-only. The answer_unit is 'percent', so answer_value is 24."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context snippet from ref_id=chen2024 provides a table (Table 1) listing the price per chip for H100, H20, and TPU v6e. The H20's price is noted as $4.63/hr, estimated based on relative system cost against H100 since it's not available on cloud providers.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","The context snippet from ref_id=chen2024 provides a table (Table 1) listing the price per chip for H100, H20, and TPU v6e. The H20's price is noted as $4.63/hr, estimated based on relative system cost against H100 since it's not available on cloud providers."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses gather information manually by writing to authors, contradicting the claim that information is gathered automatically. The answer_unit is 'is_blank', so answer_value is '0' (False).","0","is_blank","[""luccioni2025b""]","is_blank","In fact, most carbon foot print analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that most carbon footprint analyses gather information manually by writing to authors, contradicting the claim that information is gathered automatically. The answer_unit is 'is_blank', so answer_value is '0' (False)."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of their total electricity cost. This is directly mentioned in the snippet from ref_id=han2024.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of their total electricity cost. This is directly mentioned in the snippet from ref_id=han2024."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context snippets from ref_id=samsi2024 include Table II which lists the bare minimum hardware required for different LLaMA models. For LLaMA-7B, the table specifies that 1 A100 80GB GPU is required with a maximum batch size of 64. The answer_unit 'A100_80GB_GPU' matches the required format, so the answer_value is 1.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The context snippets from ref_id=samsi2024 include Table II which lists the bare minimum hardware required for different LLaMA models. For LLaMA-7B, the table specifies that 1 A100 80GB GPU is required with a maximum batch size of 64. The answer_unit 'A100_80GB_GPU' matches the required format, so the answer_value is 1."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context snippet from ref_id=khan2025 states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization'. This directly supports the assertion in the question, and the answer_value is set to '1' (True) as per the additional info's instruction for True/False questions.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization","The context snippet from ref_id=khan2025 states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization'. This directly supports the assertion in the question, and the answer_value is set to '1' (True) as per the additional info's instruction for True/False questions."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context from luccioni2023 states that researchers reached out to 500 authors and collected 95 answers. The answer_unit is 'answers', so the numeric value 95 is provided.","95","answers","[""luccioni2023""]","is_blank","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","The context from luccioni2023 states that researchers reached out to 500 authors and collected 95 answers. The answer_unit is 'answers', so the numeric value 95 is provided."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To find the cost per H100 GPU-hour, divide the total budget by total GPU hours: $100,000 / 30,000 hours = ~3.33 USD/hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To find the cost per H100 GPU-hour, divide the total budget by total GPU hours: $100,000 / 30,000 hours = ~3.33 USD/hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that in Altoona, IA, the health cost is 122% of the electricity cost. This is directly taken from Table 2 in the provided context, which lists the percentage under the '% of Electricity Cost' column for Altoona, IA.","122","percent","[""han2024""]","is_blank","Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context states that in Altoona, IA, the health cost is 122% of the electricity cost. This is directly taken from Table 2 in the provided context, which lists the percentage under the '% of Electricity Cost' column for Altoona, IA."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states in multiple snippets that JetMoE-8B was trained on 1.25T tokens. The answer_unit 'tokens' requires a numeric value in tokens, which is directly provided in the context.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states in multiple snippets that JetMoE-8B was trained on 1.25T tokens. The answer_unit 'tokens' requires a numeric value in tokens, which is directly provided in the context."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context includes a table titled 'Amazon Renewable Energy Projects' with a row for the United Kingdom showing 36 projects. The answer_unit 'projects' matches the count provided in the table.","36","projects","[""amazon2023""]","is_blank","Project Location: United Kingdom, Number of Projects: 36, Total MW Capacity†: 901","The context includes a table titled 'Amazon Renewable Energy Projects' with a row for the United Kingdom showing 36 projects. The answer_unit 'projects' matches the count provided in the table."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context snippets from ref_id=amazon2023 include a section titled 'Amazon Representation by the Numbers' which lists the gender breakdown for the U.S. workforce. Under 'Amazon Workforce (All Levels)', the data shows '31.1%68.8%' for 2023, where the higher percentage corresponds to men. The answer_unit is 'percent', so the answer_value is 68.8.","68.8","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels)
31.1%68.8%","The context snippets from ref_id=amazon2023 include a section titled 'Amazon Representation by the Numbers' which lists the gender breakdown for the U.S. workforce. Under 'Amazon Workforce (All Levels)', the data shows '31.1%68.8%' for 2023, where the higher percentage corresponds to men. The answer_unit is 'percent', so the answer_value is 68.8."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context snippet [ref_id=li2025b] states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question with a percentage value.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context snippet [ref_id=li2025b] states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question with a percentage value."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context mentions that for BlackMamba with a batch size of 84 (sparse), the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for sparse BlackMamba with batch size 84 is 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)
0.0
0.5
1.0
1.5
2.0
    Execution Time
Breakdown (seconds)
Mixtral
Mamba
Fig. 4. Execution time breakdown.","The context mentions that for BlackMamba with a batch size of 84 (sparse), the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for sparse BlackMamba with batch size 84 is 2.0 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity' metric as the ratio between calculation and communication time, which is used to evaluate scalability in distributed training. The answer_unit is 'is_blank', so the exact identifier 'granularity' is provided as answer_value.","granularity","is_blank","[""erben2023""]","is_blank","Granularity is important to evaluate scalability.We found that the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.","The context introduces the 'granularity' metric as the ratio between calculation and communication time, which is used to evaluate scalability in distributed training. The answer_unit is 'is_blank', so the exact identifier 'granularity' is provided as answer_value."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context mentions that a coalition of Microsoft employees estimated the Exxon Mobil deal could add up to 640 percent more carbon emissions compared to Microsoft's carbon removal targets. The answer_unit 'times' requires converting the percentage to a multiplier, so 640% becomes 6.4 times.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119]","The context mentions that a coalition of Microsoft employees estimated the Exxon Mobil deal could add up to 640 percent more carbon emissions compared to Microsoft's carbon removal targets. The answer_unit 'times' requires converting the percentage to a multiplier, so 640% becomes 6.4 times."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface sources for various uses, matching the question's query. The answer_unit is 'is_blank', requiring the exact term from the context.","water withdrawal","is_blank","[""li2025b""]","is_blank","""Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].""","The context defines 'water withdrawal' as freshwater taken from ground or surface sources for various uses, matching the question's query. The answer_unit is 'is_blank', requiring the exact term from the context."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years. The answer_unit 'years' requires a numeric value in years, which is directly provided in the context.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity , the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee forms a Study Panel every five years. The answer_unit 'years' requires a numeric value in years, which is directly provided in the context."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context snippets from ref_id=wu2021b explicitly state that in the US, the average household has an average of 25 connected devices, citing Deloitte 2021. The answer_unit 'devices' requires a numeric value without the unit name, which matches the provided data.","25","devices","[""wu2021b""]","[""https://arxiv.org/abs/2108.06738""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context snippets from ref_id=wu2021b explicitly state that in the US, the average household has an average of 25 connected devices, citing Deloitte 2021. The answer_unit 'devices' requires a numeric value without the unit name, which matches the provided data."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context snippet from ref_id=luccioni2025c explicitly states the energy range for pre-training LLMs as 0.8 MWh to 3,500 MWh. The answer_unit is MWh, so the answer_value is formatted as a numeric range [0.8, 3500].","[0.8, 3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context snippet from ref_id=luccioni2025c explicitly states the energy range for pre-training LLMs as 0.8 MWh to 3,500 MWh. The answer_unit is MWh, so the answer_value is formatted as a numeric range [0.8, 3500]."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders in the Jeopardy challenge in 2011. The answer_unit is 'is_blank', which requires a True/False answer using 1 or 0. Since the statement in the question is false, the answer_value is 0.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.159","The context explicitly states that IBM's Watson program beat human contenders in the Jeopardy challenge in 2011. The answer_unit is 'is_blank', which requires a True/False answer using 1 or 0. Since the statement in the question is false, the answer_value is 0."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from luccioni2025a states that '10–50 queries on GPT-3 consumes around half a liter of water [68].' This directly answers the question with a numeric range in the unit 'queries' as specified by answer_unit.","[10, 50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context snippet from luccioni2025a states that '10–50 queries on GPT-3 consumes around half a liter of water [68].' This directly answers the question with a numeric range in the unit 'queries' as specified by answer_unit."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context explicitly states that JetMoE-8B-Chat achieved an MT-Bench score of 6.681, which is higher than Llama-2-13b-Chat's 6.650. The answer_unit 'score' requires a numerical value, which is directly provided in Table 4 of the context.","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650","The context explicitly states that JetMoE-8B-Chat achieved an MT-Bench score of 6.681, which is higher than Llama-2-13b-Chat's 6.650. The answer_unit 'score' requires a numerical value, which is directly provided in Table 4 of the context."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to consume a 500ml bottle of water for roughly 10–50 medium-length responses. The answer_unit is 'responses', so the answer_value is the range [10,50] as per the context.","[10, 50]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to consume a 500ml bottle of water for roughly 10–50 medium-length responses. The answer_unit is 'responses', so the answer_value is the range [10,50] as per the context."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context snippets explicitly state that Mélange reduces deployment costs by up to 77% in conversational settings. The answer_unit 'percent' matches the required format for the answer_value as a numeric value.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context snippets explicitly state that Mélange reduces deployment costs by up to 77% in conversational settings. The answer_unit 'percent' matches the required format for the answer_value as a numeric value."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in both the samsi2024 and rubei2025 documents, confirming the bare minimum requirement without compression or quantization.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. [ref_id=samsi2024]","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in both the samsi2024 and rubei2025 documents, confirming the bare minimum requirement without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context snippets from han2024 and luccioni2025c explicitly state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The answer_unit 'round trips' matches the required format, so the answer_value is 10000.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car. [ref_id=han2024]","The context snippets from han2024 and luccioni2025c explicitly state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The answer_unit 'round trips' matches the required format, so the answer_value is 10000."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context snippet from patterson2021 states that NVIDIA estimated 80–90% of the ML workload is inference processing [Leo19]. This directly answers the question about NVIDIA's 2019 estimate, and the answer_unit 'percent' requires the numeric range format.","[80, 90]","percent","[""patterson2021""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context snippet from patterson2021 states that NVIDIA estimated 80–90% of the ML workload is inference processing [Leo19]. This directly answers the question about NVIDIA's 2019 estimate, and the answer_unit 'percent' requires the numeric range format."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context snippet from [ref_id=luccioni2024] states that for all model experimentation and evaluation in the study, they emitted 178.97 kg of CO2eq. This directly answers the question about total CO2 equivalent emissions generated during the study.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The context snippet from [ref_id=luccioni2024] states that for all model experimentation and evaluation in the study, they emitted 178.97 kg of CO2eq. This directly answers the question about total CO2 equivalent emissions generated during the study."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that using TDP for energy estimation is a common practice but leads to overestimation, with a worst-case factor of 4.1x. This indicates that TDP-based estimation is not reliable or accurate.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context states that using TDP for energy estimation is a common practice but leads to overestimation, with a worst-case factor of 4.1x. This indicates that TDP-based estimation is not reliable or accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to using older A100 hardware. Specifically, it mentions GPT-4o mini's consumption is 3.098 Wh compared to GPT-4o's 2.875 Wh for long queries. Since the question asks whether GPT-4o mini consumes less energy, the answer is False (0).","0","is_blank","[""jegham2025""]","is_blank","For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to using older A100 hardware. Specifically, it mentions GPT-4o mini's consumption is 3.098 Wh compared to GPT-4o's 2.875 Wh for long queries. Since the question asks whether GPT-4o mini consumes less energy, the answer is False (0)."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context includes a figure (Fig. 15) showing the throughput of Mixtral-CS-A100-40GB. For batch size 1, the dense model's throughput is indicated as 0.3 queries/sec based on the data points in the figure.","0.3","queries/sec","[""xia2024""]","is_blank","0 1 2 3 4 50.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
RMSE=0.03
Mixtral-CS-A100-40GB
Dense Sparse
... (Fig. 15)","The context includes a figure (Fig. 15) showing the throughput of Mixtral-CS-A100-40GB. For batch size 1, the dense model's throughput is indicated as 0.3 queries/sec based on the data points in the figure."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that direct environmental disclosures peaked in 2022 with 10% of models, but after 2022 there was a notable reversal triggered by commercial models like ChatGPT. By Q1 2025, most models fell under 'no disclosure'. This contradicts the claim that disclosures continued to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022 [...] triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category","The context states that direct environmental disclosures peaked in 2022 with 10% of models, but after 2022 there was a notable reversal triggered by commercial models like ChatGPT. By Q1 2025, most models fell under 'no disclosure'. This contradicts the claim that disclosures continued to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context snippets from Griggs et al. (2024) mention the price per chip for H100 in Table 1 as $11.06/hr. The question asks for the normalized on-demand hourly price, which matches the unit 'USD per hour' specified in the additional info.","11.06","USD per hour","[""chen2024""]","is_blank","H100 [...] Price per chip [2] $11.06/hr [...] Table 1: H100, H20, and TPU v6e specifications.","The context snippets from Griggs et al. (2024) mention the price per chip for H100 in Table 1 as $11.06/hr. The question asks for the normalized on-demand hourly price, which matches the unit 'USD per hour' specified in the additional info."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context snippets do not provide specific data on GPT-4's water usage for cooling during training. While GPT-3's training water consumption is mentioned (700,000 liters in ref_id=li2025b), and GPT-4o's inference water consumption is projected (1,334,991 to 1,579,680 kL in ref_id=jegham2025), there is no explicit figure for GPT-4 training. The answer_unit requires liters of water, but the necessary data is absent.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context snippets do not provide specific data on GPT-4's water usage for cooling during training. While GPT-3's training water consumption is mentioned (700,000 liters in ref_id=li2025b), and GPT-4o's inference water consumption is projected (1,334,991 to 1,579,680 kL in ref_id=jegham2025), there is no explicit figure for GPT-4 training. The answer_unit requires liters of water, but the necessary data is absent."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context snippet [griggs2024] states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs costing over $5,200 per month. To find the hourly cost, divide $5,200 by 30 days/month and 24 hours/day.","7.22","USD per hour","[""griggs2024""]","is_blank","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context snippet [griggs2024] states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs costing over $5,200 per month. To find the hourly cost, divide $5,200 by 30 days/month and 24 hours/day."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that increasing the BLEU score from 5 to 40 for GPT-3-based translation requires a model 1,000× larger in size. The answer_unit 'multiplier' matches the 1,000× figure from the context.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.","The context explicitly states that increasing the BLEU score from 5 to 40 for GPT-3-based translation requires a model 1,000× larger in size. The answer_unit 'multiplier' matches the 1,000× figure from the context."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context snippet from ref_id=luccioni2023 states that training a Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2. Another snippet from ref_id=wu2021a mentions that training a large model emits CO2 equivalent to 242,231 miles driven. Using the ratio from Wu2021a (626,155 lbs CO2 ≈ 242,231 miles), the equivalent driving distance is calculated.","242231","miles","[""luccioni2023"", ""wu2021a""]","is_blank","['Training a Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2', 'The carbon footprint of training one large ML model [...] is equivalent to 242,231 miles driven by an average passenger vehicle']","The context snippet from ref_id=luccioni2023 states that training a Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2. Another snippet from ref_id=wu2021a mentions that training a large model emits CO2 equivalent to 242,231 miles driven. Using the ratio from Wu2021a (626,155 lbs CO2 ≈ 242,231 miles), the equivalent driving distance is calculated."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model's inference energy is 1.0 × 10⁻⁴ kWh per inference. Total downloads are 606,096, each resulting in 1 million inferences. Total energy = 606,096 * 1,000,000 * 1.0e-4 kWh = 60,609,600 kWh = 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","Inference energy (kWh) 1.0 × 10−4 [...] BLOOMz-7B has been downloaded 606,096 times [...] (Table 5 and download statistics from context).","The BLOOMz-7B model's inference energy is 1.0 × 10⁻⁴ kWh per inference. Total downloads are 606,096, each resulting in 1 million inferences. Total energy = 606,096 * 1,000,000 * 1.0e-4 kWh = 60,609,600 kWh = 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context snippet from ref_id=shen2024 explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. The answer_unit 'H100 GPU hours' matches the unit provided in the context, so the answer_value is directly taken from the text.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context snippet from ref_id=shen2024 explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. The answer_unit 'H100 GPU hours' matches the unit provided in the context, so the answer_value is directly taken from the text."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. The answer_unit is USD, so the answer_value is 1000000000.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. The answer_unit is USD, so the answer_value is 1000000000."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context snippet [ref_id=patterson2021] explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23 FLOPS. This matches the question's requirement for the answer in FLOPS.","3.14e+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context snippet [ref_id=patterson2021] explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23 FLOPS. This matches the question's requirement for the answer in FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that for LLaMA 65B, a minimum of 8 V100 GPUs each with 32 GB of RAM are required for meaningful inferences without compression or quantization. The answer_unit 'V100_32GB_GPUs' matches the required format, so the answer_value is 8.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. [ref_id=samsi2024]","The context explicitly states that for LLaMA 65B, a minimum of 8 V100 GPUs each with 32 GB of RAM are required for meaningful inferences without compression or quantization. The answer_unit 'V100_32GB_GPUs' matches the required format, so the answer_value is 8."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the teraFLOP/s per GPU (165) and number of GPUs (192) for the 101B stage in Table 2, but does not explicitly state the duration of this specific stage required to calculate total zettaFLOPs. While total training time (21.54 days) and tokens processed (26.54B) are mentioned, the breakdown per stage is unavailable. The formula 6*N*tokens would require knowing which portion of tokens belong to the 101B stage, which isn't clearly specified.","is_blank","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages [...] Params [...] 101 [...] teraFLOP/s FLOPs [...] 165 [...] Number of GPUs [...] 192","The context provides the teraFLOP/s per GPU (165) and number of GPUs (192) for the 101B stage in Table 2, but does not explicitly state the duration of this specific stage required to calculate total zettaFLOPs. While total training time (21.54 days) and tokens processed (26.54B) are mentioned, the breakdown per stage is unavailable. The formula 6*N*tokens would require knowing which portion of tokens belong to the 101B stage, which isn't clearly specified."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context snippets mention that AlexNet has 'much lower accuracy' compared to ResNet but do not provide the exact top-1 accuracy value for AlexNet. The provided documents reference ImageNet performance but only specify ResNet-50's accuracy under different training methods. Since the exact AlexNet 2012 top-1 accuracy is not stated, the answer is_blank.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context snippets mention that AlexNet has 'much lower accuracy' compared to ResNet but do not provide the exact top-1 accuracy value for AlexNet. The provided documents reference ImageNet performance but only specify ResNet-50's accuracy under different training methods. Since the exact AlexNet 2012 top-1 accuracy is not stated, the answer is_blank."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 A800 GPUs. Multiplying 24 servers by 8 GPUs per server gives the total number of GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 A800 GPUs. Multiplying 24 servers by 8 GPUs per server gives the total number of GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context snippets from multiple references (luccioni2023, strubell2019, patterson2021, dodge2022, zschache2025) state that NAS training for a Transformer model produced 284,019 kg (626,155 lbs) of CO2. The strubell2019 table compares this to 'Car, avg incl. fuel, 1 lifetime 126,000' lbs CO2. Dividing 626,155 by 126,000 gives approximately 5 lifetimes.","5","lifetimes","[""strubell2019"", ""luccioni2023"", ""patterson2021"", ""dodge2022"", ""zschache2025""]","is_blank","[ref_id=strubell2019] Car, avg incl. fuel, 1 lifetime 126,000 [...] w/ neural architecture search 626,155","The context snippets from multiple references (luccioni2023, strubell2019, patterson2021, dodge2022, zschache2025) state that NAS training for a Transformer model produced 284,019 kg (626,155 lbs) of CO2. The strubell2019 table compares this to 'Car, avg incl. fuel, 1 lifetime 126,000' lbs CO2. Dividing 626,155 by 126,000 gives approximately 5 lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context snippet [ref_id=schwartz2019] states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train. The question asks for GPU hours, which matches the answer_unit 'hours'.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context snippet [ref_id=schwartz2019] states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train. The question asks for GPU hours, which matches the answer_unit 'hours'."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that the on-site solar energy systems are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer_unit is 'metric tons', so the numeric value is provided.","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that the on-site solar energy systems are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer_unit is 'metric tons', so the numeric value is provided."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This directly answers the question about the framework used for deploying across multiple GPUs and nodes.","vLLM","is_blank","[""zschache2025""]","is_blank","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context mentions that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This directly answers the question about the framework used for deploying across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context mentions that using two nodes increased energy consumption by a factor that depends on the model. Table B2 in ref_id=zschache2025 specifically lists the Llama 3.1 70B model with a ratio of 1.95 for energy consumption when comparing single vs. double node deployment. The answer_unit 'multiplier' requires the numeric factor, which is 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","T able B2 Comparison single vs. double node deployment, Capella system
Model Duration (s) Energy consumed (Wh)
single double ratio single double ratio
Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","The context mentions that using two nodes increased energy consumption by a factor that depends on the model. Table B2 in ref_id=zschache2025 specifically lists the Llama 3.1 70B model with a ratio of 1.95 for energy consumption when comparing single vs. double node deployment. The answer_unit 'multiplier' requires the numeric factor, which is 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific data on water consumption per ChatGPT user session in 2023. While multiple documents discuss water usage related to AI models like GPT-3 and GPT-4o, none explicitly state the gallons consumed per user session for ChatGPT in 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on water consumption per ChatGPT user session in 2023. While multiple documents discuss water usage related to AI models like GPT-3 and GPT-4o, none explicitly state the gallons consumed per user session for ChatGPT in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that scaling 700 million daily GPT-4o queries results in annual electricity use comparable to 35,000 U.S. homes. The answer_unit 'homes' matches the numeric value provided in the context.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that scaling 700 million daily GPT-4o queries results in annual electricity use comparable to 35,000 U.S. homes. The answer_unit 'homes' matches the numeric value provided in the context."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context snippets from both luccioni2025a and li2025b explicitly mention Microsoft's 34% increase in global water consumption between 2021 and 2022. The answer_unit 'percent' requires a numerical value, which matches the reported percentage.","34","percent","[""luccioni2025a"", ""li2025b""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78]. (luccioni2025a)

Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4], and another technology company’s data center water consumption saw∼34% and ∼22% increases over the same periods, respectively [6]. (li2025b)","The context snippets from both luccioni2025a and li2025b explicitly mention Microsoft's 34% increase in global water consumption between 2021 and 2022. The answer_unit 'percent' requires a numerical value, which matches the reported percentage."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that for the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs in Yelp sentiment analysis.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that for the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs in Yelp sentiment analysis."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where Evolved Transformer was run is 1.11. The answer_unit 'PUE' indicates the value should be provided as a number in the PUE unit, which matches the context's mention of 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the PUE for the Iowa datacenter where Evolved Transformer was run is 1.11. The answer_unit 'PUE' indicates the value should be provided as a number in the PUE unit, which matches the context's mention of 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context explicitly states that the total number of GPT-4o queries in 2025 is approximately 772 billion, found in multiple snippets from ref_id=jegham2025. The answer_unit 'queries' requires a numeric value in that unit, which matches the provided data.","772000000000","queries","[""jegham2025""]","is_blank","yielding a total of approximately 772 billion GPT-4o queries in 2025","The context explicitly states that the total number of GPT-4o queries in 2025 is approximately 772 billion, found in multiple snippets from ref_id=jegham2025. The answer_unit 'queries' requires a numeric value in that unit, which matches the provided data."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context includes a table (Table 3) from ref_id=shen2024 showing the OpenLLM Leaderboard Avg. scores for JetMoE-8B as 53.0. The answer_unit 'score' requires a numerical value, which matches the provided data.","53.0","score","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context includes a table (Table 3) from ref_id=shen2024 showing the OpenLLM Leaderboard Avg. scores for JetMoE-8B as 53.0. The answer_unit 'score' requires a numerical value, which matches the provided data."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context snippets provided do not mention any specific number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss renewable energy initiatives and their impact on reducing carbon footprints, none provide quantitative data on the exact number of training runs powered exclusively by renewable energy for that year.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context snippets provided do not mention any specific number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss renewable energy initiatives and their impact on reducing carbon footprints, none provide quantitative data on the exact number of training runs powered exclusively by renewable energy for that year."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that the AI Act does not mandate the disclosure of energy consumption for open-source models unless they are considered high-risk or have systemic risk. Specifically, it mentions that the Act 'does not mandate the disclosure of energy consumption' for open-source models unless they pose systemic risk, and there is a proposal to eliminate the open-source exemption.","1","is_blank","[""ebert2024""]","is_blank","Elimination of open-source exemption : Remove the exemption that allows open-source models to bypass reporting obligations.","The context states that the AI Act does not mandate the disclosure of energy consumption for open-source models unless they are considered high-risk or have systemic risk. Specifically, it mentions that the Act 'does not mandate the disclosure of energy consumption' for open-source models unless they pose systemic risk, and there is a proposal to eliminate the open-source exemption."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context snippets from ref_id=ebert2024 and ref_id=wu2021b both mention that the average PUE for a typical data center in 2020 was 1.58. The answer_unit 'PUE' requires a numeric value, which matches the provided context.","1.58","PUE","[""ebert2024"", ""wu2021b""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/"", ""https://example.com/wu2021b""]","[ref_id=ebert2024] The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].
[ref_id=wu2021b] the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context snippets from ref_id=ebert2024 and ref_id=wu2021b both mention that the average PUE for a typical data center in 2020 was 1.58. The answer_unit 'PUE' requires a numeric value, which matches the provided context."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context snippet from ref_id=kim2025 explicitly states that when the batch size increases to 32, the KV Cache expands to 5.312GB. The answer_unit 'GB' requires the numeric value in gigabytes, which matches the provided data.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context snippet from ref_id=kim2025 explicitly states that when the batch size increases to 32, the KV Cache expands to 5.312GB. The answer_unit 'GB' requires the numeric value in gigabytes, which matches the provided data."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context snippets explicitly state that for the four notable models (GPT-3, OPT-175B, GPT-4, Gemini Ultra), R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This matches the question's requirement for a percentage range in the specified answer_unit 'percent'.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context snippets explicitly state that for the four notable models (GPT-3, OPT-175B, GPT-4, Gemini Ultra), R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This matches the question's requirement for a percentage range in the specified answer_unit 'percent'."
"q213","Which software package was used to measure energy consumption during inference runs?","The context snippets from zschache2025 explicitly mention that the energy consumption and runtime during inference were measured using the CodeCarbon package. The question asks for the software package name, which matches the answer_unit 'is_blank' requiring the exact identifier from context.","CodeCarbon","is_blank","[""zschache2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context snippets from zschache2025 explicitly mention that the energy consumption and runtime during inference were measured using the CodeCarbon package. The question asks for the software package name, which matches the answer_unit 'is_blank' requiring the exact identifier from context."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that 53% of the analyzed articles cited the 3 Wh figure or the '10 times more than a Google search' claim. The answer_unit is 'percent', so the answer_value is 53.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42","The context states that 53% of the analyzed articles cited the 3 Wh figure or the '10 times more than a Google search' claim. The answer_unit is 'percent', so the answer_value is 53."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly mentions the Compute Time Calibration Function (CTCF) as the function proposed to adjust for discrepancies between theoretical and actual GPU performance. The answer_unit is 'is_blank', requiring the exact name from the context.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context explicitly mentions the Compute Time Calibration Function (CTCF) as the function proposed to adjust for discrepancies between theoretical and actual GPU performance. The answer_unit is 'is_blank', requiring the exact name from the context."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards tends to increase the energy costs of inference per response for LLaMA 65B. This is explicitly mentioned in multiple sections, including the discussion of Energy per Response (D. Energy per Response: LLaMA 65B).","1","is_blank","[""samsi2024""]","is_blank","Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.","The context states that increasing the number of shards tends to increase the energy costs of inference per response for LLaMA 65B. This is explicitly mentioned in multiple sections, including the discussion of Energy per Response (D. Energy per Response: LLaMA 65B)."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes 11 kL of water. One 12-inch silicon wafer (125g) produces 63 H100s. To calculate water consumption per GPU: (0.1% * H100 mass) * 11 kL/kg. However, the exact mass of an H100 isn't provided, making direct calculation impossible. The context directly states 'these add an additional 2.2 liters consumed [...] per GPU', which converts to 0.0022 kL.","0.0022","kL","[""morrison2025""]","is_blank","Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context states that an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes 11 kL of water. One 12-inch silicon wafer (125g) produces 63 H100s. To calculate water consumption per GPU: (0.1% * H100 mass) * 11 kL/kg. However, the exact mass of an H100 isn't provided, making direct calculation impossible. The context directly states 'these add an additional 2.2 liters consumed [...] per GPU', which converts to 0.0022 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that the open-source exemption from reporting obligations should be removed, implying that under current EU rules, open-source models are exempt. The proposal to eliminate this exemption suggests that currently they do not have to report.","0","is_blank","[""ebert2024""]","is_blank","Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The context states that the open-source exemption from reporting obligations should be removed, implying that under current EU rules, open-source models are exempt. The proposal to eliminate this exemption suggests that currently they do not have to report."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippet from [luccioni2025a] explicitly states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","The context snippet from [luccioni2025a] explicitly states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, U.S. data centers resulted in a total public health cost of about $6.7 billion. The answer_unit is USD, so the numeric value 6700000000 is provided.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.","The context states that in 2023, U.S. data centers resulted in a total public health cost of about $6.7 billion. The answer_unit is USD, so the numeric value 6700000000 is provided."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides energy consumption values for both the o3 model and GPT-4.1 nano under the 'Energy Consumption(10k input-1.5k output)(Wh)' column in Table 4. For o3, the value is 12.222 Wh, and for GPT-4.1 nano, it's 0.827 Wh. The multiplier is calculated by dividing 12.222 by 0.827, resulting in approximately 14.78.","14.78","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). [...] o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082 [...] GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094","The context provides energy consumption values for both the o3 model and GPT-4.1 nano under the 'Energy Consumption(10k input-1.5k output)(Wh)' column in Table 4. For o3, the value is 12.222 Wh, and for GPT-4.1 nano, it's 0.827 Wh. The multiplier is calculated by dividing 12.222 by 0.827, resulting in approximately 14.78."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the short-context Arena dataset with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. The answer_unit 'percent' requires the answer_value to be a numeric range in percentages, which matches the provided data.","[15, 77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that for the short-context Arena dataset with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. The answer_unit 'percent' requires the answer_value to be a numeric range in percentages, which matches the provided data."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context snippet from ref_id=li2025a's Table 3 lists FLM-101B's net tCO2e as 26. The answer_unit 'tCO2e' matches the required metric tons of CO2 equivalent, so the answer_value is 26.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. [...] net tCO2e [...] FLM-101B 26","The context snippet from ref_id=li2025a's Table 3 lists FLM-101B's net tCO2e as 26. The answer_unit 'tCO2e' matches the required metric tons of CO2 equivalent, so the answer_value is 26."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context snippets from xia2024 provide execution time breakdowns for Mixtral and BlackMamba models. Specifically, Figure 4 shows the execution time breakdown for Mixtral with sparse fine-tuning at batch size 1. The 'Forward Backward Optimizer' breakdown for Mixtral-Sparse(bsz=1) in Figure 4 indicates the total execution time is approximately 0.3 seconds.","0.3","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0
0.25
0.50
0.75
1.00
1.25
1.50
1.75
 RMSE=0.05
Mixtral-CS
Dense Sparse
0 1 2 3 4 50.0
0.2
0.4
0.6
0.8
1.0
1.2
 RMSE=0.02
Mixtral-MATH
0 5 10 15 200
2
4
6
8
10
12
14
16
 RMSE=0.79
Mamba-CS
0 2 4 6 8 10 120
2
4
6
8
10
12
 RMSE=0.42
Throughput (queries/sec)
Batch size
Mamba-MATH
Fig. 14. Estimation and validation of LLM fine-tuning throughput for
different models, datasets for A40 GPU. Dots represent ground truth and
lines present the estimation.
0 1 2 3 4 50.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
RMSE=0.03
Mixtral-CS-A100-40GB
Dense Sparse
0 5 10 15 200.0
0.5
1.0
1.5
2.0
2.5
3.0
 RMSE=0.09
Mixtral-CS-A100-80GB
0 5 10 15 200
1
2
3
4
5
 RMSE=0.55
Mixtral-CS-H100
Throughput (queries/sec)
Batch size
Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS
for different GPUs: A100 and H100.","The context snippets from xia2024 provide execution time breakdowns for Mixtral and BlackMamba models. Specifically, Figure 4 shows the execution time breakdown for Mixtral with sparse fine-tuning at batch size 1. The 'Forward Backward Optimizer' breakdown for Mixtral-Sparse(bsz=1) in Figure 4 indicates the total execution time is approximately 0.3 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context states that the public health impact of AI is 'highly unevenly distributed' and 'disproportionately affecting certain (often low-income) communities', with per-household impacts up to 200 times higher in some areas. This directly contradicts the statement that costs are evenly distributed.","0","is_blank","[""han2024""]","is_blank","Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context states that the public health impact of AI is 'highly unevenly distributed' and 'disproportionately affecting certain (often low-income) communities', with per-household impacts up to 200 times higher in some areas. This directly contradicts the statement that costs are evenly distributed."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet [ref_id=wu2021b] states that GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years as a result of Moore's law scaling and architectural optimization, referencing Sun et al., 2019. This directly supports the statement in the question, which asks about the doubling period observed as of 2019. The answer_unit is 'is_blank', so answer_value is '1' (True) based on the context.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context snippet [ref_id=wu2021b] states that GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years as a result of Moore's law scaling and architectural optimization, referencing Sun et al., 2019. This directly supports the statement in the question, which asks about the doubling period observed as of 2019. The answer_unit is 'is_blank', so answer_value is '1' (True) based on the context."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that the framework applies quantization through Ollama, an open-source platform known for edge computing and privacy-centric deployments. The question asks for the open-source tool used, which matches 'Ollama' from the context.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context mentions that the framework applies quantization through Ollama, an open-source platform known for edge computing and privacy-centric deployments. The question asks for the open-source tool used, which matches 'Ollama' from the context."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions using the WebDataset library to access datasets on-demand via shards in thetar format, specifically for spot VMs that can terminate anytime. The storage service used is Backblaze (B2), an independent S3 provider chosen for its global replication and cost-effectiveness.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. [...] We access the datasets on-demand via shards in thetar format with the WebDataset library [10].","The context mentions using the WebDataset library to access datasets on-demand via shards in thetar format, specifically for spot VMs that can terminate anytime. The storage service used is Backblaze (B2), an independent S3 provider chosen for its global replication and cost-effectiveness."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context snippets from ref_id=zschache2025 explicitly state that energy consumption during inference is shown to highly correlate with model runtime, indicating a strong linear relationship. The answer_unit is 'is_blank', so answer_value should be the exact identifier from the context.","is_blank","is_blank","[""zschache2025""]","is_blank","Additionally, the energy consumption during inference is shown to highly correlate with the model’s runtime. This makes the duration of computations a valuable proxy measure for energy consumption in settings where the latter cannot be traced.","The context snippets from ref_id=zschache2025 explicitly state that energy consumption during inference is shown to highly correlate with model runtime, indicating a strong linear relationship. The answer_unit is 'is_blank', so answer_value should be the exact identifier from the context."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context mentions that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act in the U.S. Senate on 1 Feb 2024. The answer_unit is 'is_blank', so the answer_value is the exact name from the context.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [78].","The context mentions that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act in the U.S. Senate on 1 Feb 2024. The answer_unit is 'is_blank', so the answer_value is the exact name from the context."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context snippet from ref_id=chen2024's Table 1 lists the price per chip for H100 as $11.06/hr under the 'Price per chip [2]' column. The answer_unit is 'USD per hour', so the numeric value is 11.06.","11.06","USD per hour","[""chen2024""]","is_blank","H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price
is estimated using the relative complete system cost against H100.","The context snippet from ref_id=chen2024's Table 1 lists the price per chip for H100 as $11.06/hr under the 'Price per chip [2]' column. The answer_unit is 'USD per hour', so the numeric value is 11.06."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context snippet from ref_id=morrison2025 explicitly states 'Internally, we assume a 4 year lifespan for our GPUs', indicating the estimated average GPU lifetime used in their analysis. The answer_unit 'years' matches the required format.","4","years","[""morrison2025""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","The context snippet from ref_id=morrison2025 explicitly states 'Internally, we assume a 4 year lifespan for our GPUs', indicating the estimated average GPU lifetime used in their analysis. The answer_unit 'years' matches the required format."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA 13B, the bare minimum hardware required is 2 V100 32GB GPUs. This is explicitly mentioned in Table II under the 'Model Size' and 'V100 32GB' columns.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The context states that for LLaMA 13B, the bare minimum hardware required is 2 V100 32GB GPUs. This is explicitly mentioned in Table II under the 'Model Size' and 'V100 32GB' columns."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context snippet from ref_id=luccioni2025c explicitly states that Google reported 1247.61 tons CO2e for training their Gemma family, which is over 4x the 'five cars' estimate. The answer_unit 'tCO2e' matches the reported value's unit, so the answer_value is 1247.61.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the 'five cars' number","The context snippet from ref_id=luccioni2025c explicitly states that Google reported 1247.61 tons CO2e for training their Gemma family, which is over 4x the 'five cars' estimate. The answer_unit 'tCO2e' matches the reported value's unit, so the answer_value is 1247.61."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context snippet from ref_id=strubell2019 explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). The question asks for the training time in hours, which matches the answer_unit specified.","336","hours","[""strubell2019""]","is_blank","report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context snippet from ref_id=strubell2019 explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). The question asks for the training time in hours, which matches the answer_unit specified."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context snippets from ref_id=li2025b explicitly state that the U.S. national average water consumption for electricity generation is 3.14 L/kWh. This matches the required answer_unit of 'L/kWh' as a numeric value.","3.14","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context snippets from ref_id=li2025b explicitly state that the U.S. national average water consumption for electricity generation is 3.14 L/kWh. This matches the required answer_unit of 'L/kWh' as a numeric value."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context snippets from [ref_id=wu2021b] and [ref_id=dodge2022] explicitly state that Google's hyperscale data centers had a PUE of 1.10 in 2021. The answer_unit 'PUE' requires the numeric value without the unit, which matches the provided data.","1.1","PUE","[""wu2021b"", ""dodge2022""]","is_blank","Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] (from wu2021b) and Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021 (from dodge2022).","The context snippets from [ref_id=wu2021b] and [ref_id=dodge2022] explicitly state that Google's hyperscale data centers had a PUE of 1.10 in 2021. The answer_unit 'PUE' requires the numeric value without the unit, which matches the provided data."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing when using 100% renewable energy. The answer_unit is 'percent', so the numeric value 96 is provided.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing when using 100% renewable energy. The answer_unit is 'percent', so the numeric value 96 is provided."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model with 2M queries using an NVIDIA H100 GPU has a net cost of $3460. The answer_unit is USD, so the numeric value is provided.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context explicitly states that fine-tuning a sparse Mixtral model with 2M queries using an NVIDIA H100 GPU has a net cost of $3460. The answer_unit is USD, so the numeric value is provided."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context snippets from ref_id=chung2025 state that GPUs account for 50–70% of the total provisioned power in the datacenter. This directly answers the question about the percentage of total provisioned power GPUs account for in a typical datacenter. The answer_unit 'percent' is satisfied by providing the numeric range [50,70].","[50, 70]","percent","[""chung2025""]","is_blank","• GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","The context snippets from ref_id=chung2025 state that GPUs account for 50–70% of the total provisioned power in the datacenter. This directly answers the question about the percentage of total provisioned power GPUs account for in a typical datacenter. The answer_unit 'percent' is satisfied by providing the numeric range [50,70]."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context snippet from ref_id=shen2024 states that the training was conducted on a cluster containing 12 nodes and 96 H100s. Since each node has 8 H100 GPUs (96 total / 12 nodes = 8 per node), the total number of H100 GPUs used is 96.","96","H100 GPUs","[""shen2024""]","is_blank","We con-duct training on a cluster containing 12 nodes and 96 H100s.","The context snippet from ref_id=shen2024 states that the training was conducted on a cluster containing 12 nodes and 96 H100s. Since each node has 8 H100 GPUs (96 total / 12 nodes = 8 per node), the total number of H100 GPUs used is 96."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power for a single node is over 600W. The answer_unit is Watts, so the numeric value is provided without the unit.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that during active training, the average GPU power for a single node is over 600W. The answer_unit is Watts, so the numeric value is provided without the unit."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context snippet from ref_id=strubell2019 states 'Human life, avg, 1 year 11,023' under the 'Consumption CO2e (lbs)' table. This directly provides the value in pounds (lbs) as requested.","11023","lbs","[""strubell2019""]","is_blank","Consumption CO 2e (lbs)
Human life, avg, 1 year 11,023","The context snippet from ref_id=strubell2019 states 'Human life, avg, 1 year 11,023' under the 'Consumption CO2e (lbs)' table. This directly provides the value in pounds (lbs) as requested."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there was a 1.25 times increase in inference throughput on A100 compared to V100. The answer_unit 'multiplier' requires expressing this as a numeric value, so 1.25 is the correct answer_value.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for LLaMA 13B, there was a 1.25 times increase in inference throughput on A100 compared to V100. The answer_unit 'multiplier' requires expressing this as a numeric value, so 1.25 is the correct answer_value."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context snippets from ref_id=jegham2025 explicitly state that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). The answer_unit 'Wh' requires a numeric value, so the answer_value is 0.42.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context snippets from ref_id=jegham2025 explicitly state that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). The answer_unit 'Wh' requires a numeric value, so the answer_value is 0.42."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with a 400 TPS SLO, Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave's top choice (g4dn.xlarge at $0.71). The answer_unit is 'percent', so the answer_value is 280.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that with a 400 TPS SLO, Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave's top choice (g4dn.xlarge at $0.71). The answer_unit is 'percent', so the answer_value is 280."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context snippet from ref_id=zschache2025 states that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient. This directly answers the question by identifying the most energy-efficient GPU architectures as V100 and A30.","V100 or A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context snippet from ref_id=zschache2025 states that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient. This directly answers the question by identifying the most energy-efficient GPU architectures as V100 and A30."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context states that Green AI involves reporting the computational price tag of finding, training, and running models, which refers to computational cost rather than financial cost. The question asks specifically about financial cost, which is not mentioned in the provided snippets.","0","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context states that Green AI involves reporting the computational price tag of finding, training, and running models, which refers to computational cost rather than financial cost. The question asks specifically about financial cost, which is not mentioned in the provided snippets."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context snippets from luccioni2025a explicitly state that electronic waste (e-waste) reached 62 million tonnes in 2022. The answer_unit is 'metric tons', so the numeric value is provided without the unit name.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context snippets from luccioni2025a explicitly state that electronic waste (e-waste) reached 62 million tonnes in 2022. The answer_unit is 'metric tons', so the numeric value is provided without the unit name."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides average system power per processor for TPU v2 and V100 GPU. From [patterson2021], TPU v2 has an average power of 221 Watts and V100 GPU has 325 Watts. The difference is 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Processor   Average   (Watts)  StDev   %  DNNs   used   to   calculate   average   power
TPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   Search   [So19]
V100   GPU   325   2%   Transformer   (Big),   GPT-3   [Sut21]","The context provides average system power per processor for TPU v2 and V100 GPU. From [patterson2021], TPU v2 has an average power of 221 Watts and V100 GPU has 325 Watts. The difference is 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context snippet from ref_id=li2025b states that training GPT-3 in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context snippet from ref_id=li2025b states that training GPT-3 in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. The answer_unit 'multiplier' requires expressing the increase as a numerical factor, which matches the 20× mentioned in the context.","20","multiplier","[""wu2021a""]","is_blank","(c) Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].","The context explicitly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. The answer_unit 'multiplier' requires expressing the increase as a numerical factor, which matches the 20× mentioned in the context."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context mentions that in Figure 8, OpenAI's reasoning models dominate the eco-efficiency frontier, with o3-mini achieving the highest cross-efficiency score (0.884). The question asks for the model name, which matches the answer_unit 'is_blank' requiring the exact identifier from the context.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825).","The context mentions that in Figure 8, OpenAI's reasoning models dominate the eco-efficiency frontier, with o3-mini achieving the highest cross-efficiency score (0.884). The question asks for the model name, which matches the answer_unit 'is_blank' requiring the exact identifier from the context."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context snippet from ref_id=wu2021b explicitly states that the current average lifetimes of cell phones are less than 3 years, which directly supports the statement in the question. The answer_unit is 'is_blank', so answer_value should be the exact identifier from the context, which is '1' for True.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","The context snippet from ref_id=wu2021b explicitly states that the current average lifetimes of cell phones are less than 3 years, which directly supports the statement in the question. The answer_unit is 'is_blank', so answer_value should be the exact identifier from the context, which is '1' for True."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that for CV models in intra-zone experiments, the per-GPU speedup (speedup/#GPUs) was almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2,3,4,6,8 GPUs respectively. This indicates nearly linear scaling, supporting the answer as True.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that for CV models in intra-zone experiments, the per-GPU speedup (speedup/#GPUs) was almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2,3,4,6,8 GPUs respectively. This indicates nearly linear scaling, supporting the answer as True."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context window size for FLM-101B is explicitly stated in the FLM-101B Configurations section of the li2025a document. The answer_unit 'tokens' requires the numeric value without the unit name.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context window size for FLM-101B is explicitly stated in the FLM-101B Configurations section of the li2025a document. The answer_unit 'tokens' requires the numeric value without the unit name."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLMs have lower power consumption during inference compared to diffusion models. This is because LLM decoding is memory-intensive and bottlenecked by VRAM bandwidth, leading to underutilized compute units and lower power draw. Diffusion models are more compute-intensive, drawing power close to the GPU's maximum.","1","is_blank","[""chung2025""]","is_blank","It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. [...] This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations [...] per byte of memory loaded is low [...] leading to the GPU’s computation units being underutilized, leading to low power draw.","The context states that LLMs have lower power consumption during inference compared to diffusion models. This is because LLM decoding is memory-intensive and bottlenecked by VRAM bandwidth, leading to underutilized compute units and lower power draw. Diffusion models are more compute-intensive, drawing power close to the GPU's maximum."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context snippets include data on gender representation in Amazon's workforce, including People Managers. Under the 'People Managers' section in the 'Gender' category, the percentages for 2023 are listed. The 'Women' percentage for 2023 is 50.2%.","50.2","percent","[""amazon2023""]","is_blank","Other Gender Men Women
...
People Managers
...
2023
50.2%49.6%","The context snippets include data on gender representation in Amazon's workforce, including People Managers. Under the 'People Managers' section in the 'Gender' category, the percentages for 2023 are listed. The 'Women' percentage for 2023 is 50.2%."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded from R&D costs, the fraction of computing hardware costs rises to 61–76% for the four key models (GPT-3, OPT-175B, GPT-4, Gemini Ultra). The answer_unit 'percent' requires a numeric range in percentage, which matches the provided data.","[61, 76]","percent","[""cottier2024""]","is_blank","With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded from R&D costs, the fraction of computing hardware costs rises to 61–76% for the four key models (GPT-3, OPT-175B, GPT-4, Gemini Ultra). The answer_unit 'percent' requires a numeric range in percentage, which matches the provided data."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a trade-off. The table shows that for all models, F1 scores and accuracy either slightly increased or decreased, but not always improved.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. [...] TABLE III COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLMS BEFORE AND AFTER OPTIMIZATION.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a trade-off. The table shows that for all models, F1 scores and accuracy either slightly increased or decreased, but not always improved."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context snippets from ref_id=strubell2019 explicitly state that the EPA provides the average CO2 produced as 0.954 pounds per kilowatt-hour (lbs/kWh) for power consumed in the U.S. This is derived from the equation CO2e = 0.954pt, where pt is the total power in kilowatt-hours. The answer_unit 'lbs/kWh' matches the required format, so the numeric value is 0.954.","0.954","lbs/kWh","[""strubell2019""]","is_blank","CO2e = 0 .954pt (2)
This conversion takes into account the relative proportions of different energy sources (primarily natural gas, coal, nuclear and renewable) consumed to produce energy in the United States.","The context snippets from ref_id=strubell2019 explicitly state that the EPA provides the average CO2 produced as 0.954 pounds per kilowatt-hour (lbs/kWh) for power consumed in the U.S. This is derived from the equation CO2e = 0.954pt, where pt is the total power in kilowatt-hours. The answer_unit 'lbs/kWh' matches the required format, so the numeric value is 0.954."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context snippets from ref_id=li2025b explicitly state that a recent study projects global AI electricity consumption in 2027 as 85–134 TWh. The answer_unit 'TWh' matches the unit provided in the context.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7], whereas a more aggressive projection by the recent U.S. data center energy report predicts that AI servers’ electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1].","The context snippets from ref_id=li2025b explicitly state that a recent study projects global AI electricity consumption in 2027 as 85–134 TWh. The answer_unit 'TWh' matches the unit provided in the context."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context explicitly states that in Europe, Amazon delivered 150 million packages via EVs in 2023. The answer_unit 'packages' requires a numeric value in millions, which matches the provided data.","150","packages","[""amazon2023""]","is_blank","Europe
• We deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles.
• We delivered 150 million packages via EVs.","The context explicitly states that in Europe, Amazon delivered 150 million packages via EVs in 2023. The answer_unit 'packages' requires a numeric value in millions, which matches the provided data."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluation used 3000 requests with 128 input tokens and 512 output tokens each. Total tokens per request: 128 + 512 = 640. Total tokens: 3000 * 640 = 1,920,000 tokens.","1920000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The online inference workload evaluation used 3000 requests with 128 input tokens and 512 output tokens each. Total tokens per request: 128 + 512 = 640. Total tokens: 3000 * 640 = 1,920,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, including those in sectors like oil and gas exploration. This directly contradicts the claim that the AI Act mandates such disclosures.","0","is_blank","[""ebert2024""]","is_blank","5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [4, 37].","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, including those in sectors like oil and gas exploration. This directly contradicts the claim that the AI Act mandates such disclosures."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context mentions that for very short experiments like DenseNet 201, which ran for less than half an hour, the Flexible Start optimization can lead to a significant reduction in CO2 emissions, up to 80% in the West US region. This is explicitly stated in Figure 3(a) and the accompanying text in the Dodge et al. (2022) paper.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context mentions that for very short experiments like DenseNet 201, which ran for less than half an hour, the Flexible Start optimization can lead to a significant reduction in CO2 emissions, up to 80% in the West US region. This is explicitly stated in Figure 3(a) and the accompanying text in the Dodge et al. (2022) paper."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that image generation has a mean energy consumption of 2.9 kWh per 1,000 inferences, while text classification uses 0.002 kWh. Dividing 2.9 by 0.002 gives 1450 times more energy, matching the answer_unit 'times'.","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that image generation has a mean energy consumption of 2.9 kWh per 1,000 inferences, while text classification uses 0.002 kWh. Dividing 2.9 by 0.002 gives 1450 times more energy, matching the answer_unit 'times'."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the Yelp sentiment analysis dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs in Yelp sentiment analysis.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the Yelp sentiment analysis dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs in Yelp sentiment analysis."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context includes a table titled 'Amazon Renewable Energy Projects' with a row for the United States showing 244 projects. The answer_unit is 'projects', so the numeric value is provided.","244","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
...
United States 244 17,706","The context includes a table titled 'Amazon Renewable Energy Projects' with a row for the United States showing 244 projects. The answer_unit is 'projects', so the numeric value is provided."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context snippets do not mention any specific percentage of renewable energy coverage by Amazon's AWS in 2018. While there are references to renewable energy achievements in 2023 and earlier years like 2022, 2021, and 2019, the 2018 data is not provided.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context snippets do not mention any specific percentage of renewable energy coverage by Amazon's AWS in 2018. While there are references to renewable energy achievements in 2023 and earlier years like 2022, 2021, and 2019, the 2018 data is not provided."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting energy consumption at the granularity of a single, whole generation response to a request (e.g., entire chat response, image, video) to balance accuracy and feasibility. This approach is chosen to avoid overly coarse or fine measurements, capturing model- and task-specific characteristics effectively.","single, whole generation response to a request","is_blank","[""chung2025""]","is_blank","Our approach is to mainly report energy consumption at the granularity of a single, whole generation response to a request (e.g., entire chat response, image, video). This is because any work less than the full response (e.g., per token) is not considered a complete request, and may ignore model- and task-specific characteristics.","The authors recommend reporting energy consumption at the granularity of a single, whole generation response to a request (e.g., entire chat response, image, video) to balance accuracy and feasibility. This approach is chosen to avoid overly coarse or fine measurements, capturing model- and task-specific characteristics effectively."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that in the BERT-base training experiment, the GPU accounted for 74% of the total energy consumption. The answer_unit is 'percent', so the numeric value 74 is provided.","74","percent","[""dodge2022""]","is_blank","The GPU alone accounts for 74% of the total energy consumption due to these components. (Table 1)","The context states that in the BERT-base training experiment, the GPU accounted for 74% of the total energy consumption. The answer_unit is 'percent', so the numeric value 74 is provided."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context snippet from ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit specifies the format as 'NVIDIA A100-80GB GPUs', so the answer_value is the numeric value 2.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context snippet from ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit specifies the format as 'NVIDIA A100-80GB GPUs', so the answer_value is the numeric value 2."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context mentions that the net effect of iterative optimization, considering Jevon's Paradox, resulted in a 28.5% operational power footprint reduction over two years. The answer_unit is 'percent', so the numeric value 28.5 is provided.","28.5","percent","[""wu2021a""]","is_blank","The net effect, with Jevon’s Paradox, is a 28.5% operational power footprint reduction over two years (Figure 8).","The context mentions that the net effect of iterative optimization, considering Jevon's Paradox, resulted in a 28.5% operational power footprint reduction over two years. The answer_unit is 'percent', so the numeric value 28.5 is provided."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context snippets do not mention any information about the installation of fiber optic cables globally in 2023 to support AI workloads. All references to infrastructure growth focus on model sizes, data ingestion, and system capacity increases, but not on fiber optic cable installations.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention any information about the installation of fiber optic cables globally in 2023 to support AI workloads. All references to infrastructure growth focus on model sizes, data ingestion, and system capacity increases, but not on fiber optic cable installations."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that the estimated hardware acquisition cost for GPT-4 is $800 million. This is directly mentioned in the cottier2024 document, which provides a comparison between acquisition cost and amortized costs.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context explicitly states that the estimated hardware acquisition cost for GPT-4 is $800 million. This is directly mentioned in the cottier2024 document, which provides a comparison between acquisition cost and amortized costs."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that 'Sustainable AI' was proposed by van Wynsberghe as a field that both aims to use AI in climate-positive applications and improve the environmental sustainability of AI approaches themselves. This directly contradicts the statement that it was proposed to 'only' encompass climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context states that 'Sustainable AI' was proposed by van Wynsberghe as a field that both aims to use AI in climate-positive applications and improve the environmental sustainability of AI approaches themselves. This directly contradicts the statement that it was proposed to 'only' encompass climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context mentions that for Mixtral, the maximum batch sizes supported on different GPUs are projected. Specifically, for NVIDIA A100-40GB with 40GB memory, the context states that the maximum batch size (MBS) for Mixtral is 17. This is found in Table IV which lists the MBS for various GPUs.","17","samples","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7
A100 80GB 17 2.74 1.67 25.4
H100 80GB 17 4.90 2.1 17.9","The context mentions that for Mixtral, the maximum batch sizes supported on different GPUs are projected. Specifically, for NVIDIA A100-40GB with 40GB memory, the context states that the maximum batch size (MBS) for Mixtral is 17. This is found in Table IV which lists the MBS for various GPUs."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is supported by Figure 8 and the explanation in Section B.1, which attributes the lower energy consumption of Swapping to avoiding extra computation during request restoration.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section).","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is supported by Figure 8 and the explanation in Section B.1, which attributes the lower energy consumption of Swapping to avoiding extra computation during request restoration."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context snippet from ref_id=luccioni2025a explicitly states that Google's 2024 report shows a 48% increase in GHG emissions since 2019. The answer_unit is 'percent', so the numeric value is provided without the percentage sign.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to 'increases in data center energy consumption' [42]","The context snippet from ref_id=luccioni2025a explicitly states that Google's 2024 report shows a 48% increase in GHG emissions since 2019. The answer_unit is 'percent', so the numeric value is provided without the percentage sign."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context snippets from [fernandez2025] mention projections that data centers will consume between 9.1% and 11.7% of total US energy demand by 2030. The question asks for the percentage of U.S. national electricity consumption, which matches the answer_unit 'percent' and the provided range.","[9.1, 11.7]","percent","[""fernandez2025""]","is_blank","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024;","The context snippets from [fernandez2025] mention projections that data centers will consume between 9.1% and 11.7% of total US energy demand by 2030. The question asks for the percentage of U.S. national electricity consumption, which matches the answer_unit 'percent' and the provided range."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context snippet [ref_id=dodge2022] states that doubling the duration of the 6B parameter transformer training run using Pause and Resume optimization can lead to significant savings up to about 25%. The answer_unit is 'percent', so the answer_value is 25.","25","percent","[""dodge2022""]","is_blank","for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context snippet [ref_id=dodge2022] states that doubling the duration of the 6B parameter transformer training run using Pause and Resume optimization can lead to significant savings up to about 25%. The answer_unit is 'percent', so the answer_value is 25."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. The answer_unit is 'percent', so the answer_value is the numeric value 70.","70","percent","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. The answer_unit is 'percent', so the answer_value is the numeric value 70."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context snippet from [ref_id=luccioni2025b] states that the 2019 study by Strubell et al. quantified BERT's training carbon footprint as 626,155 pounds of CO2 emissions. This matches the question's request for the value in lbs CO2e.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context snippet from [ref_id=luccioni2025b] states that the 2019 study by Strubell et al. quantified BERT's training carbon footprint as 626,155 pounds of CO2 emissions. This matches the question's request for the value in lbs CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context snippets from ref_id=jegham2025 and ref_id=li2025b both explicitly state that the estimated training energy for GPT-3 was 1,287 MWh. The answer_unit is MWh, so the numeric value is provided directly.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity [12]... (jegham2025) / GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29]. (li2025b)","The context snippets from ref_id=jegham2025 and ref_id=li2025b both explicitly state that the estimated training energy for GPT-3 was 1,287 MWh. The answer_unit is MWh, so the numeric value is provided directly."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that optimizing the MoE layer is crucial for improving LLM fine-tuning performance. The answer_unit 'is_blank' requires using the exact identifier from context, which in this case is '1' for True based on the statements in the Xia2024 document.","1","is_blank","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context explicitly states that optimizing the MoE layer is crucial for improving LLM fine-tuning performance. The answer_unit 'is_blank' requires using the exact identifier from context, which in this case is '1' for True based on the statements in the Xia2024 document."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides maximum batch sizes for Mixtral-D (dense setup) on different datasets. Table III in ref_id=xia2024 lists Mixtral-D's maximum batch size as 2 for the CS dataset (Hellaswag) using an NVIDIA A40 GPU with 48GB memory.","2","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context provides maximum batch sizes for Mixtral-D (dense setup) on different datasets. Table III in ref_id=xia2024 lists Mixtral-D's maximum batch size as 2 for the CS dataset (Hellaswag) using an NVIDIA A40 GPU with 48GB memory."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to fully local experiments. This directly supports the assertion in the question.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to fully local experiments. This directly supports the assertion in the question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context snippets do not mention any information regarding the hectares of land occupied by new AI data centers globally in 2022. All references to data centers discuss energy consumption, electricity usage, public health impacts, and related metrics, but not land area in hectares.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention any information regarding the hectares of land occupied by new AI data centers globally in 2022. All references to data centers discuss energy consumption, electricity usage, public health impacts, and related metrics, but not land area in hectares."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context snippet from ref_id=luccioni2024 explicitly states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 queries for text classification. The answer_unit 'g CO2eq' matches the required format, so the numeric value is provided.","0.32","g CO2eq","[""luccioni2024""]","is_blank","bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context snippet from ref_id=luccioni2024 explicitly states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 queries for text classification. The answer_unit 'g CO2eq' matches the required format, so the numeric value is provided."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context from ref_id=dodge2022 states that training BERT in different regions resulted in emissions ranging from 7k grams to 26k grams. The answer_unit is 'grams', so the range is converted to [7000, 26000].","[7000, 26000]","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context from ref_id=dodge2022 states that training BERT in different regions resulted in emissions ranging from 7k grams to 26k grams. The answer_unit is 'grams', so the range is converted to [7000, 26000]."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context explicitly states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.' This directly answers the question about the peak year before the decline, matching the 'year' answer_unit requirement.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context explicitly states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.' This directly answers the question about the peak year before the decline, matching the 'year' answer_unit requirement."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context snippet from Table 2 in morrison2025 lists OLMo 60M† with 'Equiv. to... (water usage, 1 person, U.S.)' as 5 days. The question asks for days, which matches the answer_unit.","5","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context snippet from Table 2 in morrison2025 lists OLMo 60M† with 'Equiv. to... (water usage, 1 person, U.S.)' as 5 days. The question asks for days, which matches the answer_unit."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context snippets provided do not mention Google's DeepMind AlphaFold servers or their specific water consumption in 2023. The documents discuss water consumption by AI models like GPT-3 and GPT-4, and mention companies like Google and Microsoft in general terms, but no data on AlphaFold's water usage is present.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context snippets provided do not mention Google's DeepMind AlphaFold servers or their specific water consumption in 2023. The documents discuss water consumption by AI models like GPT-3 and GPT-4, and mention companies like Google and Microsoft in general terms, but no data on AlphaFold's water usage is present."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate MoE layers is a way to reduce costs. The question asks if this action would increase costs, which contradicts the context's assertion. Therefore, the answer is False (0).","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate MoE layers is a way to reduce costs. The question asks if this action would increase costs, which contradicts the context's assertion. Therefore, the answer is False (0)."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context snippet from ref_id=li2025a provides a table (Table 3) listing the energy consumption for various models, including FLM-101B. The 'Energy (MkWh)' column shows 40 MkWh for FLM-101B, which converts to 40,000 kWh. The answer_unit is kWh, so the value is 40000.","40000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. [...] Energy (MkWh) [...] 40","The context snippet from ref_id=li2025a provides a table (Table 3) listing the energy consumption for various models, including FLM-101B. The 'Energy (MkWh)' column shows 40 MkWh for FLM-101B, which converts to 40,000 kWh. The answer_unit is kWh, so the value is 40000."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context mentions projections for 2028, not 2030. The study projects the total public health costs of U.S. data centers to reach more than $20 billion in 2028 under the high-growth scenario. Since the question asks about 2030 and no context supports projections for that year, the answer is blank.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context mentions projections for 2028, not 2030. The study projects the total public health costs of U.S. data centers to reach more than $20 billion in 2028 under the high-growth scenario. Since the question asks about 2030 and no context supports projections for that year, the answer is blank."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context mentions Table IV which estimates the cost of fine-tuning Mixtral on the MATH dataset with a sparse setup using different GPUs. The A40 GPU entry shows a cost of $32.7.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The context mentions Table IV which estimates the cost of fine-tuning Mixtral on the MATH dataset with a sparse setup using different GPUs. The A40 GPU entry shows a cost of $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context mentions that for sparse fine-tuning of Mixtral on NVIDIA A40-48GB, the maximum batch size (MBS) is 4 samples. This is found in Table IV which lists the MBS for different GPUs.","4","samples","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The context mentions that for sparse fine-tuning of Mixtral on NVIDIA A40-48GB, the maximum batch size (MBS) is 4 samples. This is found in Table IV which lists the MBS for different GPUs."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context snippets from xia2024 provide execution time breakdowns for Mixtral and BlackMamba models. Figure 4 shows the execution time breakdown for Mixtral with different batch sizes. For sparse fine-tuning with batch size 10, the total execution time (forward + backward + optimizer) is approximately 6.0 seconds as per the figure.","6.0","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)
0.0
2.0
4.0
6.0
8.0
Forward Backward Optimizer
...
Mixtral
Mamba
Fig. 4. Execution time breakdown.","The context snippets from xia2024 provide execution time breakdowns for Mixtral and BlackMamba models. Figure 4 shows the execution time breakdown for Mixtral with different batch sizes. For sparse fine-tuning with batch size 10, the total execution time (forward + backward + optimizer) is approximately 6.0 seconds as per the figure."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context from dodge2022 and chung2025 indicates that GPU-level monitoring is a common and practical approach due to GPUs being the dominant energy consumers in AI workloads. However, ebert2024 suggests that server-level measurement captures total computation-related power usage and is better suited for optimizing energy efficiency. The question asks if GPU-level is the preferred method, but the context does not explicitly state it as the recommended preferred method over others like server-level.","0","is_blank","[""dodge2022"", ""chung2025"", ""ebert2024""]","is_blank","This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency. [ref_id=ebert2024]","The context from dodge2022 and chung2025 indicates that GPU-level monitoring is a common and practical approach due to GPUs being the dominant energy consumers in AI workloads. However, ebert2024 suggests that server-level measurement captures total computation-related power usage and is better suited for optimizing energy efficiency. The question asks if GPU-level is the preferred method, but the context does not explicitly state it as the recommended preferred method over others like server-level."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context snippet from ref_id=luccioni2025b states that in the 2023 article on BLOOM's carbon footprint, training accounted for only half of the model's overall emissions. The answer_unit is 'percent', so the answer_value is 50.","50","percent","[""luccioni2025b""]","is_blank","finding that training accounted for only half of the model’s overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","The context snippet from ref_id=luccioni2025b states that in the 2023 article on BLOOM's carbon footprint, training accounted for only half of the model's overall emissions. The answer_unit is 'percent', so the answer_value is 50."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context explicitly states in Table II that the bare minimum hardware required for LLaMA-7B on V100 32GB GPUs is 1 GPU. The answer_unit 'V100_32GB_GPU' expects the number of GPUs, which is directly provided in the table.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The context explicitly states in Table II that the bare minimum hardware required for LLaMA-7B on V100 32GB GPUs is 1 GPU. The answer_unit 'V100_32GB_GPU' expects the number of GPUs, which is directly provided in the table."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context snippet [ref_id=li2025b] provides a table showing that in Arizona, it takes 16.7 requests to consume 500ml of water. The answer_unit 'requests' matches the numeric value provided in the table.","16.7","requests","[""li2025b""]","is_blank","Arizona 1.180 1.630 4.959 2.098 7.531 9.629 6.520 23.406 29.926 16.7","The context snippet [ref_id=li2025b] provides a table showing that in Arizona, it takes 16.7 requests to consume 500ml of water. The answer_unit 'requests' matches the numeric value provided in the table."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context snippet from ref_id=dodge2022 explicitly states that the average US home energy use emits 8.3 metric tons CO2 per year. The answer_unit is 'metric tons', so the numeric value 8.3 is provided without the unit name.","8.3","metric tons","[""dodge2022""]","is_blank","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context snippet from ref_id=dodge2022 explicitly states that the average US home energy use emits 8.3 metric tons CO2 per year. The answer_unit is 'metric tons', so the numeric value 8.3 is provided without the unit name."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context snippet [ref_id=shen2024] provides a table (Table 3) showing the GSM8k score for JetMoE-8B as 27.8. The question asks for the score, which matches the answer_unit 'score' as a numeric value.","27.8","score","[""shen2024""]","is_blank","GSM8k 14.5 17.3 16.9 27.8","The context snippet [ref_id=shen2024] provides a table (Table 3) showing the GSM8k score for JetMoE-8B as 27.8. The question asks for the score, which matches the answer_unit 'score' as a numeric value."
