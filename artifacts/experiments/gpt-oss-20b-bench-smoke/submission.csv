"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The paper explicitly states that it presents the ""ML.ENERGY Benchmark"" as a benchmark suite for measuring inference energy consumption.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The paper explicitly states that it presents the ""ML.ENERGY Benchmark"" as a benchmark suite for measuring inference energy consumption."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The figure in the Wu 2021a document shows the training‑only carbon footprint for each OSS model.  The bar for GShard‑600B is positioned at 0.8 on the y‑axis, which represents 0.8 million kg CO₂e.  Converting 0.8 million kg to tonnes gives 800 tCO₂e.","800","tCO2e","[""wu2021a""]","is_blank","is_blank","The figure in the Wu 2021a document shows the training‑only carbon footprint for each OSS model.  The bar for GShard‑600B is positioned at 0.8 on the y‑axis, which represents 0.8 million kg CO₂e.  Converting 0.8 million kg to tonnes gives 800 tCO₂e."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The table of evaluated models lists LLaMA‑33B with a parameter size of 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","The table of evaluated models lists LLaMA‑33B with a parameter size of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","is_blank","is_blank","MWh","[""is_blank""]","is_blank","is_blank","is_blank"
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The document states that hyperscale data centers have more than 40% higher efficiency than traditional ones, as measured by PUE, in 2020.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The document states that hyperscale data centers have more than 40% higher efficiency than traditional ones, as measured by PUE, in 2020."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The document states that GPT‑3 consumes one 500 mL bottle of water for roughly 10–50 medium‑length responses. Dividing one bottle by the midpoint of that range (≈30 responses) gives about 0.03 bottles per response.","0.03","500 mL bottles","[""li2025b""]","is_blank","is_blank","The document states that GPT‑3 consumes one 500 mL bottle of water for roughly 10–50 medium‑length responses. Dividing one bottle by the midpoint of that range (≈30 responses) gives about 0.03 bottles per response."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","percent","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act restricts the disclosure of energy consumption data to authorities only, and it is not publicly available to NGOs, analysts, or the general public. The text explicitly states that such information is not accessible to downstream providers or the public due to confidentiality clauses.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act restricts the disclosure of energy consumption data to authorities only, and it is not publicly available to NGOs, analysts, or the general public. The text explicitly states that such information is not accessible to downstream providers or the public due to confidentiality clauses."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The document states that for a GPU memory capacity of 100 GB, the analytical model predicts a maximum batch size of 28 for fine‑tuning Mixtral.","28","samples","[""xia2024""]","is_blank","is_blank","The document states that for a GPU memory capacity of 100 GB, the analytical model predicts a maximum batch size of 28 for fine‑tuning Mixtral."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","multiplier","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The table in the provided document lists the estimated water consumption for training GPT‑3 in U.S. data centers as 4.731 million liters.","4.731","liters","[""li2025b""]","is_blank","is_blank","The table in the provided document lists the estimated water consumption for training GPT‑3 in U.S. data centers as 4.731 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","is_blank","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","L/kWh","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","is_blank","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The documents state that training time is a key factor in estimating energy consumption and cost, and authors are encouraged to report it.","1","is_blank","[""luccioni2023"", ""strubell2019""]","is_blank","is_blank","The documents state that training time is a key factor in estimating energy consumption and cost, and authors are encouraged to report it."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that enabling automated resource utilization overlapping for the LLaMA‑65B model achieved a maximum latency reduction of 13.2%, as shown in Figure 14 of the Chen 2024 study.","13.2","percent","[""chen2024""]","is_blank","is_blank","The context states that enabling automated resource utilization overlapping for the LLaMA‑65B model achieved a maximum latency reduction of 13.2%, as shown in Figure 14 of the Chen 2024 study."
"q164","How much does an elephant weigh?","is_blank","is_blank","lbs","[""is_blank""]","is_blank","is_blank","is_blank"
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The provided context does not include energy consumption data for all five models (Meena, GShard-600B, Switch Transformer), so we cannot determine which has the highest.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The provided context does not include energy consumption data for all five models (Meena, GShard-600B, Switch Transformer), so we cannot determine which has the highest."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","Table 1 in the Strubell 2019 paper lists the CO₂ emissions for training BERT base as 626,155 lbs and the average American life as 36,156 lbs per year. Dividing the BERT emissions by the daily American CO₂ rate (36,156 lbs / 365 days ≈ 99 lbs/day) gives about 6,300 days of an American life.","6300","days","[""strubell2019""]","is_blank","is_blank","Table 1 in the Strubell 2019 paper lists the CO₂ emissions for training BERT base as 626,155 lbs and the average American life as 36,156 lbs per year. Dividing the BERT emissions by the daily American CO₂ rate (36,156 lbs / 365 days ≈ 99 lbs/day) gives about 6,300 days of an American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The paper explicitly states that using multiple spot instances with one T4 GPU each can be more cost‑efficient than a DGX‑2 node. For example, it notes that eight T4 spot instances can be more cost‑efficient than a DGX‑2 from the same cloud provider, even when accounting for additional egress costs.","1","is_blank","[""erben2023""]","is_blank","is_blank","The paper explicitly states that using multiple spot instances with one T4 GPU each can be more cost‑efficient than a DGX‑2 node. For example, it notes that eight T4 spot instances can be more cost‑efficient than a DGX‑2 from the same cloud provider, even when accounting for additional egress costs."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The provided context states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions or energy usage.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The provided context states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions or energy usage."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German Energy Efficiency Act (8 Nov 2023) explicitly states that data centers must run on 50 % renewable energy, increasing that factor to 100 % by 1 Jan 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The German Energy Efficiency Act (8 Nov 2023) explicitly states that data centers must run on 50 % renewable energy, increasing that factor to 100 % by 1 Jan 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","papers","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The Jegham et al. study explicitly states that recent estimates indicate inference can account for up to 90% of a model’s total lifecycle energy use.","90","percent","[""jegham2025""]","is_blank","is_blank","The Jegham et al. study explicitly states that recent estimates indicate inference can account for up to 90% of a model’s total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The AI Act’s transparency obligations for general‑purpose AI models (Article 53(1)(a) and Annex XI) mandate reporting of energy consumption during the model’s development phase but explicitly exclude inference energy consumption.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act’s transparency obligations for general‑purpose AI models (Article 53(1)(a) and Annex XI) mandate reporting of energy consumption during the model’s development phase but explicitly exclude inference energy consumption."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The excerpts from the paper state that the AI Act does not mandate disclosure of energy consumption during inference, indicating that providers are not required to report this.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The excerpts from the paper state that the AI Act does not mandate disclosure of energy consumption during inference, indicating that providers are not required to report this."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context states that new AI data centers typically use liquid cooling because of high server power densities, not air cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context states that new AI data centers typically use liquid cooling because of high server power densities, not air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The document states that platform‑level caching, applied to a CPU server baseline, improves power efficiency by a factor of 6.7× for the cross‑lingual Transformer language model.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that platform‑level caching, applied to a CPU server baseline, improves power efficiency by a factor of 6.7× for the cross‑lingual Transformer language model."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The Strubell 2019 table lists the BERT base model trained on 64 V100 GPUs for 79 hours, consuming 1507 kWh and emitting 1438 lbs of CO₂e. This matches the requested 79‑hour, 64‑GPU configuration.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The Strubell 2019 table lists the BERT base model trained on 64 V100 GPUs for 79 hours, consuming 1507 kWh and emitting 1438 lbs of CO₂e. This matches the requested 79‑hour, 64‑GPU configuration."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The paper by Chung et al. (2025) explicitly states that ML inference accounts for 80–90% of the total compute demand, a figure also echoed by other recent studies.","80-90%","percent","[""chung2025""]","is_blank","is_blank","The paper by Chung et al. (2025) explicitly states that ML inference accounts for 80–90% of the total compute demand, a figure also echoed by other recent studies."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The table in Morrison 2025 lists the energy usage of a 7‑billion‑parameter model as equivalent to 13 years 6 months of electricity consumption for a single U.S. household. A 6.1‑billion‑parameter model is essentially the same size, so the household‑year equivalent is the same value, 13.5 years.","13.5","household-years","[""morrison2025""]","is_blank","is_blank","The table in Morrison 2025 lists the energy usage of a 7‑billion‑parameter model as equivalent to 13 years 6 months of electricity consumption for a single U.S. household. A 6.1‑billion‑parameter model is essentially the same size, so the household‑year equivalent is the same value, 13.5 years."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The document states that in geo‑distributed NLP experiments the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). It also notes that egress costs account for more than 90% of the total cost in NLP experiments on GC and Azure.","1","is_blank","[""erben2023""]","is_blank","is_blank","The document states that in geo‑distributed NLP experiments the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). It also notes that egress costs account for more than 90% of the total cost in NLP experiments on GC and Azure."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The report states that JetMoE‑8B was trained for 30,000 H100 GPU hours and the training cluster used 96 H100 GPUs. Dividing the total GPU hours by the number of GPUs gives the wall‑clock time in hours, which is then converted to days.","13.02","days","[""shen2024""]","is_blank","is_blank","The report states that JetMoE‑8B was trained for 30,000 H100 GPU hours and the training cluster used 96 H100 GPUs. Dividing the total GPU hours by the number of GPUs gives the wall‑clock time in hours, which is then converted to days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines the term as the amount of water that is evaporated, transpired, or incorporated into products, and specifies it as ""water withdrawal minus water discharge"". This definition is explicitly stated in the provided snippets.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context defines the term as the amount of water that is evaporated, transpired, or incorporated into products, and specifies it as ""water withdrawal minus water discharge"". This definition is explicitly stated in the provided snippets."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The document states that for LLaMA‑65B the energy per second ranges from about 300 Watts at the lowest shard count (8 GPUs) up to roughly 1 Kilowatt at the highest shard count (32 GPUs).","300-1000 W","W","[""samsi2024""]","is_blank","is_blank","The document states that for LLaMA‑65B the energy per second ranges from about 300 Watts at the lowest shard count (8 GPUs) up to roughly 1 Kilowatt at the highest shard count (32 GPUs)."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","multiplier","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The table shows Qwen’s carbon emissions dropping from 0.009 kg to 0.004 kg after quantization and local inference. The reduction is (0.009–0.004)/0.009 ≈ 0.556, i.e., about 55.6 %.","55.6","percent","[""khan2025""]","is_blank","is_blank","The table shows Qwen’s carbon emissions dropping from 0.009 kg to 0.004 kg after quantization and local inference. The reduction is (0.009–0.004)/0.009 ≈ 0.556, i.e., about 55.6 %."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The abstract of the paper states that the early 2025 iteration of the ML.ENERGY Benchmark includes energy measurements for 40 widely used model architectures across 6 different tasks.","40","models","[""chung2025""]","is_blank","is_blank","The abstract of the paper states that the early 2025 iteration of the ML.ENERGY Benchmark includes energy measurements for 40 widely used model architectures across 6 different tasks."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","Unable to answer with confidence based on the provided documents.","is_blank","USD","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
