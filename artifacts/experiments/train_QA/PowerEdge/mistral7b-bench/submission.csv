"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context provides details about the ML.ENERGY Benchmark, which is a benchmark suite for measuring inference energy consumption of generative AI models. However, the name of the benchmark suite is not explicitly mentioned in the provided context.","is_blank","is_blank","[""chung2025""]","is_blank","is_blank","The context provides details about the ML.ENERGY Benchmark, which is a benchmark suite for measuring inference energy consumption of generative AI models. However, the name of the benchmark suite is not explicitly mentioned in the provided context."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context provides information about the net CO2e emissions from training the GShard-600B model, which is 4.3 tCO2e (teragrams of Carbon Dioxide equivalent).","4.3","tCO2e","[""patterson2021""]","is_blank","Training GShard-600B used 24 MWh and produced 4.3 net tCO 2 e.","The context provides information about the net CO2e emissions from training the GShard-600B model, which is 4.3 tCO2e (teragrams of Carbon Dioxide equivalent)."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The LLaMA-33B model size is provided in the table listed in the context under the 'G' column, which shows a value of 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","Table 3: Large language models used for evaluation.","The LLaMA-33B model size is provided in the table listed in the context under the 'G' column, which shows a value of 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific information about the electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours. However, it does mention the total energy consumption of Google in 2019, which was 12.2 TeraWatt-hours. Since the context does not indicate a significant change in energy consumption patterns or the number of TPU pods between 2019 and 2023, it can be inferred that the electricity consumption of all Google Cloud TPU pods worldwide in 2023 would be greater than or equal to 12.2 TeraWatt-hours, but the exact value is not available in the provided context.","is_blank","MWh","[""patterson2021""]","[""https://arxiv.org/abs/2105.01353""]","For 2019, the total energy consumption of Google was 12.2 TeraWatt-hours [patterson2021].","The context does not provide specific information about the electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours. However, it does mention the total energy consumption of Google in 2019, which was 12.2 TeraWatt-hours. Since the context does not indicate a significant change in energy consumption patterns or the number of TPU pods between 2019 and 2023, it can be inferred that the electricity consumption of all Google Cloud TPU pods worldwide in 2023 would be greater than or equal to 12.2 TeraWatt-hours, but the exact value is not available in the provided context."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context discusses the efficiency of hyperscale data centers, specifically mentioning that between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1).","1","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/""]","Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efﬁciency for hyperscale data centers (Figure 1).","The context discusses the efficiency of hyperscale data centers, specifically mentioning that between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1)."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses. It does not specify the exact volume of water consumed for a single response, but it can be inferred that the total volume for multiple responses would be proportional.","50","500 mL bottles","[""li2025b""]","is_blank","Additionally, GPT-3 needs to ‘drink’ (i.e., consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses. It does not specify the exact volume of water consumed for a single response, but it can be inferred that the total volume for multiple responses would be proportional."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context provides a sample of 60 papers from top AI conferences, and for each paper, it notes whether the authors claim their main contribution to be an improvement to accuracy or efficiency. It is stated that 75% of CVPR papers target accuracy, and no specific percentage is given for papers that target both accuracy and efficiency. No information is provided about the percentage of CVPR papers that target efficiency.","is_blank","percent","[""schwartz2019""]","[""http://cvpr2019.thecvf.com""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers).","The context provides a sample of 60 papers from top AI conferences, and for each paper, it notes whether the authors claim their main contribution to be an improvement to accuracy or efficiency. It is stated that 75% of CVPR papers target accuracy, and no specific percentage is given for papers that target both accuracy and efficiency. No information is provided about the percentage of CVPR papers that target efficiency."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context does not clearly support a definitive answer. The AI Act discussed in the context focuses on energy consumption reporting for data centers, but it does not explicitly mention energy consumption data from AI inferences being publicly available to NGOs, analysts, and the general public.","is_blank","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","is_blank","The context does not clearly support a definitive answer. The AI Act discussed in the context focuses on energy consumption reporting for data centers, but it does not explicitly mention energy consumption data from AI inferences being publicly available to NGOs, analysts, and the general public."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides a table (ref_id=xia2024) that lists the maximum batch size for different GPUs, including a GPU with 100GB memory capacity. For this GPU, the maximum batch size supported for fine-tuning Mixtral is 28.","28","samples","[""xia2024""]","is_blank","0 20 40 60 80 100 1200
5
10
15
20
25
30
35
40
A100-40GB
A100-80GB
A40
H100
bsz=28
bsz=35
Projected GPU capacity
Ground Truth Projection
Max batch size
GPU DRAM capacity
Fig. 13. Projected maximum batch size of Mixtral for different GPUs.","The context provides a table (ref_id=xia2024) that lists the maximum batch size for different GPUs, including a GPU with 100GB memory capacity. For this GPU, the maximum batch size supported for fine-tuning Mixtral is 28."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides a comparison of the inference performance of LLaMA models on V100 and A100 GPUs. Specifically, it mentions that for LLaMA 7B, there is a 2 times increase in inference latency on the A100 when compared to the V100. However, the context does not provide a specific speedup factor for the 7B model when using A100 GPUs compared to V100 GPUs for the inference throughput.","is_blank","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context provides a comparison of the inference performance of LLaMA models on V100 and A100 GPUs. Specifically, it mentions that for LLaMA 7B, there is a 2 times increase in inference latency on the A100 when compared to the V100. However, the context does not provide a specific speedup factor for the 7B model when using A100 GPUs compared to V100 GPUs for the inference throughput."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides an estimate of the operational water consumption for GPT-3's training phase in Microsoft's data centers. The estimate is based on the annualized average on-site PUE and WUE for each location, as reported by Microsoft, and the estimated training energy of 1287 MWh for GPT-3. The water consumption for each request is calculated using the on-site and off-site water consumption values for each location from the table in the context.","is_blank","liters","[""li2025b""]","is_blank","Table 1 in the context","The context provides an estimate of the operational water consumption for GPT-3's training phase in Microsoft's data centers. The estimate is based on the annualized average on-site PUE and WUE for each location, as reported by Microsoft, and the estimated training energy of 1287 MWh for GPT-3. The water consumption for each request is calculated using the on-site and off-site water consumption values for each location from the table in the context."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context discusses the operationalization of sustainability impact assessments (SIAs) within the risk assessments required under the AI Act, which involves integrating environmental considerations into the existing risk management frameworks that high-risk AI model providers and GPAI providers must follow. It is stated that these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety. This suggests that the authors propose that SIAs should apply not just to high-risk AI but across all AI systems.","1","is_blank","[""ebert2024""]","is_blank","6.4 Sustainability Impact Assessments","The context discusses the operationalization of sustainability impact assessments (SIAs) within the risk assessments required under the AI Act, which involves integrating environmental considerations into the existing risk management frameworks that high-risk AI model providers and GPAI providers must follow. It is stated that these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety. This suggests that the authors propose that SIAs should apply not just to high-risk AI but across all AI systems."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context provides information about AWS's water use effectiveness (WUE) in 2023, which is stated to be 0.18 liters of water per kilowatt-hour (L/kWh).","0.18","L/kWh","[""amazon2023""]","is_blank","As of 2023, AWS’s industry-leading global data center WUE is 0.18 liters of water per kilowatt-hour (L/kWh) ([amazon2023])","The context provides information about AWS's water use effectiveness (WUE) in 2023, which is stated to be 0.18 liters of water per kilowatt-hour (L/kWh)."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context discusses the deployment of large language models (LLMs) and emphasizes the use of local inference as a sustainability measure. Local inference is mentioned as a method that reduces both network overhead and carbon footprint when deploying large language models.","1","is_blank","[""khan2025""]","is_blank","To address these concerns, this study proposes a framework for LLM deployment that emphasizes local inference, aiming to mitigate environmental impact while preserving model performance and user experience.","The context discusses the deployment of large language models (LLMs) and emphasizes the use of local inference as a sustainability measure. Local inference is mentioned as a method that reduces both network overhead and carbon footprint when deploying large language models."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context discusses the calculation of carbon emissions during model training, which includes the power consumption of the hardware used, the training time, and the carbon intensity of the energy grid. It is stated that the power consumption of the hardware used can be calculated using the Thermal Design Power (TDP) of the hardware, which is a measure of the energy it needs under the maximum theoretical load.","is_blank","is_blank","[""luccioni2023""]","is_blank","Hardware power. In order to calculate the power consumption of the hardware used for model training, we refer to its Thermal Design Power, or TDP, which indicates the energy it needs under the maximum theoretical load.","The context discusses the calculation of carbon emissions during model training, which includes the power consumption of the hardware used, the training time, and the carbon intensity of the energy grid. It is stated that the power consumption of the hardware used can be calculated using the Thermal Design Power (TDP) of the hardware, which is a measure of the energy it needs under the maximum theoretical load."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context provides a figure (Figure 14) that shows the performance improvement achieved by enabling the automated resource utilization overlapping feature for the LLaMA-65B model. The figure illustrates that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% reduction in latency with the feature enabled.","13.2","percent","[""chen2024""]","is_blank","Figure 14","The context provides a figure (Figure 14) that shows the performance improvement achieved by enabling the automated resource utilization overlapping feature for the LLaMA-65B model. The figure illustrates that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% reduction in latency with the feature enabled."
"q164","How much does an elephant weigh?","The context does not provide a direct answer to the question about the weight of an elephant. However, it does mention the environmental impact of AI models in terms of power usage, carbon emissions, and water consumption. While the context does not specify the weight of an elephant, it does provide information about the environmental impact of AI models, which can be indirectly related to the question if we consider the energy consumption of AI models and the energy consumption of elephants in their natural habitats.","is_blank","lbs","[""morrison2025""]","is_blank","is_blank","The context does not provide a direct answer to the question about the weight of an elephant. However, it does mention the environmental impact of AI models in terms of power usage, carbon emissions, and water consumption. While the context does not specify the weight of an elephant, it does provide information about the environmental impact of AI models, which can be indirectly related to the question if we consider the energy consumption of AI models and the energy consumption of elephants in their natural habitats."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides energy consumption values for several large NLP DNNs, including T5, Meena, GShard-600B, Switch Transformer, and GPT-3. The energy consumption values for T5, Meena, and GPT-3 are explicitly stated, while the energy consumption for GShard-600B and Switch Transformer are estimated. No energy consumption value is provided for GShard, so it is not possible to determine which of the five large NLP DNNs has the highest energy consumption based on the provided context.","is_blank","is_blank","[""patterson2021""]","is_blank","Table 4 and Appendix A show a CO 2 e calculation for five of them: T5, Meena, GShard, and Switch Transformer from Google plus GPT-3 from Open AI that runs on Microsoft Azure Cloud: ... Table 4 and Appendix A show a CO 2 e calculation for T5, Meena, GShard, and Switch Transformer, but no CO 2 e calculation is shown for GShard-600B or Switch Transformer, only an estimate.","The context provides energy consumption values for several large NLP DNNs, including T5, Meena, GShard-600B, Switch Transformer, and GPT-3. The energy consumption values for T5, Meena, and GPT-3 are explicitly stated, while the energy consumption for GShard-600B and Switch Transformer are estimated. No energy consumption value is provided for GShard, so it is not possible to determine which of the five large NLP DNNs has the highest energy consumption based on the provided context."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context provides information about the carbon footprint of training different models, including BERT. Specifically, it is stated that training BERT on GPU is roughly equivalent to a trans-American flight, which implies a significant amount of CO2 emissions. However, the context does not provide a specific number of days for the CO2 emissions from an average American life equivalent to training BERT.","is_blank","days","[""strubell2019""]","[""https://arxiv.org/abs/1909.05858""]","Training BERT on GPU is roughly equivalent to a trans-American flight.","The context provides information about the carbon footprint of training different models, including BERT. Specifically, it is stated that training BERT on GPU is roughly equivalent to a trans-American flight, which implies a significant amount of CO2 emissions. However, the context does not provide a specific number of days for the CO2 emissions from an average American life equivalent to training BERT."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context provides data in Table 4 that compares the Evolved Transformer and the Transformer (Big) model on the WMT'24 EN-DE BLUE task. It is shown that the Evolved Transformer has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure than the Transformer (Big) model. However, the question asks about the performance of the Evolved Transformer architecture compared to the Evolved Transformers architecture on the same task, which is not directly addressed in the provided context.","is_blank","is_blank","[""patterson2021""]","is_blank","Table 4","The context provides data in Table 4 that compares the Evolved Transformer and the Transformer (Big) model on the WMT'24 EN-DE BLUE task. It is shown that the Evolved Transformer has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure than the Transformer (Big) model. However, the question asks about the performance of the Evolved Transformer architecture compared to the Evolved Transformers architecture on the same task, which is not directly addressed in the provided context."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The provided context does not mention a specific dataset used for testing energy-efficient large language models in the financial domain.","is_blank","is_blank","[""fernandez2025""]","is_blank","is_blank","The provided context does not mention a specific dataset used for testing energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context discusses the cost-efficiency of using multiple T4 spot instances for distributed training compared to a DGX-2 node. It is stated that T4 instances can be more cost-efficient than a DGX-2 node or the A10 offerings from LambdaLabs.","1","is_blank","[""erben2023""]","is_blank","By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.","The context discusses the cost-efficiency of using multiple T4 spot instances for distributed training compared to a DGX-2 node. It is stated that T4 instances can be more cost-efficient than a DGX-2 node or the A10 offerings from LambdaLabs."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context discusses the environmental impacts of AI, including its energy consumption and greenhouse gas emissions. It mentions that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage, indicating that it does not address these environmental concerns.","1","is_blank","[""luccioni2025b""]","is_blank","Similarly, sustainability considerations were also lacking in the 2023 US Executive Or der regarding AI [20], which did not mention AI’s green-house gas emissions nor energy usage,","The context discusses the environmental impacts of AI, including its energy consumption and greenhouse gas emissions. It mentions that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage, indicating that it does not address these environmental concerns."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context provides information about the EU Data Collection and Reporting Obligations for Data Centers established by the Energy Efficiency Directive EU/2023/1791 and the Commission Delegated Regulation EU/2024/1364. These rules apply to all data centers in the EU with a power demand of the installed information technology (IT) of at least 500kW, which includes small-sized data centers. The required data includes energy consumption, power utilization, temperature set points, waste heat utilization, water usage, and use of renewable energy. The Delegated Regulation provides specific key performance indicators and methodology, most notable is the requirement to measure and report the energy consumption of the installed information technology.","1","is_blank","[""ebert2024""]","[""https://www.europa.eu/""]","The required data includes water usage and use of renewable energy (EED, Annex VII(c)).","The context provides information about the EU Data Collection and Reporting Obligations for Data Centers established by the Energy Efficiency Directive EU/2023/1791 and the Commission Delegated Regulation EU/2024/1364. These rules apply to all data centers in the EU with a power demand of the installed information technology (IT) of at least 500kW, which includes small-sized data centers. The required data includes energy consumption, power utilization, temperature set points, waste heat utilization, water usage, and use of renewable energy. The Delegated Regulation provides specific key performance indicators and methodology, most notable is the requirement to measure and report the energy consumption of the installed information technology."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context provides a sample of 60 papers from top AI conferences, and for each paper, it is noted whether the authors claim their main contribution to be an improvement to accuracy, efficiency, both, or other. However, the context does not specify which papers targeted both accuracy and efficiency.","is_blank","papers","[""schwartz2019""]","[""https://acl2018.org"", ""https://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com""]","For each paper, it is noted whether the authors claim their main contribution to be an improvement to accuracy, efficiency, both, or other.","The context provides a sample of 60 papers from top AI conferences, and for each paper, it is noted whether the authors claim their main contribution to be an improvement to accuracy, efficiency, both, or other. However, the context does not specify which papers targeted both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context provides an estimate that inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025""]","is_blank","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].","The context provides an estimate that inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context discusses the AI Act and its provisions, but it does not clearly state whether the Act requires providers to report both training and inference energy consumption for general-purpose AI models. The Act is discussed in terms of energy consumption during the development phase, but there is no mention of energy consumption during the inference phase.","is_blank","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","is_blank","The context discusses the AI Act and its provisions, but it does not clearly state whether the Act requires providers to report both training and inference energy consumption for general-purpose AI models. The Act is discussed in terms of energy consumption during the development phase, but there is no mention of energy consumption during the inference phase."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context discusses the AI Act, which aims to address climate-related concerns in AI development and deployment. The Act is proposed to bring reporting on energy consumption from AI inferences back into the scope, but it does not currently mandate the disclosure of energy consumption during the inference phase. The Act also fails to address indirect greenhouse gas emissions from AI applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The context discusses the AI Act, which aims to address climate-related concerns in AI development and deployment. The Act is proposed to bring reporting on energy consumption from AI inferences back into the scope, but it does not currently mandate the disclosure of energy consumption during the inference phase. The Act also fails to address indirect greenhouse gas emissions from AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context discusses the water usage of AI data centers, focusing on the water consumption during the cooling process. It mentions that nearly all the server energy is converted into heat, which must be removed to avoid overheating. This process involves two stages: server-level cooling and facility-level cooling. In the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods. However, the context specifically states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. In the facility-level cooling stage, heat is rejected from the data center facility to the outside environment, and water-intensive cooling towers and water evaporation-assisted air cooling are two common approaches used in many data centers. Therefore, it can be inferred that new AI data centers often rely on liquid cooling, which may involve water consumption.","1","is_blank","[""li2025b""]","is_blank","In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.","The context discusses the water usage of AI data centers, focusing on the water consumption during the cooling process. It mentions that nearly all the server energy is converted into heat, which must be removed to avoid overheating. This process involves two stages: server-level cooling and facility-level cooling. In the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods. However, the context specifically states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. In the facility-level cooling stage, heat is rejected from the data center facility to the outside environment, and water-intensive cooling towers and water evaporation-assisted air cooling are two common approaches used in many data centers. Therefore, it can be inferred that new AI data centers often rely on liquid cooling, which may involve water consumption."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context discusses the optimization of the operational power footprint for a cross-lingual Transformer language model. It mentions that platform-level caching, GPU acceleration, low precision data format, and algorithmic optimization can significantly reduce the infrastructure resources required to serve the model at scale. The compounded benefits of these optimizations are stated to reduce the operational energy footprint by more than 800×.","is_blank","multiplier","[""wu2021a""]","is_blank","Figure 7","The context discusses the optimization of the operational power footprint for a cross-lingual Transformer language model. It mentions that platform-level caching, GPU acceleration, low precision data format, and algorithmic optimization can significantly reduce the infrastructure resources required to serve the model at scale. The compounded benefits of these optimizations are stated to reduce the operational energy footprint by more than 800×."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The provided context does not directly provide the CO2 emissions in pounds from training a BERT base model for 79 hours using 64 V100 GPUs. However, it does mention that the BERT base model was trained on 16 TPU chips for 4 days (96 hours) and on 4 DGX-2H servers, totaling 64 Tesla V100 GPUs. Since 4 days is approximately 79 hours, we can estimate that the CO2 emissions for 79 hours of training on 64 V100 GPUs would be similar to the emissions for 4 days of training on 16 TPU chips. The context does not provide a specific value for the CO2 emissions for the 4 days of training on 16 TPU chips, but a related study (ref_id: luccioni2025b) mentions that training BERT reached 626,155 pounds of CO2 emissions. Therefore, we can estimate that the CO2 emissions for 79 hours of training on 64 V100 GPUs would be greater than 626,155 pounds.","is_blank","lbs","[""strubell2019"", ""luccioni2025b""]","[""https://arxiv.org/abs/1909.05858"", ""https://arxiv.org/abs/2503.07486""]","BERT base model was trained on 16 TPU chips for 4 days (96 hours) and on 4 DGX-2H servers, totaling 64 Tesla V100 GPUs. The CO2 emissions for training BERT reached 626,155 pounds.","The provided context does not directly provide the CO2 emissions in pounds from training a BERT base model for 79 hours using 64 V100 GPUs. However, it does mention that the BERT base model was trained on 16 TPU chips for 4 days (96 hours) and on 4 DGX-2H servers, totaling 64 Tesla V100 GPUs. Since 4 days is approximately 79 hours, we can estimate that the CO2 emissions for 79 hours of training on 64 V100 GPUs would be similar to the emissions for 4 days of training on 16 TPU chips. The context does not provide a specific value for the CO2 emissions for the 4 days of training on 16 TPU chips, but a related study (ref_id: luccioni2025b) mentions that training BERT reached 626,155 pounds of CO2 emissions. Therefore, we can estimate that the CO2 emissions for 79 hours of training on 64 V100 GPUs would be greater than 626,155 pounds."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context states that ML inference reportedly accounts for 80-90% of the total compute demand according to several sources, including Chung et al. (2025), Patterson et al. (2021), and Luccioni et al. (2024).","80-90","percent","[""chung2025"", ""patterson2021"", ""luccioni2024""]","[""https://arxiv.org/abs/2505.06371"", ""https://arxiv.org/abs/2203.08144"", ""https://dl.acm.org/doi/10.1145/3518136.3545142""]","Chung et al. (2025) states 'ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60]'. Patterson et al. (2021) states 'NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19]'. Luccioni et al. (2024) states 'inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28]'.","The context states that ML inference reportedly accounts for 80-90% of the total compute demand according to several sources, including Chung et al. (2025), Patterson et al. (2021), and Luccioni et al. (2024)."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides information about the energy consumption of training a 6.1B-parameter language model on 256 NVIDIA A100s for 8 days, which amounts to 13.8 MWh. To convert this to household-years, we can assume an average annual household electricity consumption in the US is about 10,766 kWh (source: https://www.eia.gov/electricity/data/browser/). Dividing the energy consumption of the language model by the annual household electricity consumption gives us the number of household-years. However, the context does not provide the number of households, so we cannot calculate the exact number of household-years.","1.26","household-years","[""dodge2022""]","[""https://arxiv.org/abs/2206.08535""]","The total energy amounted to a staggering 13.8 MWh.","The context provides information about the energy consumption of training a 6.1B-parameter language model on 256 NVIDIA A100s for 8 days, which amounts to 13.8 MWh. To convert this to household-years, we can assume an average annual household electricity consumption in the US is about 10,766 kWh (source: https://www.eia.gov/electricity/data/browser/). Dividing the energy consumption of the language model by the annual household electricity consumption gives us the number of household-years. However, the context does not provide the number of households, so we cannot calculate the exact number of household-years."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context provides information about the costs of different cloud providers for various services, including egress costs. For the NLP experiments, the external egress cost for Google Cloud (GC) is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""erben2023""]","is_blank","The costs are detailed in the 'VM On-Demand, VM Spot, Egress Internal, Egress External, and Dataloading' tables in the context.","The context provides information about the costs of different cloud providers for various services, including egress costs. For the NLP experiments, the external egress cost for Google Cloud (GC) is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides information about the JetMoE-8B model, including the total number of tokens used for training (1.25T) and the number of GPU hours (30,000 H100 GPU hours). However, it does not specify the pre-training GPU hours for the JetMoE-8B model or the number of GPUs used during pre-training. Therefore, it is not possible to estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","is_blank","days","[""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat""]","This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","The context provides information about the JetMoE-8B model, including the total number of tokens used for training (1.25T) and the number of GPU hours (30,000 H100 GPU hours). However, it does not specify the pre-training GPU hours for the JetMoE-8B model or the number of GPUs used during pre-training. Therefore, it is not possible to estimate the total wall-clock time in days required to pre-train the JetMoE-8B model."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines water consumption as 'water withdrawal minus water discharge', and means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'. It is further specified that water consumption reflects the impact on downstream water availability and is crucial for assessing watershed-level scarcity.","is_blank","is_blank","[""li2025b""]","is_blank","• Water consumption:It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13].","The context defines water consumption as 'water withdrawal minus water discharge', and means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'. It is further specified that water consumption reflects the impact on downstream water availability and is crucial for assessing watershed-level scarcity."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context provides information about the energy per second for inference with LLaMA 65B across different shard configurations. It is stated that the energy per second is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","300-1000","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context provides information about the energy per second for inference with LLaMA 65B across different shard configurations. It is stated that the energy per second is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context provides information about the energy consumption of different models in zero-shot classification. It mentions that the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, with a minor accuracy reduction of 0.07 points.","7","multiplier","[""zschache2025""]","is_blank","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context provides information about the energy consumption of different models in zero-shot classification. It mentions that the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, with a minor accuracy reduction of 0.07 points."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context provides a table (Table III) comparing the performance metrics and carbon emissions for five LLMs before and after optimization. The table shows a reduction in carbon emissions for the model Qwen when applying quantization and local inference for sentiment classification.","0.004","percent","[""khan2025""]","is_blank","Table III","The context provides a table (Table III) comparing the performance metrics and carbon emissions for five LLMs before and after optimization. The table shows a reduction in carbon emissions for the model Qwen when applying quantization and local inference for sentiment classification."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context discusses the ML.ENERGY Benchmark, which includes various generative AI model architectures across different tasks. However, the number of architectures is not explicitly stated.","is_blank","models","[""chung2025""]","is_blank","generative AI model architectures across a wide range of tasks – including Large Language Model (LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models – and (2) more up-to-date hardware and software stacks following rapid advancements in each area.","The context discusses the ML.ENERGY Benchmark, which includes various generative AI model architectures across different tasks. However, the number of architectures is not explicitly stated."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context does not provide specific information about the health cost, in USD, when training a Llama-3.1 scale model in Altoona, Iowa. However, it does mention that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that exceeds 120% of the training electricity cost. Since electricity cost is not specified for Altoona, Iowa, it is impossible to calculate the health cost based on this information.","is_blank","USD","[""han2024""]","is_blank","is_blank","The context does not provide specific information about the health cost, in USD, when training a Llama-3.1 scale model in Altoona, Iowa. However, it does mention that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that exceeds 120% of the training electricity cost. Since electricity cost is not specified for Altoona, Iowa, it is impossible to calculate the health cost based on this information."
