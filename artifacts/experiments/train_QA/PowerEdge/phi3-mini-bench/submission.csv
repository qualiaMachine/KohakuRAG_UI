"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions the ML.ENERGY Benchmark and Leaderboard as tools for measuring and optimizing the energy consumption of generative AI models, which includes a wide range of tasks and model architectures. The benchmark suite is described as extensible and provides a comprehensive set of tools for this purpose.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://github.com/ml-energy/benchmark"", ""https://ml.energy/leaderboard""]","is_blank","The context mentions the ML.ENERGY Benchmark and Leaderboard as tools for measuring and optimizing the energy consumption of generative AI models, which includes a wide range of tasks and model architectures. The benchmark suite is described as extensible and provides a comprehensive set of tools for this purpose."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context provides specific information about the energy consumption and resulting CO2e emissions from training the GShard-600B model, stating that it used 24 MWh and produced 4.3 net tCO2e.","4.3","tCO2e","[""patterson2021""]","is_blank","is_blank","The context provides specific information about the energy consumption and resulting CO2e emissions from training the GShard-600B model, stating that it used 24 MWh and produced 4.3 net tCO2e."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides specific model sizes for LLaMA-33B, LLaMA-65B, and LLaMA3-70B models, indicating that LLaMA-33B has 64.7 GB of parameters.","64.7","GB","[""chen2024"", ""samsi2024"", ""luccioni2025c"", ""han2024""]","[""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md"", ""https://github.com/meta-llama/llama-models/blob/main/MODEL_CARD.md"", ""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md"", ""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md""]","Model Parameters L d G LLaMA-33B 64.7 GB 60 6656 1","The context provides specific model sizes for LLaMA-33B, LLaMA-65B, and LLaMA3-70B models, indicating that LLaMA-33B has 64.7 GB of parameters."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide specific information on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context directly states that between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference with more than 40% higher efficiency for hyperscale data centers.","1","is_blank","[""wu2021b"", ""patterson2021""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://www.google.com/about/datacenters/efficiency/""]","PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]; PUE of Facebook datacenters is 1.10 (2020) [Facebook]; the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context directly states that between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference with more than 40% higher efficiency for hyperscale data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","","","500 mL bottles","[]","is_blank","is_blank",""
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context indicates that a large majority of papers from top AI conferences, including CVPR, target accuracy over efficiency, with only a small portion arguing for efficiency results.","75","percent","[""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019""]","[""http://cvpr2019.thecvf.com"", ""http://acl2018.org"", ""http://nips.cc/Conferences/2018"", ""http://acl2018.org"", ""http://cvpr2019.thecvf.com"", ""http://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com"", ""http://cvpr2019.thecvf.com""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efﬁciency result.","The context indicates that a large majority of papers from top AI conferences, including CVPR, target accuracy over efficiency, with only a small portion arguing for efficiency results."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context discusses the AI Act's shortcomings in addressing environmental impacts of AI, specifically the lack of mandatory reporting for energy consumption during inference and the failure to address indirect greenhouse gas emissions. It suggests policy proposals to include energy consumption in reporting and clarify provider responsibilities for AI models.","1","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The AI Act contains energy- and climate-related transparency and risk management obligations, primarily for providers of AI systems or models. However, it falls short of a comprehensive framework for mitigating the environmental risks posed by AI systems.","The context discusses the AI Act's shortcomings in addressing environmental impacts of AI, specifically the lack of mandatory reporting for energy consumption during inference and the failure to address indirect greenhouse gas emissions. It suggests policy proposals to include energy consumption in reporting and clarify provider responsibilities for AI models."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides a projected maximum batch size for fine-tuning Mixtral on GPUs with capacities of 100GB and 120GB, stating it will be 28 and 35, respectively.","28, 35","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context provides a projected maximum batch size for fine-tuning Mixtral on GPUs with capacities of 100GB and 120GB, stating it will be 28 and 35, respectively."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context indicates that using A100 GPUs results in anywhere from a 2 times to a 1.25 times increase in inference latency compared to V100 GPUs for smaller LLaMA models, specifically 7B and 13B.","1.25 to 2","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context indicates that using A100 GPUs results in anywhere from a 2 times to a 1.25 times increase in inference latency compared to V100 GPUs for smaller LLaMA models, specifically 7B and 13B."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides an estimate of GPT-3's operational water consumption footprint in Microsoft's U.S. data centers, with specific figures given for water consumption per kilowatt-hour (L/kWh) and total water consumption in million liters. The table and accompanying text directly state the water consumption values.","2200","liters","[""li2025b""]","is_blank","is_blank","The context provides an estimate of GPT-3's operational water consumption footprint in Microsoft's U.S. data centers, with specific figures given for water consumption per kilowatt-hour (L/kWh) and total water consumption in million liters. The table and accompanying text directly state the water consumption values."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","","","is_blank","[]","is_blank","is_blank",""
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context provides information on AWS's water use effectiveness (WUE) in 2023, stating it as 0.18 liters of water per kilowatt-hour (L/kWh), which is a 5% improvement from 2022 and a 28% improvement since 2021. AWS is working on optimizing water consumption and aims to improve WUE by reducing incoming water use, with strategies including using IoT technologies to identify leaks and investing in water replenishment projects.","0.18","L/kWh","[""amazon2023"", ""ebert2024"", ""li2025b""]","is_blank","is_blank","The context provides information on AWS's water use effectiveness (WUE) in 2023, stating it as 0.18 liters of water per kilowatt-hour (L/kWh), which is a 5% improvement from 2022 and a 28% improvement since 2021. AWS is working on optimizing water consumption and aims to improve WUE by reducing incoming water use, with strategies including using IoT technologies to identify leaks and investing in water replenishment projects."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context discusses the benefits of local inference in reducing network overhead and carbon footprint, as well as the use of quantization to lower computational and memory requirements without significantly compromising model performance.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/3483410""]","Local inference allows models to run directly on user devices while maintaining data privacy, significantly reducing both network overhead and carbon footprint.","The context discusses the benefits of local inference in reducing network overhead and carbon footprint, as well as the use of quantization to lower computational and memory requirements without significantly compromising model performance."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context discusses the importance of estimating compute cost in GPU-based or cloud environments, particularly through the lens of carbon emissions and energy consumption during model training. It provides details on how to calculate the carbon footprint of model training using factors like hardware power consumption, training time, and energy grid carbon intensity.","1","is_blank","[""luccioni2023"", ""strubell2019"", ""erben2023"", ""cottier2024"", ""kim2025""]","[""https://www.ieee-pub.org/publications_abstracts/80/4/040"", ""https://arxiv.org/abs/2101.07498"", ""https://arxiv.org/abs/2101.07498"", ""https://arxiv.org/abs/2101.07498"", ""https://arxiv.org/abs/2101.07498""]","TRUE","The context discusses the importance of estimating compute cost in GPU-based or cloud environments, particularly through the lens of carbon emissions and energy consumption during model training. It provides details on how to calculate the carbon footprint of model training using factors like hardware power consumption, training time, and energy grid carbon intensity."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model achieved up to a 13.2% performance improvement with automated resource utilization overlapping, as shown in Figure 14.","13.2","percent","[""chen2024""]","is_blank","is_blank","The context states that the LLaMA-65B model achieved up to a 13.2% performance improvement with automated resource utilization overlapping, as shown in Figure 14."
"q164","How much does an elephant weigh?","The context discusses the environmental impact of AI models, including their power usage, carbon emissions, and water consumption, but does not provide specific information about the weight of an elephant.","is_blank","lbs","[""morrison2025, amazon2023, amazon2023, morrison2025, wu2021a""]","is_blank","is_blank","The context discusses the environmental impact of AI models, including their power usage, carbon emissions, and water consumption, but does not provide specific information about the weight of an elephant."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides detailed energy consumption figures for various NLP models, including T5, Meena, GShard-600B, and Switch Transformer, with specific numbers given for each. However, it does not directly compare the energy consumption of all models to determine which has the highest, nor does it explicitly state which model is the most energy-intensive.","is_blank","is_blank","[""patterson2021,jegham2025,zschache2025""]","is_blank","is_blank","The context provides detailed energy consumption figures for various NLP models, including T5, Meena, GShard-600B, and Switch Transformer, with specific numbers given for each. However, it does not directly compare the energy consumption of all models to determine which has the highest, nor does it explicitly state which model is the most energy-intensive."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context provides information on the carbon emissions from training BERT, stating that the BERT base model was trained on 16 TPU chips for 4 days, resulting in 96 hours of training time.","4","days","[""luccioni2025b"", ""strubell2019""]","[""https://link_to_luccioni2025b_document"", ""https://link_to_strubell2019_document""]","4. BERTbase V100x64 was trained on 16 TPU chips for 4 days (96 hours).","The context provides information on the carbon emissions from training BERT, stating that the BERT base model was trained on 16 TPU chips for 4 days, resulting in 96 hours of training time."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","","","is_blank","[]","is_blank","is_blank",""
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context mentions the use of four additional datasets from the HuggingFace platform to assess the generalizability of the findings, indicating that the original dataset used was not the only one tested.","4","is_blank","[""fernandez2025"", ""zschache2025""]","[""HuggingFace platform""]","These datasets were selected from the HuggingFace platform","The context mentions the use of four additional datasets from the HuggingFace platform to assess the generalizability of the findings, indicating that the original dataset used was not the only one tested."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context discusses the comparison of cost-efficiency between eight T4 spot instances and a DGX-2 node, indicating that using multiple T4 instances can be more cost-effective than a DGX-2 node while paying additional egress costs.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context discusses the comparison of cost-efficiency between eight T4 spot instances and a DGX-2 node, indicating that using multiple T4 instances can be more cost-effective than a DGX-2 node while paying additional egress costs."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context mentions that the 2023 US Executive Order regarding AI did not address AI's greenhouse gas emissions or energy usage, highlighting a disconnect in AI regulation concerning sustainability.","is_blank","is_blank","[""luccioni2025b"", ""ebert2024"", ""dodge2022"", ""luccioni2025a""]","is_blank","is_blank","The context mentions that the 2023 US Executive Order regarding AI did not address AI's greenhouse gas emissions or energy usage, highlighting a disconnect in AI regulation concerning sustainability."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German 2023 Energy Efficiency Act establishes specific efficiency and renewable energy targets for data centers, requiring them to reach a PUE factor between 1.5 and 1.2, an ERF of 10% to 20%, and to run on 50% renewable energy, increasing to 100% by January 1, 2027. This is supported by the context detailing the Act's requirements for data centers.","100%","is_blank","[""ebert2024"", ""ebert2024""]","[""https://example.com/document1"", ""https://example.com/document2""]","is_blank","The German 2023 Energy Efficiency Act establishes specific efficiency and renewable energy targets for data centers, requiring them to reach a PUE factor between 1.5 and 1.2, an ERF of 10% to 20%, and to run on 50% renewable energy, increasing to 100% by January 1, 2027. This is supported by the context detailing the Act's requirements for data centers."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context indicates that a large majority of papers from ACL conferences target accuracy, with only a small portion arguing for efficiency improvements. Specifically, 90% of ACL papers target accuracy, and only 10% argue for efficiency results.","10","papers","[""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019"", ""schwartz2019""]","[""https://acl2018.org"", ""https://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com"", ""https://acl2018.org"", ""https://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com"", ""https://acl2018.org"", ""https://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com"", ""https://acl2018.org"", ""https://nips.cc/Conferences/2018""]","90% of ACL papers target accuracy, 10% argue for a new efﬁciency result.","The context indicates that a large majority of papers from ACL conferences target accuracy, with only a small portion arguing for efficiency improvements. Specifically, 90% of ACL papers target accuracy, and only 10% argue for efficiency results."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","Recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use, as indicated by the context mentioning that inference can account for up to 90% of a model's total lifecycle energy use.","1","percent","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598v6""]","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].","Recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use, as indicated by the context mentioning that inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context discusses the AI Act's requirements for providers, particularly focusing on transparency obligations for general-purpose AI models and the need for including energy consumption during the inference phase in reporting. It also highlights the Act's failure to mandate the disclosure of energy consumption during inference and its focus on transparency for high-risk AI systems.","1","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","is_blank","The context discusses the AI Act's requirements for providers, particularly focusing on transparency obligations for general-purpose AI models and the need for including energy consumption during the inference phase in reporting. It also highlights the Act's failure to mandate the disclosure of energy consumption during inference and its focus on transparency for high-risk AI systems."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context discusses the AI Act's shortcomings in mandating the disclosure of energy consumption during the inference phase of AI models, highlighting this as a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37]. For example, a recent investigation has revealed Microsoft’s aggressive pitch of its AI models to ExxonMobile to optimize fossil fuel exploration [35].","The context discusses the AI Act's shortcomings in mandating the disclosure of energy consumption during the inference phase of AI models, highlighting this as a crucial omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context indicates that new AI data centers often rely on liquid cooling due to high server power densities, as it is stated that 'new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.'","Liquid cooling","is_blank","[""li2025b""]","[""Not provided in the context""]","is_blank","The context indicates that new AI data centers often rely on liquid cooling due to high server power densities, as it is stated that 'new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.'"
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","","","multiplier","[]","is_blank","is_blank",""
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","","","lbs","[]","is_blank","is_blank",""
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context mentions that ML inference accounts for 80-90% of the total compute demand, as reported by various sources including a paper and AWS.","80-90","percent","[""chung2025"", ""patterson2021"", ""luccioni2024"", ""fernandez2025"", ""luccioni2023""]","[""https://arxiv.org/pdf/2505.06371v2.pdf"", ""https://arxiv.org/pdf/2504.17674v1.pdf"", ""https://arxiv.org/pdf/2505.06371v2.pdf"", ""https://arxiv.org/pdf/2504.17674v1.pdf"", ""https://arxiv.org/pdf/2505.06371v2.pdf"", ""https://arxiv.org/pdf/2504.17674v1.pdf"", ""https://arxiv.org/pdf/2505.06371v2.pdf""]","ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60]. According to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28]","The context mentions that ML inference accounts for 80-90% of the total compute demand, as reported by various sources including a paper and AWS."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides information on the energy consumption of training a large language model with over 6.1 billion parameters, estimating the total energy consumption to be approximately 103,500 kWh.","103,500","household-years","[""dodge2022""]","[""https://arxiv.org/abs/2206.08418""]","is_blank","The context provides information on the energy consumption of training a large language model with over 6.1 billion parameters, estimating the total energy consumption to be approximately 103,500 kWh."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","","","is_blank","[]","is_blank","is_blank",""
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides the total number of tokens used for pre-training and the GPU hours but does not specify the number of GPU hours per day or the number of days required for pre-training.","is_blank","days","[""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat"", ""https://huggingface.co/ajibawa-2023/Code-290k-ShareGPT"", ""https://arxiv.org/abs/2108.07732"", ""https://github.com/bigcode-project/bigcode-evaluation-harness"", ""https://github.com/myshell-ai/JetMoE""]","JetMoE-8B is trained with 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context provides the total number of tokens used for pre-training and the GPU hours but does not specify the number of GPU hours per day or the number of days required for pre-training."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context defines water consumption as 'water withdrawal minus water discharge', which means the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","1","is_blank","[""li2025b""]","is_blank","is_blank","The context defines water consumption as 'water withdrawal minus water discharge', which means the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context provides detailed information on the energy per second for inference with LLaMA 65B across different GPU shard configurations, indicating an average increase in energy with the number of shards and a range from 300 Watts to 1 Kilowatt.","300-10000","W","[""samsi2024""]","is_blank","is_blank","The context provides detailed information on the energy per second for inference with LLaMA 65B across different GPU shard configurations, indicating an average increase in energy with the number of shards and a range from 300 Watts to 1 Kilowatt."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context indicates that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with a minor accuracy reduction of 0.07 points.","7","multiplier","[""zschache2025"", ""chung2025"", ""jegham2025"", ""luccioni2024""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics"", ""https://acm.forth.gr/facc/2024/"", ""https://arxiv.org/abs/2406.01156"", ""https://doi.org/10.1145/3581557.3581611""]","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context indicates that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with a minor accuracy reduction of 0.07 points."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","","","percent","[]","is_blank","is_blank",""
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","","","models","[]","is_blank","is_blank",""
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","","","USD","[]","is_blank","is_blank",""
