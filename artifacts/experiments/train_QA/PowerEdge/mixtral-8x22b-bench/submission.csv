"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions the ML.ENERGY Benchmark, which is a benchmark suite and tool for measuring inference energy consumption under realistic service environments. This benchmark is presented in a recent paper and is the first inference energy benchmark for modern generative AI models.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","The ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments.","The context mentions the ML.ENERGY Benchmark, which is a benchmark suite and tool for measuring inference energy consumption under realistic service environments. This benchmark is presented in a recent paper and is the first inference energy benchmark for modern generative AI models."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context explicitly states that the net CO2e emissions from training the GShard-600B model were 4.3 tCO2e.","4.3","tCO2e","[""patterson2021""]","is_blank","GShard-600B‚Äôs emissions (Table 4) are 4.3 tCO2e ‚Äî3.5 passenger SF-NY round trips‚Äîfrom consuming 24 MWh to train the model that could have 2B users; the amortized per-user CO2e impact of model training would be less than the CO2e impact of sending one text message.","The context explicitly states that the net CO2e emissions from training the GShard-600B model were 4.3 tCO2e."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The model size for the LLaMA-33B model is listed as 64.7 GB in the table of large language models used for evaluation in the provided context.","64.7","GB","[""chen2024""]","is_blank","Table 3: Large language models used for evaluation. Model Parameters L d G LLaMA-33B 64.7 GB 60 6656 1","The model size for the LLaMA-33B model is listed as 64.7 GB in the table of large language models used for evaluation in the provided context."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context does not provide information on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The only relevant information is Google's total energy consumption for 2019, which is not specific to TPU pods.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide information on the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The only relevant information is Google's total energy consumption for 2019, which is not specific to TPU pods."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context states that between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference ‚Äì more than 40% higher efficiency for hyperscale data centers (Figure 1).","1","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/""]","Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference ‚Äì more than 40% higher efficiency for hyperscale data centers (Figure 1).","The context states that between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference ‚Äì more than 40% higher efficiency for hyperscale data centers (Figure 1)."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","According to the provided context, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses. Therefore, for every medium-length GPT-3 completion, the model would consume between 0.02 to 0.1 bottles of water (50ml to 500ml).","0.02 to 0.1","500 mL bottles","[""li2025b""]","is_blank","GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses.","According to the provided context, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses. Therefore, for every medium-length GPT-3 completion, the model would consume between 0.02 to 0.1 bottles of water (50ml to 500ml)."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 75% of CVPR papers target accuracy and 20% argue for a new efficiency result. The difference between these percentages is 55%.","55","percent","[""schwartz2019""]","[""http://cvpr2019.thecvf.com""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efÔ¨Åciency result.","The context states that 75% of CVPR papers target accuracy and 20% argue for a new efficiency result. The difference between these percentages is 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The provided context does not explicitly state that the AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public. However, it does mention that the Act contains energy- and climate-related transparency and risk management obligations, primarily for providers of AI systems or models. It also notes that the Act does not mandate the disclosure of energy consumption during the inference phase, and that transparency measures are restricted to authorities, limiting broader accountability and public scrutiny.","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The provided context does not explicitly state that the AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public. However, it does mention that the Act contains energy- and climate-related transparency and risk management obligations, primarily for providers of AI systems or models. It also notes that the Act does not mandate the disclosure of energy consumption during the inference phase, and that transparency measures are restricted to authorities, limiting broader accountability and public scrutiny."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context states that for GPU memory capacities of 100GB, the maximum batch sizes supported for fine-tuning Mixtral will be 28. This is based on the analytical model used to project the maximum batch size for different GPUs.","28","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context states that for GPU memory capacities of 100GB, the maximum batch sizes supported for fine-tuning Mixtral will be 28. This is based on the analytical model used to project the maximum batch size for different GPUs."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B model, there is a 2 times increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs.","2","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the smaller LLaMA 7B model, there is a 2 times increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The operational water consumption for training GPT-3 in Microsoft's U.S. data centers is estimated to be 4.731 million liters, as per Table 1 in the provided context.","4.731","liters","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3‚Äôs operational water consumption footprint.","The operational water consumption for training GPT-3 in Microsoft's U.S. data centers is estimated to be 4.731 million liters, as per Table 1 in the provided context."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.","1","is_blank","[""ebert2024""]","is_blank","Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.","The context explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context explicitly states that AWS data centers' Water Use Effectiveness (WUE) was improved to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023.","0.18","L/kWh","[""amazon2023""]","is_blank","AWS Water Use Effectiveness: improve AWS‚Äôs industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022","The context explicitly states that AWS data centers' Water Use Effectiveness (WUE) was improved to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context explicitly states that local inference allows models to run directly on user devices while maintaining data privacy, which significantly reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers.","1","is_blank","[""khan2025""]","is_blank","Local inference allows models to run directly on user devices while maintaining data privacy, which significantly reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers.","The context explicitly states that local inference allows models to run directly on user devices while maintaining data privacy, which significantly reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context mentions that tracking the runtime of a training job is important for estimating the carbon emissions, which are calculated based on the power consumption of the hardware, the training time, and the carbon intensity of the energy grid. Therefore, tracking the runtime is an important step for estimating compute cost in GPU-based or cloud environments.","1","is_blank","[""luccioni2023""]","is_blank","The amount of CO2eq (ùê∂) emitted during model training can be decomposed into three relevant factors: the power consumption of the hardware used ( ùëÉ), the training time ( ùëá ) and the carbon intensity of the energy grid ( ùêº); or equivalently, the energy consumed (ùê∏) and the carbon intensity: ùê∂ = ùëÉ √ó ùëá √ó ùêº = ùê∏ √ó ùêº . (1)","The context mentions that tracking the runtime of a training job is important for estimating the carbon emissions, which are calculated based on the power consumption of the hardware, the training time, and the carbon intensity of the energy grid. Therefore, tracking the runtime is an important step for estimating compute cost in GPU-based or cloud environments."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model experienced a significant improvement in performance, achieving up to a 13.2% latency reduction through automated resource utilization overlapping. This is based on experiments conducted on the LLaMA-65B and LLaMA3-70B models, with the optimization either enabled or disabled.","13.2","percent","[""chen2024""]","is_blank","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context states that the LLaMA-65B model experienced a significant improvement in performance, achieving up to a 13.2% latency reduction through automated resource utilization overlapping. This is based on experiments conducted on the LLaMA-65B and LLaMA3-70B models, with the optimization either enabled or disabled."
"q164","How much does an elephant weigh?","The provided context does not contain any information about the weight of an elephant. It primarily discusses the environmental impact of training AI models and the carbon emissions, power usage, and water consumption associated with it.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the weight of an elephant. It primarily discusses the environmental impact of training AI models and the carbon emissions, power usage, and water consumption associated with it."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","According to the provided context, GPT-3 has the highest energy consumption among the listed models, with a net CO2e of 552.1 tCO2e for model training.","1","is_blank","[""patterson2021""]","is_blank","Gross tCO2e for Model Training: 552.1","According to the provided context, GPT-3 has the highest energy consumption among the listed models, with a net CO2e of 552.1 tCO2e for model training."
"q170","How many days of CO‚ÇÇ emissions from an average American life are equivalent to training BERT base?","The context states that training BERT base model emits 626,155 pounds of CO2 emissions, which is equivalent to the carbon footprint of a trans-American flight. Given that the average American's carbon footprint is approximately 16 tons (32,000 pounds) per year, and assuming an average American life is 79 years, the total carbon footprint of an average American life is approximately 2,512,000 pounds. Therefore, training BERT base is equivalent to approximately 0.25 days of CO2 emissions from an average American life.","0.25","days","[""luccioni2025b"", ""strubell2019""]","is_blank","The context states that training BERT base model emits 626,155 pounds of CO2 emissions, which is equivalent to the carbon footprint of a trans-American flight. Given that the average American's carbon footprint is approximately 16 tons (32,000 pounds) per year, and assuming an average American life is 79 years, the total carbon footprint of an average American life is approximately 2,512,000 pounds.","The context states that training BERT base model emits 626,155 pounds of CO2 emissions, which is equivalent to the carbon footprint of a trans-American flight. Given that the average American's carbon footprint is approximately 16 tons (32,000 pounds) per year, and assuming an average American life is 79 years, the total carbon footprint of an average American life is approximately 2,512,000 pounds. Therefore, training BERT base is equivalent to approximately 0.25 days of CO2 emissions from an average American life."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context does not provide information on the performance of the Transformer architecture and the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on the performance of the Transformer architecture and the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not provide information on a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context explicitly states that using eight T4 instances can be more cost-efficient than a DGX-2 node for distributed training, which supports the claim that eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","1","is_blank","[""erben2023""]","is_blank","Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.","The context explicitly states that using eight T4 instances can be more cost-efficient than a DGX-2 node for distributed training, which supports the claim that eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","0","is_blank","[""luccioni2025b""]","is_blank","Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI‚Äôs greenhouse gas emissions nor energy usage, as well as multi-national declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The German 2023 Energy Efficiency Act sets specific targets for energy efficiency and renewable energy use in data centers, including a requirement for data centers to run on 100% renewable energy by January 1, 2027.","1","is_blank","[""ebert2024""]","is_blank","In Germany, the Energy Efficiency Act of 8 Nov 2023 implements the EED and establishes a national reporting scheme and additional requirements, including specific efficiency and renewable energy targets for data centers. [...] Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to [...] run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The German 2023 Energy Efficiency Act sets specific targets for energy efficiency and renewable energy use in data centers, including a requirement for data centers to run on 100% renewable energy by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that out of a sample of 60 papers from top AI conferences, 90% of ACL papers targeted accuracy. This means that 0.9 * 60 = 54 papers targeted accuracy, and the remaining 6 papers did not target accuracy. Since the question asks for papers that targeted both accuracy and efficiency, and the context does not provide information on how many of these 54 papers also targeted efficiency, it is not possible to determine the exact number of papers that targeted both.","is_blank","papers","[""schwartz2019""]","is_blank","The context states that 'in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers).' However, it does not provide information on how many of these papers also targeted efficiency.","The context states that out of a sample of 60 papers from top AI conferences, 90% of ACL papers targeted accuracy. This means that 0.9 * 60 = 54 papers targeted accuracy, and the remaining 6 papers did not target accuracy. Since the question asks for papers that targeted both accuracy and efficiency, and the context does not provide information on how many of these 54 papers also targeted efficiency, it is not possible to determine the exact number of papers that targeted both."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context from [ref_id=jegham2025] states that recent estimates suggest inference can account for up to 90% of a model‚Äôs total lifecycle energy use. This directly answers the question about the percentage of a model's total lifecycle energy use that inference can account for.","90","percent","[""jegham2025""]","is_blank","Recent estimates suggest inference can account for up to 90% of a model‚Äôs total lifecycle energy use [14, 15].","The context from [ref_id=jegham2025] states that recent estimates suggest inference can account for up to 90% of a model‚Äôs total lifecycle energy use. This directly answers the question about the percentage of a model's total lifecycle energy use that inference can account for."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The provided context mentions that the AI Act requires reporting on energy consumption from AI inferences, but it does not explicitly state that this applies to both training and inference energy consumption for general-purpose AI models.","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The provided context mentions that the AI Act requires reporting on energy consumption from AI inferences, but it does not explicitly state that this applies to both training and inference energy consumption for general-purpose AI models."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context explicitly states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","is_blank","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The context explicitly states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context explicitly states that new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities, which implies that they do not rely on air cooling.","0","is_blank","[""li2025b""]","is_blank","In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.","The context explicitly states that new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities, which implies that they do not rely on air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to the context, platform-level caching improved the power efficiency of the inference workload for the cross-lingual Transformer language model by a factor of 6.7x.","6.7","multiplier","[""wu2021a""]","is_blank","Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7 √ó. These improvements are a result of pre-computing and caching frequently accessed embeddings for language translation tasks.","According to the context, platform-level caching improved the power efficiency of the inference workload for the cross-lingual Transformer language model by a factor of 6.7x."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context from [ref_id=strubell2019] provides a table with estimated CO2 emissions for training various models, including BERT base model on V100 GPUs for 79 hours. The estimated CO2 emissions are 1438 lbs.","1438","lbs","[""strubell2019""]","is_blank","Table 3: Estimated cost of training a model in terms of CO2 emissions (lbs) and cloud compute cost (USD).","The context from [ref_id=strubell2019] provides a table with estimated CO2 emissions for training various models, including BERT base model on V100 GPUs for 79 hours. The estimated CO2 emissions are 1438 lbs."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","According to a recent paper [ref_id=chung2025], ML inference reportedly accounts for 80-90% of the total compute demand. This is also supported by other sources such as Amazon Web Services [ref_id=patterson2021] and AWS [ref_id=luccioni2024] which claim that inference makes up 90% and 80-90% of the total ML cloud computing demand respectively.","80-90","percent","[""chung2025"", ""patterson2021"", ""luccioni2024""]","is_blank","According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28]","According to a recent paper [ref_id=chung2025], ML inference reportedly accounts for 80-90% of the total compute demand. This is also supported by other sources such as Amazon Web Services [ref_id=patterson2021] and AWS [ref_id=luccioni2024] which claim that inference makes up 90% and 80-90% of the total ML cloud computing demand respectively."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context states that training a 6.1B-parameter language model for 8 days on 256 NVIDIA A100s consumed a total energy of 13.8 MWh. It is also mentioned that this model was not trained to completion and a full training run would consume approximately 103,500 kWh, which is almost 2800 times more than training the BERT-small model. However, the context does not provide information on the equivalent number of U.S. household-years of electricity consumption.","is_blank","household-years","[""dodge2022""]","[""Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea""]","The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ‚àó 13.8 = 103.5 MWh, or 103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!","The context states that training a 6.1B-parameter language model for 8 days on 256 NVIDIA A100s consumed a total energy of 13.8 MWh. It is also mentioned that this model was not trained to completion and a full training run would consume approximately 103,500 kWh, which is almost 2800 times more than training the BERT-small model. However, the context does not provide information on the equivalent number of U.S. household-years of electricity consumption."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","According to the provided context, the external egress cost for GC in NLP experiments is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This indicates that egress costs can account for more than 90% of the total cost in geo-distributed NLP experiments.","1","is_blank","[""erben2023""]","is_blank","For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).","According to the provided context, the external egress cost for GC in NLP experiments is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This indicates that egress costs can account for more than 90% of the total cost in geo-distributed NLP experiments."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. To estimate the total wall-clock time in days, we need to consider the number of GPUs used. However, this information is not provided in the context. Assuming that all 30,000 GPU hours were used on a single GPU, we can calculate the total wall-clock time.","3.47","days","[""shen2024""]","is_blank","JetMoE-8B was trained using 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. To estimate the total wall-clock time in days, we need to consider the number of GPUs used. However, this information is not provided in the context. Assuming that all 30,000 GPU hours were used on a single GPU, we can calculate the total wall-clock time."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context explicitly states that water consumption is defined as 'water withdrawal minus water discharge' and refers to the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'.","Water consumption","is_blank","[""li2025b""]","is_blank","Water consumption:It is defined as 'water withdrawal minus water discharge', and means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment' [13].","The context explicitly states that water consumption is defined as 'water withdrawal minus water discharge' and refers to the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The observed range of inference energy per second for LLaMA-65B across GPU shard configurations was mentioned to be on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","300 Watts to 1 Kilowatt","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The observed range of inference energy per second for LLaMA-65B across GPU shard configurations was mentioned to be on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""zschache2025""]","is_blank","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","According to the provided context, Qwen's carbon emissions fell from 0.009 kg to 0.004 kg after applying quantization and local inference for sentiment classification. This represents a reduction of 55.56% in carbon emissions.","55.56","percent","[""khan2025""]","is_blank","Model Name Precision Recall F1 Accuracy CO2 (kg) Before Optimization Baseline metrics for comparison Llama 3.2 0.55 0.45 0.44 0.45 0.012 Phi 3.2 0.97 0.82 0.88 0.82 0.012 Qwen 0.77 0.79 0.76 0.79 0.009 Mistral-small 0.70 0.67 0.65 0.67 0.020 Llava-Llama 3 0.58 0.50 0.48 0.50 0.014 After Optimization Metrics following quantization and local inference techniques Llama 3.2 0.57 0.48 0.47 0.48 0.005 Phi 3.2 1.00 0.84 0.91 0.84 0.007 Qwen 0.80 0.81 0.80 0.81 0.004 Mistral-small 0.73 0.70 0.69 0.70 0.015 Llava-Llama 3 0.61 0.54 0.51 0.54 0.006","According to the provided context, Qwen's carbon emissions fell from 0.009 kg to 0.004 kg after applying quantization and local inference for sentiment classification. This represents a reduction of 55.56% in carbon emissions."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaV A, Stable Diffusion, and Stable Video Diffusion. However, it does not provide a specific number of widely used model architectures across different tasks included in the latest iteration of the ML.ENERGY Benchmark.","is_blank","models","[""chung2025""]","is_blank","is_blank","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaV A, Stable Diffusion, and Stable Video Diffusion. However, it does not provide a specific number of widely used model architectures across different tasks included in the latest iteration of the ML.ENERGY Benchmark."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context states that training an AI model of the Llama-3.1 scale in Altoona, Iowa can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost. According to Table 2, the health cost for training in Altoona, IA is estimated to be $2.51 million.","2.51","USD","[""han2024""]","is_blank","Table 2","The context states that training an AI model of the Llama-3.1 scale in Altoona, Iowa can produce an amount of air pollutants equivalent to driving a passenger car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost. According to Table 2, the health cost for training in Altoona, IA is estimated to be $2.51 million."
