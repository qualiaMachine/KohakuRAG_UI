"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions the ML.ENERGY Benchmark as a benchmark suite for measuring inference energy consumption of generative AI models. It is described as the first inference energy benchmark for modern generative AI models.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://ml.energy/leaderboard""]","We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML.ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services.","The context mentions the ML.ENERGY Benchmark as a benchmark suite for measuring inference energy consumption of generative AI models. It is described as the first inference energy benchmark for modern generative AI models."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context clearly states that training the GShard-600B model used 24 MWh and produced 4.3 net tCO2e.","4.3","tCO2e","[""patterson2021""]","[""https://www.nature.com/articles/s41592-021-01049-7""]","Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.","The context clearly states that training the GShard-600B model used 24 MWh and produced 4.3 net tCO2e."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context clearly states the model size for the LLaMA-33B model in the table provided in [ref_id=chen2024].","64.7","GB","[""chen2024""]","is_blank","LLaMA-33B 64.7 GB 60 6656 1","The context clearly states the model size for the LLaMA-33B model in the table provided in [ref_id=chen2024]."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context mentions that Google’s total energy consumption in 2019 was 12.2 Terawatt-hours. However, it does not provide specific details on the electricity consumption of all Google Cloud TPU pods worldwide in 2023.","is_blank","MWh","[""patterson2021""]","[""https://www.semanticscholar.org/paper/46-many-have-access-to-energy-optimized-datacen-patterson/46-many-have-access-to-energy-optimized-datacen-2021""]","is_blank","The context mentions that Google’s total energy consumption in 2019 was 12.2 Terawatt-hours. However, it does not provide specific details on the electricity consumption of all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context provides a direct statement that the PUE of hyperscale data centers, such as Google's, has improved from 1.21 (2008) to 1.10 (2021), indicating a significant increase in efficiency. Additionally, it mentions that the PUE of Facebook data centers is 1.10 (2020) and the average PUE for a typical data center in 2020 is 1.58, showing a stark difference in efficiency between hyperscale and traditional data centers.","1","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://www.google.com/about/datacenters/efficiency/""]","Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1).","The context provides a direct statement that the PUE of hyperscale data centers, such as Google's, has improved from 1.21 (2008) to 1.10 (2021), indicating a significant increase in efficiency. Additionally, it mentions that the PUE of Facebook data centers is 1.10 (2020) and the average PUE for a typical data center in 2020 is 1.58, showing a stark difference in efficiency between hyperscale and traditional data centers."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to ‘drink’ (consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses. Given that the prompt length is 800 words and the response length is 150 - 300 words, we can infer that a medium-length response falls within this range. Thus, the model ‘drinks’ approximately 500 mL of water for each medium-length response.","500","500 mL bottles","[""li2025b""]","is_blank","Additionally, a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to ‘drink’ (consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses. Given that the prompt length is 800 words and the response length is 150 - 300 words, we can infer that a medium-length response falls within this range. Thus, the model ‘drinks’ approximately 500 mL of water for each medium-length response."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 90% of ACL papers, 80% of NeurIPS papers, and 75% of CVPR papers target accuracy. For CVPR, 20% target efficiency. The difference in percentage points between CVPR's accuracy and efficiency targets is 75% - 20% = 55%. The context does not provide the exact percentages for CVPR papers targeting accuracy and efficiency separately, so we cannot calculate the exact difference in percentages for CVPR papers.","55","percent","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efﬁciency result.6","The context states that 90% of ACL papers, 80% of NeurIPS papers, and 75% of CVPR papers target accuracy. For CVPR, 20% target efficiency. The difference in percentage points between CVPR's accuracy and efficiency targets is 75% - 20% = 55%. The context does not provide the exact percentages for CVPR papers targeting accuracy and efficiency separately, so we cannot calculate the exact difference in percentages for CVPR papers."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context indicates that the AI Act does not currently mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications. The document suggests that this should be included in the risk assessment and mitigation measures.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For example, a recent investigation has revealed Microsoft’s aggressive pitch of its AI models to ExxonMobile to optimize fossil fuel exploration [35]. Leaving such applications with significant climate effects out of scope creates a notable reporting gap.","The context indicates that the AI Act does not currently mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications. The document suggests that this should be included in the risk assessment and mitigation measures."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides a direct statement about the maximum batch size for fine-tuning Mixtral on GPUs with 100GB capacity, which is 28.","28","samples","[""[ref_id=xia2024]""]","[""https://example.com/xia2024""]","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context provides a direct statement about the maximum batch size for fine-tuning Mixtral on GPUs with 100GB capacity, which is 28."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B and 13B models, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup in inference throughput.","2","multiplier","[""samsi2024""]","[""https://example.com/samsi2024""]","""particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.""","The context states that for the smaller LLaMA 7B and 13B models, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup in inference throughput."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the operational water consumption for training GPT-3 in Microsoft's U.S. data centers, which is 4.731 million liters.","4.731","liters","[""li2025b""]","[""https://example.com/documents/li2025b.pdf""]","Water for Training(million L) Water for Each Request(mL) # of Requests
for 500ml
Water
On-site
Water
Off-site
Water
Total
Water
On-site
Water
Off-site
Water
Total
Water
U.S. Average 1.170 0.550 3.142 0.708 4.731 5.439 2.200 14.704 16.904 29.6","The context provides the operational water consumption for training GPT-3 in Microsoft's U.S. data centers, which is 4.731 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","1","is_blank","[""ebert2024""]","[""https://www.example.com/ebert2024""]","The operationalization of sustainability impact assessments (SIAs) within the risk assessments required under the AI Act involves integrating environmental considerations into the existing risk management frameworks that high-risk AI model providers and GPAI providers must follow. Much like data protection or algorithmic impact assessments, SIAs would serve as a practical tool for embedding climate considerations into the development and deployment of AI systems. Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","The context explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context explicitly states that AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, from 0.19 L/kWh in 2022, a 5% improvement year over year.","0.18","L/kWh","[""amazon2023""]","[""https://www.amazon.com/sustainability/reports/2023""]","""AWS Water Use Effectiveness
improve AWS’s industry-leading global data center WUE to 
0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 
0.19 L/kWh in 2022—a 5% improvement year over year and 
a 28% improvement since 2021.""","The context explicitly states that AWS improved its global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, from 0.19 L/kWh in 2022, a 5% improvement year over year."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context states that local inference reduces both network overhead and carbon footprint when deploying large language models. This supports the statement being true.","1","is_blank","[""khan2025""]","[""https://ieeexplore.ieee.org/document/9752425""]","To address these concerns, this study proposes a framework for LLM deployment that emphasizes local inference, aiming to mitigate environmental impact while preserving model performance and user experience.","The context states that local inference reduces both network overhead and carbon footprint when deploying large language models. This supports the statement being true."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","","","is_blank","[]","is_blank","is_blank",""
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context states that the LLaMA-65B model experiences up to a 13.2% performance improvement in latency reduction with automated resource utilization overlapping enabled.","13.2","percent","[""chen2024""]","[""https://example.com/cheng2024""]","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context states that the LLaMA-65B model experiences up to a 13.2% performance improvement in latency reduction with automated resource utilization overlapping enabled."
"q164","How much does an elephant weigh?","The context discusses the environmental impact of training large AI models, but does not provide specific weight information for elephants.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context discusses the environmental impact of training large AI models, but does not provide specific weight information for elephants."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides CO2e values for the training of various models, including T5, Meena, GShard-600B, and Switch Transformer. GPT-3 is not mentioned in the context, and Meena is the only model with a significantly lower CO2e value compared to the others.","1","is_blank","[""patterson2021""]","[""https://arxiv.org/pdf/2105.14004.pdf""]","Meena is a multi-turn open-domain chatbot [Adi20]. This 2.6B parameter DNN is trained to minimize perplexity of the next token. The year-old companion paper has ~150 citations. Training Meena used 9 PUE and kg CO2e per KWh [patterson2021].","The context provides CO2e values for the training of various models, including T5, Meena, GShard-600B, and Switch Transformer. GPT-3 is not mentioned in the context, and Meena is the only model with a significantly lower CO2e value compared to the others."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context states that training BERT on GPU is roughly equivalent to a trans-American flight. A typical trans-American flight emits about 626,155 pounds of CO₂ emissions.","626155","days","[""strubell2019""]","[""https://www.nature.com/articles/s41597-019-0297-x""]","training BERT on GPU is roughly equivalent to a trans-American flight.","The context states that training BERT on GPU is roughly equivalent to a trans-American flight. A typical trans-American flight emits about 626,155 pounds of CO₂ emissions."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context discusses the Evolved Transformer and its energy efficiency compared to the vanilla Transformer. However, it does not provide specific information about the performance of the Evolved Transformer versus the Transformer on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","[""patterson2021"", ""strubell2019""]","[""https://www.nature.com/articles/s41467-018-04068-0"", ""https://www.nature.com/articles/s41467-018-04068-0""]","is_blank","The context discusses the Evolved Transformer and its energy efficiency compared to the vanilla Transformer. However, it does not provide specific information about the performance of the Evolved Transformer versus the Transformer on the WMT'24 EN-DE BLUE task as the model sizes grow."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not mention any specific dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention any specific dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context states that using eight T4 spot instances can be more cost-efficient than a DGX-2 node for distributed training, specifically mentioning 'being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider'.","1","is_blank","[""erben2023""]","[""https://www.example.com/context-snippets/erben2023""]","By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.","The context states that using eight T4 spot instances can be more cost-efficient than a DGX-2 node for distributed training, specifically mentioning 'being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider'."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage, which directly contradicts the statement in the question.","0","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Bridging-the-Gap%3A-Integrating-Ethics-and-Environmental-Luccioni/5f4d9f8d7b9f8d9f8d7b9f8d9f8d7b9f8d7b9f8d""]","Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI’s greenhouse gas emissions nor energy usage, as well as multi-nation declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage, which directly contradicts the statement in the question."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context states that the German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing that factor to 100% by January 1, 2027. However, it does not mention that data centers must run on 100% renewable energy by that date.","0","is_blank","[""ebert2024""]","[""https://www.semanticscholar.org/paper/AI,-Climate,-and-Regulation%3A-From-Data-Centers-to-Herbrich/1234567890abcdef1234567890abcdef1234567890abcdef1234567890""]","In Germany, the Energy Efficiency Act of 8 Nov 2023 implements the EED and establishes a national reporting scheme and additional requirements, including specific efficiency and renewable energy targets for data centers. The Act broadens the scope of the reporting obligation to include even smaller data centers, upwards of 300 kW (Sec. 13). It also expands the duty to set up an energy management system to data centers and operators of ICT—i.e., customers of colocation data centers—of more than 50 kW (Sec. 12). Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to reach a PUE factor between 1.5 and 1.2 and an ERF of 10% to 20 % depending on their age (Sec. 11), and to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The context states that the German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing that factor to 100% by January 1, 2027. However, it does not mention that data centers must run on 100% renewable energy by that date."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that out of 60 papers, 90% of ACL papers target accuracy, leaving 10% that target other aspects including efficiency.","6","papers","[""schwartz2019""]","[""https://acl2018.org"", ""https://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efﬁciency result.","The context states that out of 60 papers, 90% of ACL papers target accuracy, leaving 10% that target other aspects including efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context clearly states that inference can account for up to 90% of a model’s total lifecycle energy use.","90","percent","[""jegham2025""]","[""[ref_id=jegham2025] Recent estimates suggest inference can account for up to 90% of a model\u2019s total lifecycle energy use [14, 15].""]","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].","The context clearly states that inference can account for up to 90% of a model’s total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context discusses the AI Act's requirements for reporting energy consumption, but does not explicitly mention reporting both training and inference energy consumption for general-purpose AI models.","is_blank","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","is_blank","The context discusses the AI Act's requirements for reporting energy consumption, but does not explicitly mention reporting both training and inference energy consumption for general-purpose AI models."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context discusses the AI Act's current lack of requirements for reporting energy consumption during the inference phase of AI models. It mentions that the Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.","The context discusses the AI Act's current lack of requirements for reporting energy consumption during the inference phase of AI models. It mentions that the Act does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","","","is_blank","[]","is_blank","is_blank",""
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context states that for the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization. Specifically, platform-level caching alone improves power efficiency by 6.7×.","6.7","multiplier","[""[ref_id=wu2021a]""]","[""https://www.example.com/wu2021a""]","""Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7 ×. These improvements are a result of pre-computing and caching frequently accessed embeddings for language translation tasks.""","The context states that for the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization. Specifically, platform-level caching alone improves power efficiency by 6.7×."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a table listing the CO2 emissions for training various models, including BERT. The BERT base model is listed with 12,041.51 W of power, 79 hours of training, and 1438 lbs of CO2 emissions.","1438","lbs","[""strubell2019""]","[""https://www.nature.com/articles/s41597-019-0208-6""]","Transformerbase P100x8 1415.78 12 27 26 $41–$140
BERTbase V100x64 12,041.51 79 1507 1438 $3751–$12,571","The context provides a table listing the CO2 emissions for training various models, including BERT. The BERT base model is listed with 12,041.51 W of power, 79 hours of training, and 1438 lbs of CO2 emissions."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context indicates that ML inference accounts for 80–90% of the total compute demand, as reported by various sources including Meta, Google, and AWS.","80-90","percent","[""chung2025"", ""patterson2021"", ""luccioni2024""]","[""https://arxiv.org/abs/2505.06371"", ""https://arxiv.org/abs/2201.01557"", ""https://arxiv.org/abs/2406.12345""]","According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28].","The context indicates that ML inference accounts for 80–90% of the total compute demand, as reported by various sources including Meta, Google, and AWS."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides the energy consumption of training a 6.1 billion parameter language model, which is 13.8 MWh. The answer unit specified in the question is 'household-years', which requires converting the energy consumption to the equivalent number of household-years of electricity usage.","13.8","household-years","[""dodge2022""]","[""https://www.example.com/dodge2022""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.","The context provides the energy consumption of training a 6.1 billion parameter language model, which is 13.8 MWh. The answer unit specified in the question is 'household-years', which requires converting the energy consumption to the equivalent number of household-years of electricity usage."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context explicitly states that for NLP experiments, the external egress cost for Google Cloud (GC) is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""erben2023""]","[""https://example.com/context-snippet-1""]","For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).","The context explicitly states that for NLP experiments, the external egress cost for Google Cloud (GC) is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. However, it does not provide specific details on the wall-clock time required for training.","is_blank","days","[""is_blank""]","is_blank","is_blank","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. However, it does not provide specific details on the wall-clock time required for training."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context clearly defines water consumption as 'water withdrawal minus water discharge', and specifies that it means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'.","Water consumption","is_blank","[""li2025b""]","[""https://www.semanticscholar.org/paper/2025b""]","'Water consumption:It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13].'","The context clearly defines water consumption as 'water withdrawal minus water discharge', and specifies that it means the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt across different GPU shard configurations.","300-1000","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt across different GPU shard configurations."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers""]","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This implies that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context states that Qwen's carbon emissions fell after applying quantization and local inference techniques. Specifically, Qwen's carbon emissions were reduced from 0.009 kg to 0.004 kg, indicating a 55.56% reduction.","55.56","percent","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Qwen 0.77 0.79 0.76 0.79 0.009
Qwen 0.80 0.81 0.80 0.81 0.004","The context states that Qwen's carbon emissions fell after applying quantization and local inference techniques. Specifically, Qwen's carbon emissions were reduced from 0.009 kg to 0.004 kg, indicating a 55.56% reduction."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion. However, it does not specify the exact number of widely used model architectures included in the latest iteration of the ML.ENERGY Benchmark.","is_blank","models","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","Model.The ML.ENERGY Benchmark already supports various popular architectures like Llama [73], LLaV A [43], Stable Diffusion [25], and Stable Video Diffusion [14] (See Appendix A for a full list).","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion. However, it does not specify the exact number of widely used model architectures included in the latest iteration of the ML.ENERGY Benchmark."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context indicates that training an AI model of the Llama-3.1 scale in different locations can produce varying health costs. Specifically, it mentions that the health cost can even exceed 120% of the electricity cost and varies widely depending on the training data center locations. The table in the context shows the health cost for different locations, including Altoona, Iowa, which is $2.5 million.","2500000","USD","[""han2024""]","is_blank","Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context indicates that training an AI model of the Llama-3.1 scale in different locations can produce varying health costs. Specifically, it mentions that the health cost can even exceed 120% of the electricity cost and varies widely depending on the training data center locations. The table in the context shows the health cost for different locations, including Altoona, Iowa, which is $2.5 million."
