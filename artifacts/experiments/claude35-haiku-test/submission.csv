"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","According to the context from Wu et al. (2021), the total energy consumption of U.S. data centers increased by about 4% from 2010-2014, which is a significant slowdown compared to previous periods.","4","percent","[""wu2021b""]","is_blank","is_blank","According to the context from Wu et al. (2021), the total energy consumption of U.S. data centers increased by about 4% from 2010-2014, which is a significant slowdown compared to previous periods."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","According to the Amazon Solar Farm Maryland–CPV Backbone project description, the solar farm will avoid more than 64,000 metric tons of CO2e annually, which is equivalent to taking more than 13,900 cars off the road in 2023.","13900","cars","[""amazon2023""]","is_blank","is_blank","According to the Amazon Solar Farm Maryland–CPV Backbone project description, the solar farm will avoid more than 64,000 metric tons of CO2e annually, which is equivalent to taking more than 13,900 cars off the road in 2023."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","According to the context, AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, specifically including two data centers in Virginia, one in California, and one in Singapore.","4","data centers","[""amazon2023""]","is_blank","is_blank","According to the context, AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, specifically including two data centers in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to the context from Morrison et al. (2025), they estimated the embodied carbon emissions of a GPU at 463 kg CO2eq per GPU, based on the lack of disclosure from NVIDIA. They amortize these emissions over a 4-year GPU lifespan, which results in 0.013 kg of CO2eq per GPU hour.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","According to the context from Morrison et al. (2025), they estimated the embodied carbon emissions of a GPU at 463 kg CO2eq per GPU, based on the lack of disclosure from NVIDIA. They amortize these emissions over a 4-year GPU lifespan, which results in 0.013 kg of CO2eq per GPU hour."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","According to the context from Cottier et al., the amortized hardware and energy cost for training GPT-4 was $40M, while the hardware acquisition cost was $800M - representing an approximately 20x difference between amortized and acquisition costs.","20","ratio","[""cottier2024""]","is_blank","is_blank","According to the context from Cottier et al., the amortized hardware and energy cost for training GPT-4 was $40M, while the hardware acquisition cost was $800M - representing an approximately 20x difference between amortized and acquisition costs."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides a specific CO2e value for an air travel round trip from SF to NY in the Strubell et al. research paper, which lists 1984 lbs of CO2 emissions for a single passenger round trip.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","The context provides a specific CO2e value for an air travel round trip from SF to NY in the Strubell et al. research paper, which lists 1984 lbs of CO2 emissions for a single passenger round trip."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","According to the Open LLM Leaderboard section, FLM-101B achieved an average performance score of 43.94 when evaluated across four tasks: ARC, HellaSwag, MMLU, and TruthfulQA. This score represents over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","43.94","score","[""li2025a""]","is_blank","is_blank","According to the Open LLM Leaderboard section, FLM-101B achieved an average performance score of 43.94 when evaluated across four tasks: ARC, HellaSwag, MMLU, and TruthfulQA. This score represents over 90% of the performance of GLM-130B, which has 7 times more FLOPs."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","According to the context, there was a more than 6,750 fold improvement in processor clock speed between the Intel 4004 microprocessor in 1971 and typical microprocessors in 2021. Specifically, the Intel 4004 ran at 740 kHz in 1971, while typical microprocessors in 2021 can run at 5,000,000 kHz.","6750","fold","[""wu2021b""]","is_blank","is_blank","According to the context, there was a more than 6,750 fold improvement in processor clock speed between the Intel 4004 microprocessor in 1971 and typical microprocessors in 2021. Specifically, the Intel 4004 ran at 740 kHz in 1971, while typical microprocessors in 2021 can run at 5,000,000 kHz."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context from Luccioni et al. (2023) directly states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to 14.8 days when using 10,000 GPUs.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context from Luccioni et al. (2023) directly states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to 14.8 days when using 10,000 GPUs."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","According to Table 3 in the context from the Morrison et al. 2025 paper, the GPU Power Usage for the Llama 3.2 1B model at an 8 request/s frequency is 0.036 kWh for 2400 prompts from ShareGPT.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","According to Table 3 in the context from the Morrison et al. 2025 paper, the GPU Power Usage for the Llama 3.2 1B model at an 8 request/s frequency is 0.036 kWh for 2400 prompts from ShareGPT."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context directly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx, specifically for data centers in Loudoun, Prince William, and Fairfax counties between January 1, 2023 and December 1, 2024.","13000","tons","[""han2024""]","is_blank","is_blank","The context directly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx, specifically for data centers in Loudoun, Prince William, and Fairfax counties between January 1, 2023 and December 1, 2024."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper reveals that the total training time for the 101B model using their growth strategy was 21.54 days, which represents a 72% time savings compared to training a 101B model from scratch (which would take an estimated 76.74 days).","72","percent","[""li2025a""]","is_blank","is_blank","The paper reveals that the total training time for the 101B model using their growth strategy was 21.54 days, which represents a 72% time savings compared to training a 101B model from scratch (which would take an estimated 76.74 days)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","According to the paper, the analysis using the EPA's COBRA modeling tool predicts that by 2028, U.S. data centers could contribute to approximately 1,300 premature deaths due to scope-2 air pollution from electricity generation.","1300","deaths","[""han2024""]","is_blank","is_blank","According to the paper, the analysis using the EPA's COBRA modeling tool predicts that by 2028, U.S. data centers could contribute to approximately 1,300 premature deaths due to scope-2 air pollution from electricity generation."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","According to the Dodge et al. (2022) paper, their training run of the 6.1 billion parameter transformer was only trained for 13% of its total projected time. They estimate that a full training run would take approximately 60 days.","60","days","[""dodge2022""]","is_blank","is_blank","According to the Dodge et al. (2022) paper, their training run of the 6.1 billion parameter transformer was only trained for 13% of its total projected time. They estimate that a full training run would take approximately 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context does not provide a clear comparison of GPU energy consumption across all models for 1,000 queries in an appendix. The documents discuss energy consumption and inference measurement techniques, but do not definitively state which specific model has the highest GPU energy consumption.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide a clear comparison of GPU energy consumption across all models for 1,000 queries in an appendix. The documents discuss energy consumption and inference measurement techniques, but do not definitively state which specific model has the highest GPU energy consumption."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, which directly answers the question about the launch year.","2014","year","[""stone2022""]","is_blank","is_blank","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, which directly answers the question about the launch year."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","According to the UN's Global E-Waste Monitor 2024, about 22% of global e-waste has been formally collected and recycled, with electronic waste generation rising five times faster than recycling rates.","22","percent","[""luccioni2025a""]","is_blank","is_blank","According to the UN's Global E-Waste Monitor 2024, about 22% of global e-waste has been formally collected and recycled, with electronic waste generation rising five times faster than recycling rates."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides specific energy consumption data for the BLOOM-176B model's pre-training from multiple sources. According to the table in the Luccioni et al. paper, the BLOOM-7B model's pre-training energy consumption was 51,686 kWh, which gives an indication of the energy use for pre-training the BLOOM model.","51686","MWh","[""luccioni2024""]","is_blank","is_blank","The context provides specific energy consumption data for the BLOOM-176B model's pre-training from multiple sources. According to the table in the Luccioni et al. paper, the BLOOM-7B model's pre-training energy consumption was 51,686 kWh, which gives an indication of the energy use for pre-training the BLOOM model."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The provided context does not specify what percentage of the Switch Transformer's 1500 billion parameters are activated per token. While the context mentions the Switch Transformer and its parameter count, it does not provide details about parameter activation rate.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not specify what percentage of the Switch Transformer's 1500 billion parameters are activated per token. While the context mentions the Switch Transformer and its parameter count, it does not provide details about parameter activation rate."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","According to the hyperparameters table and accompanying text, the JetMoE-8B architecture includes 8 experts for each Mixture-of-Experts (MoE) layer. The authors specifically state that they 'set the same number of experts to 8 and top-k to 2 for every layer'.","8","experts","[""shen2024""]","is_blank","is_blank","According to the hyperparameters table and accompanying text, the JetMoE-8B architecture includes 8 experts for each Mixture-of-Experts (MoE) layer. The authors specifically state that they 'set the same number of experts to 8 and top-k to 2 for every layer'."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","Based on the context, the execution time for a dense BlackMamba model with a batch size of 30 fine-tuned on an NVIDIA A40-48 GB GPU is approximately 2 seconds. This can be seen in Figure 4, which shows the execution time breakdown across different batch sizes and model configurations.","2","second","[""xia2024""]","is_blank","is_blank","Based on the context, the execution time for a dense BlackMamba model with a batch size of 30 fine-tuned on an NVIDIA A40-48 GB GPU is approximately 2 seconds. This can be seen in Figure 4, which shows the execution time breakdown across different batch sizes and model configurations."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to the paper, the English portion of the FLM-101B model required 28.22 zettaFLOPs to train. This is explicitly stated in the text when detailing the total computational cost of the model, which is broken down into 28.22 zettaFLOPs for English and 24.54 for Chinese.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","According to the paper, the English portion of the FLM-101B model required 28.22 zettaFLOPs to train. This is explicitly stated in the text when detailing the total computational cost of the model, which is broken down into 28.22 zettaFLOPs for English and 24.54 for Chinese."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for the financial sentiment classification task used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz with 16.0 GB of installed memory, running on a Windows 11 Pro 64-bit system. The hardware details are specifically described in the hardware and software setting section of the experimental setup.","Intel Core i7-1165G7","is_blank","[""khan2025""]","is_blank","is_blank","The experimental setup for the financial sentiment classification task used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz with 16.0 GB of installed memory, running on a Windows 11 Pro 64-bit system. The hardware details are specifically described in the hardware and software setting section of the experimental setup."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context from the Luccioni et al. paper describes a systematic comparison of machine learning models, but does not specify an exact number of models sampled. The study compares task-specific and general-purpose models across different tasks, but does not enumerate a precise count of models.","is_blank","models","[""luccioni2024""]","is_blank","is_blank","The context from the Luccioni et al. paper describes a systematic comparison of machine learning models, but does not specify an exact number of models sampled. The study compares task-specific and general-purpose models across different tasks, but does not enumerate a precise count of models."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","According to the document, increasing GPU utilization up to 80% leads to a 3-fold reduction in the overall carbon footprint for Language Model (LM) training. This is specifically stated in the context that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×'.","3","multiplier","[""wu2021a""]","is_blank","is_blank","According to the document, increasing GPU utilization up to 80% leads to a 3-fold reduction in the overall carbon footprint for Language Model (LM) training. This is specifically stated in the context that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×'."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","According to the context, Cottier et al. (2025) found that the total compute for model development is between 1.2x to 4x larger than the final training run alone, with a median estimate of 2.2x. This was based on sampling evidence from GPT-3, OPT-175B, BLOOM, and Meta's AI infrastructure.","2.2","multiplier","[""cottier2024""]","is_blank","is_blank","According to the context, Cottier et al. (2025) found that the total compute for model development is between 1.2x to 4x larger than the final training run alone, with a median estimate of 2.2x. This was based on sampling evidence from GPT-3, OPT-175B, BLOOM, and Meta's AI infrastructure."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The document provides a detailed estimate of energy consumption for a 6.1 billion parameter transformer model trained on 256 NVIDIA A100 GPUs. The researchers found that their partial training run (13% completion) consumed 13.8 MWh, and they estimated a full training run would consume approximately 103.5 MWh or 103,500 kWh.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The document provides a detailed estimate of energy consumption for a 6.1 billion parameter transformer model trained on 256 NVIDIA A100 GPUs. The researchers found that their partial training run (13% completion) consumed 13.8 MWh, and they estimated a full training run would consume approximately 103.5 MWh or 103,500 kWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context describes Jevons' Paradox as an economic principle where efficiency gains paradoxically lead to increased consumption. In the paper, the authors argue that technical efficiency improvements in AI may not automatically result in net environmental benefits due to potential rebound effects that spur higher overall resource usage.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The context describes Jevons' Paradox as an economic principle where efficiency gains paradoxically lead to increased consumption. In the paper, the authors argue that technical efficiency improvements in AI may not automatically result in net environmental benefits due to potential rebound effects that spur higher overall resource usage."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context clearly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, based on conservative estimates using electricity consumption projections and water usage intensity factors from data center operators.","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context clearly states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, based on conservative estimates using electricity consumption projections and water usage intensity factors from data center operators."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. The paper highlights that computational costs are increasing exponentially, with diminishing returns, but Red AI continues to grow rather than decline.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. The paper highlights that computational costs are increasing exponentially, with diminishing returns, but Red AI continues to grow rather than decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","According to the context, the total wall-clock time required to train the FLM-101B model using a growth strategy was 21.54 days. This represents a 72% time savings compared to training a 101B model from scratch, which would have taken an estimated 76.74 days.","21.54","days","[""li2025a""]","is_blank","is_blank","According to the context, the total wall-clock time required to train the FLM-101B model using a growth strategy was 21.54 days. This represents a 72% time savings compared to training a 101B model from scratch, which would have taken an estimated 76.74 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","According to the context from Wu et al., the majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, not over 80%. Multiple sections of the document explicitly state that GPU utilization is low, with significant room for efficiency improvements.","0","is_blank","[""wu2021a""]","is_blank","is_blank","According to the context from Wu et al., the majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, not over 80%. Multiple sections of the document explicitly state that GPU utilization is low, with significant room for efficiency improvements."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","Multiple sources consistently report that the training of GPT-3 consumed 1,287 megawatt-hours (MWh) of electricity. This is specifically mentioned in two references: Li et al.'s research paper and Jegham et al.'s paper, both of which cite the same energy consumption figure.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","Multiple sources consistently report that the training of GPT-3 consumed 1,287 megawatt-hours (MWh) of electricity. This is specifically mentioned in two references: Li et al.'s research paper and Jegham et al.'s paper, both of which cite the same energy consumption figure."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The AI Energy Score project is a collaborative initiative aimed at creating a standardized method for comparing the inference efficiency of AI models across different tasks and contexts.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The AI Energy Score project is a collaborative initiative aimed at creating a standardized method for comparing the inference efficiency of AI models across different tasks and contexts."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","Based on Figure 6 in the context, the longest kernel of the MoE layer for BlackMamba appears to be the matrix multiplication for W1, which can be seen as the highest vertical bar in the kernel breakdown graph for the BlackMamba model.","is_blank","microseconds","[""xia2024""]","is_blank","is_blank","Based on Figure 6 in the context, the longest kernel of the MoE layer for BlackMamba appears to be the matrix multiplication for W1, which can be seen as the highest vertical bar in the kernel breakdown graph for the BlackMamba model."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","From the hyperparameters table and accompanying text in section 4.2, the JetMoE-8B model is configured with 8 total experts per layer, with a top-k value of 2, meaning 2 experts are selected for activation in each layer of the model for a given token.","2","experts","[""shen2024""]","is_blank","is_blank","From the hyperparameters table and accompanying text in section 4.2, the JetMoE-8B model is configured with 8 total experts per layer, with a top-k value of 2, meaning 2 experts are selected for activation in each layer of the model for a given token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context from Schwartz et al. (2019) directly confirms the claim, stating that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context from Schwartz et al. (2019) directly confirms the claim, stating that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","According to the context from the wu2021b document, global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction.","6.4","percent","[""wu2021b""]","is_blank","is_blank","According to the context from the wu2021b document, global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","According to the context, Amazon's AWS achieved 100% renewable energy matching in 22 data center regions in 2023, which is an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon's AWS achieved 100% renewable energy matching in 22 data center regions in 2023, which is an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context indicates that the field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project workshop. By 2025, this would make AI approximately 69 years old.","69","years","[""stone2022""]","is_blank","is_blank","The context indicates that the field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project workshop. By 2025, this would make AI approximately 69 years old."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al. that quantified the energy required using neural architecture search (NAS), a process for identifying the best model architecture with respect to an optimization objective.","Neural Architecture Search","is_blank","[""luccioni2025c""]","is_blank","is_blank","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al. that quantified the energy required using neural architecture search (NAS), a process for identifying the best model architecture with respect to an optimization objective."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context from the Chung et al. 2025 paper, targeting an average Time Per Output Token (TPOT) of 100 ms leads to a 44% reduction in energy consumption compared to minimizing latency for the Llama 3.1 8B model.","44","percent","[""chung2025""]","is_blank","is_blank","According to the context from the Chung et al. 2025 paper, targeting an average Time Per Output Token (TPOT) of 100 ms leads to a 44% reduction in energy consumption compared to minimizing latency for the Llama 3.1 8B model."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the document, the maximum batch size for BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU is 20 samples.","20","samples","[""xia2024""]","is_blank","is_blank","According to Table III in the document, the maximum batch size for BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU is 20 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to the context, Amazon held 1.3 gigawatts (GW) of energy storage capacity in 2023, which was an increase from 445 megawatts (MW) in 2022.","1.3","GW","[""amazon2023""]","is_blank","is_blank","According to the context, Amazon held 1.3 gigawatts (GW) of energy storage capacity in 2023, which was an increase from 445 megawatts (MW) in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","According to the study, the projected annual carbon emissions from GPT-4o inference in 2025 are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","According to the study, the projected annual carbon emissions from GPT-4o inference in 2025 are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context documents do not provide a specific percentage of AI inference workloads powered by coal in Asia for 2023. While the documents discuss energy consumption and sources for data centers, they do not break down energy sources specifically for Asian AI workloads in that year.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context documents do not provide a specific percentage of AI inference workloads powered by coal in Asia for 2023. While the documents discuss energy consumption and sources for data centers, they do not break down energy sources specifically for Asian AI workloads in that year."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context from the Ebert et al. 2024 paper explicitly states that the average data center Power Usage Effectiveness (PUE) in 2023 was 1.58 globally. This is directly referenced in the technical background section discussing data center efficiency metrics.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context from the Ebert et al. 2024 paper explicitly states that the average data center Power Usage Effectiveness (PUE) in 2023 was 1.58 globally. This is directly referenced in the technical background section discussing data center efficiency metrics."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","According to multiple citations from the JetMoE-8B paper, the model has 8B total parameters but only activates 2B parameters for each input token during inference, which reduces computational costs by about 70% compared to Llama2-7B.","2","parameters","[""shen2024""]","is_blank","is_blank","According to multiple citations from the JetMoE-8B paper, the model has 8B total parameters but only activates 2B parameters for each input token during inference, which reduces computational costs by about 70% compared to Llama2-7B."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","According to the context from Luccioni et al.'s 2025 research, the Llama 7B model has an estimated 14 tCO2e (tons of carbon dioxide equivalent) from its pre-training process, which matches the specific environmental impact table in the document.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the context from Luccioni et al.'s 2025 research, the Llama 7B model has an estimated 14 tCO2e (tons of carbon dioxide equivalent) from its pre-training process, which matches the specific environmental impact table in the document."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides specific details about Amazon's electric delivery van numbers in 2022 and 2023. In the United States, Amazon increased its electric delivery van fleet from 2,600 in 2022 to 11,800 in 2023, representing a total increase of 9,200 electric delivery vans.","9200","electric delivery vans","[""amazon2023""]","is_blank","is_blank","The context provides specific details about Amazon's electric delivery van numbers in 2022 and 2023. In the United States, Amazon increased its electric delivery van fleet from 2,600 in 2022 to 11,800 in 2023, representing a total increase of 9,200 electric delivery vans."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.' This directly contradicts the statement that such emissions do not occur.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context explicitly states that operational environmental impacts of LLMs include 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.' This directly contradicts the statement that such emissions do not occur."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","According to Table 4 in the paper, the o3 model consumes 12.222 ± 1.082 Wh for a long prompt with 10k input and 1.5k output.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","According to Table 4 in the paper, the o3 model consumes 12.222 ± 1.082 Wh for a long prompt with 10k input and 1.5k output."
"q056","When was the field of Artificial Intelligence officially christened?","The context clearly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. McCarthy is credited with first using the term 'artificial intelligence' in the workshop proposal.","1956","year","[""stone2022""]","is_blank","is_blank","The context clearly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. McCarthy is credited with first using the term 'artificial intelligence' in the workshop proposal."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide a specific water use effectiveness (WUE) value for Google's AI-dedicated data centers in 2024. While the document discusses water consumption in data centers extensively, it does not give a precise WUE figure for Google specifically.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide a specific water use effectiveness (WUE) value for Google's AI-dedicated data centers in 2024. While the document discusses water consumption in data centers extensively, it does not give a precise WUE figure for Google specifically."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context directly supports the statement about 770 million people lacking access to electricity. Specifically, the document states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity' according to the International Energy Agency.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context directly supports the statement about 770 million people lacking access to electricity. Specifically, the document states that 'approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity' according to the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, for a maximum generation length of 512 tokens, the LLaMA 65B model consumes approximately 3-4 Joules per output token. This is evident from the text describing the energy per decoded token for different batch sizes and shard configurations.","3.5","joules per token","[""samsi2024""]","is_blank","is_blank","According to the context, for a maximum generation length of 512 tokens, the LLaMA 65B model consumes approximately 3-4 Joules per output token. This is evident from the text describing the energy per decoded token for different batch sizes and shard configurations."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","According to the context from the Wu et al. paper, by converting 32-bit floating-point numerical representation to 16-bit, Facebook could reduce the overall RM2 (recommendation model 2) model size by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","According to the context from the Wu et al. paper, by converting 32-bit floating-point numerical representation to 16-bit, Facebook could reduce the overall RM2 (recommendation model 2) model size by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that the widely cited claim of AI reducing global GHG emissions by 5-10% lacks clear scientific grounding and rigorous calculations. The estimate originates from a Boston Consulting Group (BCG) report, but the underlying methodology is unclear and lacks substantive evidence.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context indicates that the widely cited claim of AI reducing global GHG emissions by 5-10% lacks clear scientific grounding and rigorous calculations. The estimate originates from a Boston Consulting Group (BCG) report, but the underlying methodology is unclear and lacks substantive evidence."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from Wu et al. suggests that sparsely-activated neural networks can improve model accuracy at a lower operational energy footprint. However, the exact claim of consuming less than 1/10th the energy is not definitively stated. The Shen et al. reference indicates that sparse activation can significantly reduce training and inference costs, reducing computation by about 70% for a specific model.","is_blank","is_blank","[""wu2021a"", ""shen2024""]","is_blank","is_blank","The context from Wu et al. suggests that sparsely-activated neural networks can improve model accuracy at a lower operational energy footprint. However, the exact claim of consuming less than 1/10th the energy is not definitively stated. The Shen et al. reference indicates that sparse activation can significantly reduce training and inference costs, reducing computation by about 70% for a specific model."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context from Schwartz et al. (2019) directly states that Grover was trained on 256 TPU chips for two weeks, with an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The context from Schwartz et al. (2019) directly states that Grover was trained on 256 TPU chips for two weeks, with an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context directly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1 on an NVIDIA A40-48GB GPU.","53","percent","[""xia2024""]","is_blank","is_blank","The context directly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1 on an NVIDIA A40-48GB GPU."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","Based on the context, Google Translate performs 1 billion queries per day, and the given reference model Flan-T5-xxl uses 0.083 kWh per 1,000 queries. Multiplying these values yields the daily energy consumption estimate.","83","MWh","[""luccioni2024""]","is_blank","is_blank","Based on the context, Google Translate performs 1 billion queries per day, and the given reference model Flan-T5-xxl uses 0.083 kWh per 1,000 queries. Multiplying these values yields the daily energy consumption estimate."
"q067","What was the average global data center PUE in 2023?","The context from the Ebert et al. 2024 paper specifically states the average global data center Power Usage Effectiveness (PUE) in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context from the Ebert et al. 2024 paper specifically states the average global data center Power Usage Effectiveness (PUE) in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","There is no information in the provided context that specifies the exact number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. The context contains details about renewable energy projects for Amazon and some general information about Microsoft's sustainability efforts, but not the specific detail requested.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","There is no information in the provided context that specifies the exact number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. The context contains details about renewable energy projects for Amazon and some general information about Microsoft's sustainability efforts, but not the specific detail requested."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the analysis by Cottier et al., Gemini Ultra has the highest fraction of R&D staff costs at 49% of the total amortized model development costs when equity is included.","49","percent","[""cottier2024""]","is_blank","is_blank","According to the analysis by Cottier et al., Gemini Ultra has the highest fraction of R&D staff costs at 49% of the total amortized model development costs when equity is included."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context clearly states that the inaugural 2015 Study Panel consisted of seventeen members from various disciplines, including AI experts from academia, corporate laboratories, and industry, as well as scholars in law, political science, policy, and economics.","17","people","[""stone2022""]","is_blank","is_blank","The context clearly states that the inaugural 2015 Study Panel consisted of seventeen members from various disciplines, including AI experts from academia, corporate laboratories, and industry, as well as scholars in law, political science, policy, and economics."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","According to the context, manufacturing accounts for 74% of a client device's total carbon footprint. Specifically, the document by Wu et al. notes that 'Reducing embodied carbon cost for edge devices is important, as manufacturing carbon cost accounts for 74% of the total footprint of client devices.'","74","percent","[""wu2021a""]","is_blank","is_blank","According to the context, manufacturing accounts for 74% of a client device's total carbon footprint. Specifically, the document by Wu et al. notes that 'Reducing embodied carbon cost for edge devices is important, as manufacturing carbon cost accounts for 74% of the total footprint of client devices.'"
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context specifically indicates that models with more parameters do not always consume more energy during inference. The paper by Chung et al. provides an example of Phi-3 Mini and Small models, where the larger Small model can actually consume less energy than the Mini model as batch size grows, due to differences in attention mechanisms and memory usage.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context specifically indicates that models with more parameters do not always consume more energy during inference. The paper by Chung et al. provides an example of Phi-3 Mini and Small models, where the larger Small model can actually consume less energy than the Mini model as batch size grows, due to differences in attention mechanisms and memory usage."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","According to the context, the Study Panel explicitly states that they 'found no cause for concern that AI is an imminent threat to humankind'. They further clarify that no machines with self-sustaining long-term goals exist, nor are they likely to be developed in the near future.","0","is_blank","[""stone2022""]","is_blank","is_blank","According to the context, the Study Panel explicitly states that they 'found no cause for concern that AI is an imminent threat to humankind'. They further clarify that no machines with self-sustaining long-term goals exist, nor are they likely to be developed in the near future."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","There is no specific information in the provided context about OpenAI's API request CO2 emissions for January 2024. While the documents discuss carbon emissions from AI models in general, none provide a precise measurement for OpenAI's API emissions in that specific month.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","There is no specific information in the provided context about OpenAI's API request CO2 emissions for January 2024. While the documents discuss carbon emissions from AI models in general, none provide a precise measurement for OpenAI's API emissions in that specific month."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context from Luccioni et al. (2025), Meta reports that their Llama 3 family of models emitted 11,390 tons of CO2e during pre-training, which is over 40 times the original 'five cars' estimate from the Strubell et al. study.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the context from Luccioni et al. (2025), Meta reports that their Llama 3 family of models emitted 11,390 tons of CO2e during pre-training, which is over 40 times the original 'five cars' estimate from the Strubell et al. study."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","According to Figure 2(d) in the context, the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over a 1.5-year period between 2019 and 2021.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","According to Figure 2(d) in the context, the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over a 1.5-year period between 2019 and 2021."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","Multiple context passages confirm that AlphaGo, a computer program developed by Google DeepMind, defeated the human Go champion in a five-game match. This was considered a significant milestone for artificial intelligence, achieved through advanced machine learning techniques like reinforcement learning.","1","is_blank","[""stone2022""]","is_blank","is_blank","Multiple context passages confirm that AlphaGo, a computer program developed by Google DeepMind, defeated the human Go champion in a five-game match. This was considered a significant milestone for artificial intelligence, achieved through advanced machine learning techniques like reinforcement learning."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","Continuous batching is a batching strategy that dynamically replaces completed requests with new ones during LLM inference, which helps improve GPU utilization and reduce idle time. This approach mitigates the inefficiencies of static batching, where a fixed batch size leads to GPU under-utilization when generation lengths vary.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","Continuous batching is a batching strategy that dynamically replaces completed requests with new ones during LLM inference, which helps improve GPU utilization and reduce idle time. This approach mitigates the inefficiencies of static batching, where a fixed batch size leads to GPU under-utilization when generation lengths vary."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","According to the context, the entire JetMoE-8B alignment process (dSFT and dDPO fine-tuning) required 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","According to the context, the entire JetMoE-8B alignment process (dSFT and dDPO fine-tuning) required 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected the g6e.xlarge instance, which was 26.7% more expensive than the top instance selected by InferSave. The g6e.xlarge instance cost $2.699, compared to InferSave's top choice g4dn.xlarge at $2.13.","26.7","percent","[""kim2025""]","is_blank","is_blank","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected the g6e.xlarge instance, which was 26.7% more expensive than the top instance selected by InferSave. The g6e.xlarge instance cost $2.699, compared to InferSave's top choice g4dn.xlarge at $2.13."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context from the Luccioni et al. 2024 paper directly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context from the Luccioni et al. 2024 paper directly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","According to the context from a 2025 study by Luccioni et al., the GPU energy usage for 1,000 inference queries ranges from 0.06 Wh for bert-tiny to over 3,426 Wh for Command-R Plus, with variation depending on model size, architecture, and task complexity.","3,426","Wh","[""luccioni2025c""]","is_blank","is_blank","According to the context from a 2025 study by Luccioni et al., the GPU energy usage for 1,000 inference queries ranges from 0.06 Wh for bert-tiny to over 3,426 Wh for Command-R Plus, with variation depending on model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that there is no universal, one-size-fits-all solution for AI ethics and sustainability. The paper emphasizes the need for nuanced, context-specific approaches that take into account varied ethical dimensions and different cultural perspectives.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states that there is no universal, one-size-fits-all solution for AI ethics and sustainability. The paper emphasizes the need for nuanced, context-specific approaches that take into account varied ethical dimensions and different cultural perspectives."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context does not provide a specific gross carbon intensity value for the U.S. average energy mix in 2021. While the documents discuss carbon intensity and provide some related details, there is no direct statement about the precise value for the U.S. grid in 2021.","is_blank","kg of CO2e/KWh","[""is_blank""]","is_blank","is_blank","The context does not provide a specific gross carbon intensity value for the U.S. average energy mix in 2021. While the documents discuss carbon intensity and provide some related details, there is no direct statement about the precise value for the U.S. grid in 2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","Hivemind is a decentralized, PyTorch-based framework designed to enable distributed deep learning training across heterogeneous and geographically distributed hardware. It was specifically developed to allow participants to donate computing resources for collaborative model training, with unique features like handling peer dropouts and using a distributed hash table (DHT) for coordination.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","Hivemind is a decentralized, PyTorch-based framework designed to enable distributed deep learning training across heterogeneous and geographically distributed hardware. It was specifically developed to allow participants to donate computing resources for collaborative model training, with unique features like handling peer dropouts and using a distributed hash table (DHT) for coordination."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","According to the context, Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and describing an AI system's societal impacts, ethical considerations, and environmental footprint.","Social Transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and describing an AI system's societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","According to the context, the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This was specifically noted for the German public administration text classification experiments on the FKTG dataset.","Linear model with sentence embeddings","is_blank","[""zschache2025""]","is_blank","is_blank","According to the context, the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This was specifically noted for the German public administration text classification experiments on the FKTG dataset."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context reveals that the researchers developed an LLM inference system called Lamina, which implements model-attention disaggregation across heterogeneous GPUs. This is explicitly stated in multiple sections of the paper, including the abstract and conclusion.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The context reveals that the researchers developed an LLM inference system called Lamina, which implements model-attention disaggregation across heterogeneous GPUs. This is explicitly stated in multiple sections of the paper, including the abstract and conclusion."
"q093","How many parameters does the largest T5 model have?","Based on the context snippets, the largest T5 model referenced has 1.5 billion parameters. This detail is found in the context from the Erben 2023 paper discussing the Whisper model, which notes it features sizes 'from 37.8M to 1.5B parameters'.","1500000000","parameters","[""erben2023""]","is_blank","is_blank","Based on the context snippets, the largest T5 model referenced has 1.5 billion parameters. This detail is found in the context from the Erben 2023 paper discussing the Whisper model, which notes it features sizes 'from 37.8M to 1.5B parameters'."
"q094","What is the total number of parameters in the JetMoE-8B model?","According to Table 1 and the text, JetMoE-8B has a total of 8B parameters (Ptotal), while only activating 2B parameters (Pactive) for each input token, reducing inference computation by about 70% compared to Llama2-7B.","8","parameters","[""shen2024""]","is_blank","is_blank","According to Table 1 and the text, JetMoE-8B has a total of 8B parameters (Ptotal), while only activating 2B parameters (Pactive) for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","According to multiple context snippets, Google's data center water consumption increased by approximately 20% from 2021 to 2022. This information is specifically reported in both the Li et al. and Luccioni et al. papers.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","According to multiple context snippets, Google's data center water consumption increased by approximately 20% from 2021 to 2022. This information is specifically reported in both the Li et al. and Luccioni et al. papers."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","According to the table in the context from Khan et al. (2025), 'Carbon Intensity' is defined as 'CO2 emissions per unit of electricity consumed', with a unit of gCO2/kWh, and is referenced from the International Energy Agency.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","According to the table in the context from Khan et al. (2025), 'Carbon Intensity' is defined as 'CO2 emissions per unit of electricity consumed', with a unit of gCO2/kWh, and is referenced from the International Energy Agency."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","From Table 2 in the context, the final growth stage of FLM-101B (101B stage) shows a FLOPs utilization of 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","From Table 2 in the context, the final growth stage of FLM-101B (101B stage) shows a FLOPs utilization of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","According to the Cottier et al. 2024 paper, the estimated amortized hardware and energy cost for training GPT-4 was $40 million. This represents the spread-out cost of hardware and energy used in the final training run.","40","USD","[""cottier2024""]","is_blank","is_blank","According to the Cottier et al. 2024 paper, the estimated amortized hardware and energy cost for training GPT-4 was $40 million. This represents the spread-out cost of hardware and energy used in the final training run."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context directly states that full-stack optimization for a Transformer-based universal translation model can reduce operational carbon footprint by 810 times through techniques like platform-level caching, GPU acceleration, and algorithmic changes.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context directly states that full-stack optimization for a Transformer-based universal translation model can reduce operational carbon footprint by 810 times through techniques like platform-level caching, GPU acceleration, and algorithmic changes."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","When training was spread across four continents (the C-8 experiment), the NLP throughput was reduced by 41% compared to the local A-8 experiment. This indicates that the NLP task achieved less than 60% of its local throughput when distributed across four continents.","0.59","multiplier","[""erben2023""]","is_blank","is_blank","When training was spread across four continents (the C-8 experiment), the NLP throughput was reduced by 41% compared to the local A-8 experiment. This indicates that the NLP task achieved less than 60% of its local throughput when distributed across four continents."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context directly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities across 15 water replenishment activities in 10 countries.","3.5","liters","[""amazon2023""]","is_blank","is_blank","The context directly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities across 15 water replenishment activities in 10 countries."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The research paper directly investigates how custom tags in prompts can reduce energy consumption of Large Language Models (LLMs) across zero-shot, one-shot, and few-shot techniques for code completion tasks. The authors found that using specific tags can lower energy consumption during model inference without compromising performance.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The research paper directly investigates how custom tags in prompts can reduce energy consumption of Large Language Models (LLMs) across zero-shot, one-shot, and few-shot techniques for code completion tasks. The authors found that using specific tags can lower energy consumption during model inference without compromising performance."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","According to the Luccioni et al. 2025 paper, NVIDIA shipped 3.7 million data center GPUs in 2024, which is more than a million more units than in 2023.","3.7","GPUs","[""luccioni2025a""]","is_blank","is_blank","According to the Luccioni et al. 2025 paper, NVIDIA shipped 3.7 million data center GPUs in 2024, which is more than a million more units than in 2023."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to the document, in breaking down the components of amortized hardware CapEx + energy costs, AI accelerator chips account for an average of 44% of the total cost.","44","percent","[""cottier2024""]","is_blank","is_blank","According to the document, in breaking down the components of amortized hardware CapEx + energy costs, AI accelerator chips account for an average of 44% of the total cost."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","According to multiple sources, Facebook's data centers have achieved a Power Usage Effectiveness (PUE) of approximately 1.10, which is significantly more efficient than typical small-scale data centers.","1.10","PUE","[""wu2021a"", ""wu2021b"", ""ebert2024""]","is_blank","is_blank","According to multiple sources, Facebook's data centers have achieved a Power Usage Effectiveness (PUE) of approximately 1.10, which is significantly more efficient than typical small-scale data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context explicitly mentions the Finnish project ETAIROS, which stands for 'Ethical AI for the Governance of the Society'. The project proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly mentions the Finnish project ETAIROS, which stands for 'Ethical AI for the Governance of the Society'. The project proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","According to the document, the estimated training cost for Google's Gemini Ultra was $30 million, which is derived from their analysis of publicly-announced training run costs for frontier AI models.","30","USD","[""cottier2024""]","is_blank","is_blank","According to the document, the estimated training cost for Google's Gemini Ultra was $30 million, which is derived from their analysis of publicly-announced training run costs for frontier AI models."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context indicates that the AI Act requires providers of GPAI models with systemic risk and HRAI systems to conduct risk assessments. The authors argue these assessments should include environmental risks, interpreting the Act's fundamental rights provisions to encompass environmental protection.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act requires providers of GPAI models with systemic risk and HRAI systems to conduct risk assessments. The authors argue these assessments should include environmental risks, interpreting the Act's fundamental rights provisions to encompass environmental protection."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context clearly indicates that the EPA's recently tightened primary standard for the annual average limit of PM2.5 is 9 µg/m³, which is higher than the WHO's recommended level of 5 µg/m³.","9","µg/m³","[""han2024""]","is_blank","is_blank","The context clearly indicates that the EPA's recently tightened primary standard for the annual average limit of PM2.5 is 9 µg/m³, which is higher than the WHO's recommended level of 5 µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","According to a life cycle assessment referenced in the context, one Amazon Kindle e-reader produces the same amount of CO2 as 115 physical print books.","115","books","[""luccioni2025a""]","is_blank","is_blank","According to a life cycle assessment referenced in the context, one Amazon Kindle e-reader produces the same amount of CO2 as 115 physical print books."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the research paper, in some low-income counties, the per-household health burden from air pollutants could exceed that in less-impacted communities by more than 200 times. This disparity is specifically highlighted in multiple sections of the document as a key finding of their study on data center public health impacts.","200","multiplier","[""han2024""]","is_blank","is_blank","According to the research paper, in some low-income counties, the per-household health burden from air pollutants could exceed that in less-impacted communities by more than 200 times. This disparity is specifically highlighted in multiple sections of the document as a key finding of their study on data center public health impacts."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","From the data in Table B1 in the Zschache 2025 reference, the energy consumption for the DS Llama 70B model on the FKTG dataset is 702.06 Wh, which is measured on the Capella system in a single node configuration, with an average over 10 runs.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","From the data in Table B1 in the Zschache 2025 reference, the energy consumption for the DS Llama 70B model on the FKTG dataset is 702.06 Wh, which is measured on the Capella system in a single node configuration, with an average over 10 runs."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","According to the context from the Schwartz et al. 2019 paper, the OpenAI GPT2-XL model contains 1.5 billion parameters.","1.5","parameters","[""schwartz2019""]","is_blank","is_blank","According to the context from the Schwartz et al. 2019 paper, the OpenAI GPT2-XL model contains 1.5 billion parameters."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","Jevons Paradox describes how technological progress improving efficiency can lead to increased overall resource consumption. In the context, this is explicitly discussed in multiple sources, particularly in the paper by Luccioni et al., which explains that efficiency improvements often result in higher total usage rather than reduced consumption.","Jevons Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","Jevons Paradox describes how technological progress improving efficiency can lead to increased overall resource consumption. In the context, this is explicitly discussed in multiple sources, particularly in the paper by Luccioni et al., which explains that efficiency improvements often result in higher total usage rather than reduced consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context indicates that the carbon footprint of training a single large ML model like Meena is equivalent to 242,231 miles driven by an average passenger vehicle. While this provides insight into the energy usage of a single model training run, it does not directly compare the number of Meena training runs to a GPT-3 full training run.","is_blank","multiplier","[""wu2021a""]","is_blank","is_blank","The context indicates that the carbon footprint of training a single large ML model like Meena is equivalent to 242,231 miles driven by an average passenger vehicle. While this provides insight into the energy usage of a single model training run, it does not directly compare the number of Meena training runs to a GPT-3 full training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the study, the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh. This makes image generation the most energy-intensive task examined in the research.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","According to Table 2 in the study, the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh. This makes image generation the most energy-intensive task examined in the research."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","According to the context from the Strubell et al. 2019 paper, the estimated CO2e for an average American life in one year is 36,156 pounds.","36156","lbs","[""strubell2019""]","is_blank","is_blank","According to the context from the Strubell et al. 2019 paper, the estimated CO2e for an average American life in one year is 36,156 pounds."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","According to Figure 6 and the accompanying text, Marion County in West Virginia has the highest per-household health cost at $1,218.30 among the counties studied from 2019 to 2023.","Marion","is_blank","[""han2024""]","is_blank","is_blank","According to Figure 6 and the accompanying text, Marion County in West Virginia has the highest per-household health cost at $1,218.30 among the counties studied from 2019 to 2023."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","","","multiplier","[]","is_blank","is_blank",""
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","According to the study by Luccioni et al., the fine-tuning energy cost for the BLOOMz-7B model was 7,571 kWh, which adds approximately 15% to the initial training energy consumption of 51,686 kWh.","7571","kWh","[""luccioni2024"", ""ebert2024""]","is_blank","is_blank","According to the study by Luccioni et al., the fine-tuning energy cost for the BLOOMz-7B model was 7,571 kWh, which adds approximately 15% to the initial training energy consumption of 51,686 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The context specifically states the total number of parameters for the FLM-101B model in its final stage. The model is described as having 101 billion parameters, trained progressively from 16B to 51B and finally to 101B.","101","parameters","[""li2025a""]","is_blank","is_blank","The context specifically states the total number of parameters for the FLM-101B model in its final stage. The model is described as having 101 billion parameters, trained progressively from 16B to 51B and finally to 101B."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","From the context provided by Dodge et al. 2022, a 6.1 billion parameter transformer model's full training run was estimated to consume 103,500 kWh. This was calculated by scaling up their partial training run of 13% to a full 60-day training period.","103500","inferences","[""dodge2022""]","is_blank","is_blank","From the context provided by Dodge et al. 2022, a 6.1 billion parameter transformer model's full training run was estimated to consume 103,500 kWh. This was calculated by scaling up their partial training run of 13% to a full 60-day training period."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The study provides a direct statement about the total energy consumed during their model experimentation and evaluation. In the 'Multi-Purpose Models' section, they explicitly state the total energy consumption.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The study provides a direct statement about the total energy consumed during their model experimentation and evaluation. In the 'Multi-Purpose Models' section, they explicitly state the total energy consumption."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","Based on the table in the context, for the BLOOMz-7B model, the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning is 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","Based on the table in the context, for the BLOOMz-7B model, the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context clearly indicates that the dataset for the German nuclear waste site objection texts is called the FKTG-dataset, which was scraped from a website and contains statements about the process of selecting a repository site for high-level radioactive waste in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly indicates that the dataset for the German nuclear waste site objection texts is called the FKTG-dataset, which was scraped from a website and contains statements about the process of selecting a repository site for high-level radioactive waste in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The provided context documents do not contain specific information about Meta's Llama 3 inference serving clusters' freshwater consumption in 2024. While the documents discuss water consumption and energy efficiency for AI models like GPT-3 and Llama 3, they do not provide a precise figure for Llama 3's inference water usage in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The provided context documents do not contain specific information about Meta's Llama 3 inference serving clusters' freshwater consumption in 2024. While the documents discuss water consumption and energy efficiency for AI models like GPT-3 and Llama 3, they do not provide a precise figure for Llama 3's inference water usage in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","There is no specific information in the provided context about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While the context discusses rare earth metals and their mining impact, it does not address recycling rates.","is_blank","percent","[""is_blank""]","is_blank","is_blank","There is no specific information in the provided context about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While the context discusses rare earth metals and their mining impact, it does not address recycling rates."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context from Strubell et al. shows that a round trip between San Francisco and New York produces 1,984 lbs of CO2e. The Evolved Transformer NAS model with neural architecture search emitted 626,155 lbs of CO2. By dividing these values, approximately 315 passengers would represent the equivalent carbon emissions.","315","passengers","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","The context from Strubell et al. shows that a round trip between San Francisco and New York produces 1,984 lbs of CO2e. The Evolved Transformer NAS model with neural architecture search emitted 626,155 lbs of CO2. By dividing these values, approximately 315 passengers would represent the equivalent carbon emissions."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the May 2025 data from OpenRouter, 84% of LLM token usage occurred through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","is_blank","According to the May 2025 data from OpenRouter, 84% of LLM token usage occurred through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to the context, the minimum number of NVIDIA A100 80GB GPUs required to run meaningful LLaMA-13B inference without compression or quantization is 1 GPU. The Table II in the document shows that for the 13B model, only 1 A100 GPU is required, with a maximum batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","According to the context, the minimum number of NVIDIA A100 80GB GPUs required to run meaningful LLaMA-13B inference without compression or quantization is 1 GPU. The Table II in the document shows that for the 13B model, only 1 A100 GPU is required, with a maximum batch size of 64."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context from Dodge et al. (2022) provides a detailed breakdown of a 6.1 billion parameter transformer model's training energy consumption. They report that the model was trained on 256 NVIDIA A100 GPUs for 8 days (only 13% of a complete training run), consuming 13.8 MWh. They estimate that a full training run would consume approximately 103.5 MWh or 103,500 kWh, which translates to CO2 emissions in metric tons.","103.5","metric tons","[""dodge2022""]","is_blank","is_blank","The context from Dodge et al. (2022) provides a detailed breakdown of a 6.1 billion parameter transformer model's training energy consumption. They report that the model was trained on 256 NVIDIA A100 GPUs for 8 days (only 13% of a complete training run), consuming 13.8 MWh. They estimate that a full training run would consume approximately 103.5 MWh or 103,500 kWh, which translates to CO2 emissions in metric tons."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The research paper discusses carbon emissions reduction through pruning and quantizing large language models, but does not specify an exact total carbon emissions figure avoided in 2023. The abstract indicates methods can reduce carbon emissions by up to 45% through quantization techniques.","45","tCO2e","[""khan2025""]","is_blank","is_blank","The research paper discusses carbon emissions reduction through pruning and quantizing large language models, but does not specify an exact total carbon emissions figure avoided in 2023. The abstract indicates methods can reduce carbon emissions by up to 45% through quantization techniques."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","In a specific experimental scenario with Llama2-7b, using 2 A100s and 1 A10G resulted in a 24% cost saving over an A100-only strategy. The hybrid approach of mixing GPU types consistently yields the lowest deployment cost by allowing A100s to satisfy bulk service demands while A10Gs handle remaining load at reduced cost.","24","percent","[""griggs2024""]","is_blank","is_blank","In a specific experimental scenario with Llama2-7b, using 2 A100s and 1 A10G resulted in a 24% cost saving over an A100-only strategy. The hybrid approach of mixing GPU types consistently yields the lowest deployment cost by allowing A100s to satisfy bulk service demands while A10Gs handle remaining load at reduced cost."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to the Chen et al. (2025) paper, the price per hour for an NVIDIA H20 is estimated at $4.63/hr. This is noted as an estimated price since the H20 is not readily available on cloud service providers, and the price is calculated using the relative complete system cost against the H100.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","According to the Chen et al. (2025) paper, the price per hour for an NVIDIA H20 is estimated at $4.63/hr. This is noted as an estimated price since the H20 is not readily available on cloud service providers, and the price is calculated using the relative complete system cost against the H100."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context directly contradicts the statement. According to the document, most carbon footprint analyses for AI models require manually contacting authors to gather information, rather than gathering it automatically.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context directly contradicts the statement. According to the document, most carbon footprint analyses for AI models require manually contacting authors to gather information, rather than gathering it automatically."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","According to the paper, in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","is_blank","According to the paper, in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to the context, 4 A100 GPUs with 80GB memory are the minimum hardware configuration required to run LLaMA 7B inference without compression or quantization. This is based on the baseline hardware requirements table in the document, which shows that a single A100 GPU can handle a batch size of 64 for the 7B model.","4","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","According to the context, 4 A100 GPUs with 80GB memory are the minimum hardware configuration required to run LLaMA 7B inference without compression or quantization. This is based on the baseline hardware requirements table in the document, which shows that a single A100 GPU can handle a batch size of 64 for the 7B model."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The research paper directly states that their experimental results revealed 'methods can reduce energy consumption and carbon emissions by up to 45% post quantization', which precisely matches the claim in the question.","1","is_blank","[""khan2025""]","is_blank","is_blank","The research paper directly states that their experimental results revealed 'methods can reduce energy consumption and carbon emissions by up to 45% post quantization', which precisely matches the claim in the question."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","According to the context, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers and were able to collect only 95 answers, representing a 15.4% response rate.","95","answers","[""luccioni2023"", ""luccioni2025b""]","is_blank","is_blank","According to the context, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers and were able to collect only 95 answers, representing a 15.4% response rate."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context provides specific details about the JetMoE-8B training: a total budget of $100,000, using 30,000 H100 GPU hours. To calculate the cost per GPU hour, I'll divide the total budget by the total GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The context provides specific details about the JetMoE-8B training: a total budget of $100,000, using 30,000 H100 GPU hours. To calculate the cost per GPU hour, I'll divide the total budget by the total GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","According to Table 2 in the document, when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost, which is over 100%.","122","percent","[""han2024""]","is_blank","is_blank","According to Table 2 in the document, when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost, which is over 100%."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","According to the context, JetMoE-8B was trained on 1.25T (1.25 trillion) tokens from primarily English data including web documents, mathematics, and code.","1.25","tokens","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained on 1.25T (1.25 trillion) tokens from primarily English data including web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","According to the Amazon Renewable Energy Projects table in the context, as of January 2024, Amazon had 36 renewable energy projects in the United Kingdom, with a total MW capacity of 901.","36","projects","[""amazon2023""]","is_blank","is_blank","According to the Amazon Renewable Energy Projects table in the context, as of January 2024, Amazon had 36 renewable energy projects in the United Kingdom, with a total MW capacity of 901."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","According to the Amazon 2023 Sustainability Report, the workforce representation data shows that for the U.S. workforce across all levels in 2023, 44.1% identified as men.","44.1","percent","[""amazon2023""]","is_blank","is_blank","According to the Amazon 2023 Sustainability Report, the workforce representation data shows that for the U.S. workforce across all levels in 2023, 44.1% identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","According to the context, Apple reports that its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","According to the context, Apple reports that its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","","","seconds","[]","is_blank","is_blank",""
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The authors introduced the 'granularity metric', which is defined as the ratio of calculation to communication time when scaling distributed training across different regions and hardware setups.","Granularity metric","is_blank","[""erben2023""]","is_blank","is_blank","The authors introduced the 'granularity metric', which is defined as the ratio of calculation to communication time when scaling distributed training across different regions and hardware setups."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year.","640","times","[""luccioni2025a""]","is_blank","is_blank","According to the context, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context provides a clear definition of water withdrawal as freshwater taken from ground or surface sources, either temporarily or permanently, for uses such as agricultural, industrial, or municipal purposes.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The context provides a clear definition of water withdrawal as freshwater taken from ground or surface sources, either temporarily or permanently, for uses such as agricultural, industrial, or municipal purposes."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","According to the context, the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is stated explicitly in the preface of the document.","5","years","[""stone2022""]","is_blank","is_blank","According to the context, the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is stated explicitly in the preface of the document."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","According to multiple references from the Wu et al. 2021 document, the average U.S. household in 2021 was reported to have 25 connected devices, including cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b""]","is_blank","is_blank","According to multiple references from the Wu et al. 2021 document, the average U.S. household in 2021 was reported to have 25 connected devices, including cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","According to the context from Luccioni et al. (2025), the energy consumption range for pre-training large language models spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with significant variation across different models and sizes.","3500","MWh","[""luccioni2025c""]","is_blank","is_blank","According to the context from Luccioni et al. (2025), the energy consumption range for pre-training large language models spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with significant variation across different models and sizes."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program successfully beat human contenders in the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders' is false.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context clearly states that IBM's Watson program successfully beat human contenders in the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders' is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to the context from Luccioni et al. 2025, one study estimates that 10-50 queries to GPT-3 consume approximately half a liter of water.","10-50","queries","[""luccioni2025a""]","is_blank","is_blank","According to the context from Luccioni et al. 2025, one study estimates that 10-50 queries to GPT-3 consume approximately half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","","","score","[]","is_blank","is_blank",""
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to the context, GPT-3 needs to 'drink' a 500 mL bottle of water for roughly 10-50 medium-length responses, depending on deployment conditions. This means for a single 500 mL bottle, the number of medium-length responses could range between 10 to 50.","50","responses","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs to 'drink' a 500 mL bottle of water for roughly 10-50 medium-length responses, depending on deployment conditions. This means for a single 500 mL bottle, the number of medium-length responses could range between 10 to 50."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The Griggs et al. 2024 paper reports that in conversational chat settings, Mélange can reduce deployment costs by up to 77%. This is consistently mentioned across multiple sections of the paper, including the abstract and conclusion.","77","percent","[""griggs2024""]","is_blank","is_blank","The Griggs et al. 2024 paper reports that in conversational chat settings, Mélange can reduce deployment costs by up to 77%. This is consistently mentioned across multiple sections of the paper, including the abstract and conclusion."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The document clearly states that for the LLaMA 65B model, a minimum of 4 A100 GPUs with 80GB of memory are required to run meaningful inferences without compression or quantization. This is directly mentioned in multiple sections of the paper.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The document clearly states that for the LLaMA 65B model, a minimum of 4 A100 GPUs with 80GB of memory are required to run meaningful inferences without compression or quantization. This is directly mentioned in multiple sections of the paper."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","Multiple sources in the context consistently report that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","is_blank","Multiple sources in the context consistently report that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","Multiple sources in the context consistently report that NVIDIA estimated inference processing to account for 80 to 90% of machine learning (ML) cloud computing demand in 2019, specifically according to AWS, which was the largest global cloud provider at the time.","85","percent","[""luccioni2024"", ""fernandez2025"", ""chung2025""]","is_blank","is_blank","Multiple sources in the context consistently report that NVIDIA estimated inference processing to account for 80 to 90% of machine learning (ML) cloud computing demand in 2019, specifically according to AWS, which was the largest global cloud provider at the time."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The study's Ethical Considerations Statement explicitly reports the total carbon emissions generated during their model experimentation and evaluation as 178.97 kg of CO2 equivalent. This figure encompasses the emissions from running 88 models on 3 datasets 10 times to ensure statistical significance.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The study's Ethical Considerations Statement explicitly reports the total carbon emissions generated during their model experimentation and evaluation as 178.97 kg of CO2 equivalent. This figure encompasses the emissions from running 88 models on 3 datasets 10 times to ensure statistical significance."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on Thermal Design Power (TDP) is not reliable. Multiple sources highlight that TDP-based estimations are nearly always an overestimation and can lead to significant inaccuracies in energy consumption calculations.","0","is_blank","[""chung2025"", ""luccioni2023"", ""cottier2024"", ""ebert2024""]","is_blank","is_blank","The context indicates that estimating GPU energy consumption based on Thermal Design Power (TDP) is not reliable. Multiple sources highlight that TDP-based estimations are nearly always an overestimation and can lead to significant inaccuracies in energy consumption calculations."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries, specifically due to reliance on older A100 GPU nodes instead of more efficient H100s. For instance, GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context indicates that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries, specifically due to reliance on older A100 GPU nodes instead of more efficient H100s. For instance, GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","From Figure 8 in the context, the ground truth throughput for Mixtral-CS dense model with batch size 1 is 0.3 queries/sec.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","From Figure 8 in the context, the ground truth throughput for Mixtral-CS dense model with batch size 1 is 0.3 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","According to the paper by Luccioni et al., the direct release of environmental information for notable AI models peaked in 2022 with 10% of models releasing some information. However, after 2022, there was a notable reversal in this trend, with direct environmental disclosures dramatically reducing, especially due to the rise of commercial and proprietary models like ChatGPT.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","According to the paper by Luccioni et al., the direct release of environmental information for notable AI models peaked in 2022 with 10% of models releasing some information. However, after 2022, there was a notable reversal in this trend, with direct environmental disclosures dramatically reducing, especially due to the rise of commercial and proprietary models like ChatGPT."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","According to the table in the Griggs et al. (2024) paper, the normalized on-demand hourly price for an H100 GPU is $7.5164. This price was derived by normalizing the RunPod H100 pricing to match major cloud platform pricing structures.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","According to the table in the Griggs et al. (2024) paper, the normalized on-demand hourly price for an H100 GPU is $7.5164. This price was derived by normalizing the RunPod H100 pricing to match major cloud platform pricing structures."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","Multiple sources in the context indicate that OpenAI's GPT-3 training used approximately 5 million liters of water. The research by Li et al. analyzed GPT-3's freshwater consumption and specifically estimated over 5 million liters used during training.","5","liters of water","[""jegham2025"", ""li2025b""]","is_blank","is_blank","Multiple sources in the context indicate that OpenAI's GPT-3 training used approximately 5 million liters of water. The research by Li et al. analyzed GPT-3's freshwater consumption and specifically estimated over 5 million liters used during training."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context from Griggs et al. specifies that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, I'll divide the monthly cost by the number of hours in a month (30 days * 24 hours).","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context from Griggs et al. specifies that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, I'll divide the monthly cost by the number of hours in a month (30 days * 24 hours)."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context from Wu et al. (2021), to increase the BLEU score of a GPT-3-based language translation task from 5 to 40 requires a model that is 1,000 times larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context from Wu et al. (2021), to increase the BLEU score of a GPT-3-based language translation task from 5 to 40 requires a model that is 1,000 times larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to the context, a study by Strubell et al. found that training a Transformer model with Neural Architecture Search produces 626,155 lbs of CO2 emissions, which is comparable to the lifetime emissions of five cars. The context does not provide a direct calculation of driving distance equivalent to these emissions.","is_blank","miles","[""strubell2019"", ""luccioni2023"", ""dodge2022""]","is_blank","is_blank","According to the context, a study by Strubell et al. found that training a Transformer model with Neural Architecture Search produces 626,155 lbs of CO2 emissions, which is comparable to the lifetime emissions of five cars. The context does not provide a direct calculation of driving distance equivalent to these emissions."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","Based on the context, the BLOOMz-7B model has an inference energy consumption of 1.0 × 10−4 kWh per inference. With 606,096 downloads and estimating 1 million inferences per download, the total energy consumption would be calculated by multiplying the inference energy by the number of total inferences.","60.61","MWh","[""luccioni2024""]","is_blank","is_blank","Based on the context, the BLOOMz-7B model has an inference energy consumption of 1.0 × 10−4 kWh per inference. With 606,096 downloads and estimating 1 million inferences per download, the total energy consumption would be calculated by multiplying the inference energy by the number of total inferences."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context from the research paper explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours during its pre-training process.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context from the research paper explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours during its pre-training process."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The research by Cottier et al. predicts that if the current trend of AI training cost growth continues at 2.4x per year, the largest training runs will exceed $1 billion by 2027. This projection is based on analyzing the historical growth of training costs for frontier AI models.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The research by Cottier et al. predicts that if the current trend of AI training cost growth continues at 2.4x per year, the largest training runs will exceed $1 billion by 2027. This projection is based on analyzing the historical growth of training costs for frontier AI models."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","Based on the context from the Luccioni et al. 2023 paper, the total training time for GPT-3 was estimated to be over 3.5 million GPU hours, which is equivalent to approximately 14.8 days using 10,000 GPUs.","3500000","FLOPS","[""luccioni2023""]","is_blank","is_blank","Based on the context from the Luccioni et al. 2023 paper, the total training time for GPT-3 was estimated to be over 3.5 million GPU hours, which is equivalent to approximately 14.8 days using 10,000 GPUs."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization is 8 GPUs. While the paper notes the model could theoretically be sharded across 6 V100 GPUs, they chose 8 GPUs since the model architecture is better suited for balanced sharding across 8 GPUs.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","According to the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization is 8 GPUs. While the paper notes the model could theoretically be sharded across 6 V100 GPUs, they chose 8 GPUs since the model architecture is better suited for balanced sharding across 8 GPUs."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","In the 101B training stage, the single-GPU throughput consistently exceeded 160 teraFLOPs/sec with a GPU utilization rate of 52.88%. Using this throughput data and the training configuration details, I can estimate the total computational work performed during this stage in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","In the 101B training stage, the single-GPU throughput consistently exceeded 160 teraFLOPs/sec with a GPU utilization rate of 52.88%. Using this throughput data and the training configuration details, I can estimate the total computational work performed during this stage in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","There is no direct mention of AlexNet's top-1 accuracy on ImageNet in 2012 in these context snippets. While one snippet discusses AlexNet briefly, it does not provide the specific top-1 accuracy metric requested.","is_blank","percent","[""is_blank""]","is_blank","is_blank","There is no direct mention of AlexNet's top-1 accuracy on ImageNet in 2012 in these context snippets. While one snippet discusses AlexNet briefly, it does not provide the specific top-1 accuracy metric requested."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context explicitly states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, which indicates a total of 24 * 8 = 192 A800 GPUs used for training.","192","GPUs","[""li2025a""]","is_blank","is_blank","The context explicitly states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, which indicates a total of 24 * 8 = 192 A800 GPUs used for training."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The Strubell et al. 2019 study estimated that performing neural architecture search (NAS) to train a Transformer-based model would produce 626,155 pounds (284 metric tons) of CO2 emissions, which is comparable to the lifetime carbon emissions of five cars.","626155","lifetimes","[""strubell2019"", ""luccioni2023"", ""luccioni2025c""]","is_blank","is_blank","The Strubell et al. 2019 study estimated that performing neural architecture search (NAS) to train a Transformer-based model would produce 626,155 pounds (284 metric tons) of CO2 emissions, which is comparable to the lifetime carbon emissions of five cars."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context directly states that FAIR's RoBERTa was trained on 160GB of text, which is approximately 40 billion word-pieces, and this training process required around 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The context directly states that FAIR's RoBERTa was trained on 160GB of text, which is approximately 40 billion word-pieces, and this training process required around 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the Amazon 2023 Sustainability Report, their on-site solar energy systems in 2023 avoid 47,500 metric tons of carbon dioxide equivalent (CO2e) each year compared to nonrenewable electricity sources.","47500","metric tons","[""amazon2023""]","is_blank","is_blank","According to the Amazon 2023 Sustainability Report, their on-site solar energy systems in 2023 avoid 47,500 metric tons of carbon dioxide equivalent (CO2e) each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","Several context snippets discuss using distributed computing frameworks for deploying large language models across multiple GPUs. Specifically, the zschache2025 reference mentions using vllm library with a ray cluster for multi-node computations, and deploying models by distributing model weights across multiple GPUs using tensor model parallelism (TMP) and pipeline model parallelism (PMP).","vllm","is_blank","[""zschache2025""]","is_blank","is_blank","Several context snippets discuss using distributed computing frameworks for deploying large language models across multiple GPUs. Specifically, the zschache2025 reference mentions using vllm library with a ray cluster for multi-node computations, and deploying models by distributing model weights across multiple GPUs using tensor model parallelism (TMP) and pipeline model parallelism (PMP)."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to Table B2 in the document, when deploying the Llama 3.1 70B model on two nodes instead of one, the energy consumption increased by a factor of 1.95 (from 48.60 Wh on a single node to 94.88 Wh on two nodes).","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","According to Table B2 in the document, when deploying the Llama 3.1 70B model on two nodes instead of one, the energy consumption increased by a factor of 1.95 (from 48.60 Wh on a single node to 94.88 Wh on two nodes)."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","While the context discusses water consumption for AI models like GPT-3, it does not provide a precise figure for water consumption per ChatGPT user session in 2023. One study suggests 10-50 queries on GPT-3 consumes around half a liter of water, but this is not specific to a single user session in 2023.","is_blank","gallons of water","[""luccioni2025a""]","is_blank","is_blank","While the context discusses water consumption for AI models like GPT-3, it does not provide a precise figure for water consumption per ChatGPT user session in 2023. One study suggests 10-50 queries on GPT-3 consumes around half a liter of water, but this is not specific to a single user session in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","According to the paper, when scaling 700 million daily GPT-4o queries to annual electricity consumption, the total usage would be equivalent to the electricity consumption of 35,000 U.S. residential households.","35000","homes","[""jegham2025""]","is_blank","is_blank","According to the paper, when scaling 700 million daily GPT-4o queries to annual electricity consumption, the total usage would be equivalent to the electricity consumption of 35,000 U.S. residential households."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the context from Luccioni et al.'s 2025 paper, Microsoft reported a 34% increase in global water consumption between 2021 and 2022, which totaled over 1.7 billion gallons.","34","percent","[""luccioni2025a""]","is_blank","is_blank","According to the context from Luccioni et al.'s 2025 paper, Microsoft reported a 34% increase in global water consumption between 2021 and 2022, which totaled over 1.7 billion gallons."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that for Yelp sentiment analysis, traditional models perform considerably worse than large language models (LLMs). The analysis indicates that LLMs justify their energy costs by significantly outperforming traditional models in this specific sentiment analysis task.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that for Yelp sentiment analysis, traditional models perform considerably worse than large language models (LLMs). The analysis indicates that LLMs justify their energy costs by significantly outperforming traditional models in this specific sentiment analysis task."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","From the context document (morrison2025), the Power Usage Effectiveness (PUE) for Augusta (the Iowa datacenter) was 1.12, and this specific reference is mentioned for the cluster where the 13B model was trained.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","From the context document (morrison2025), the Power Usage Effectiveness (PUE) for Augusta (the Iowa datacenter) was 1.12, and this specific reference is mentioned for the cluster where the 13B model was trained."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the context, the total estimated number of GPT-4o queries for 2025 is approximately 772 billion. This projection is based on a monthly prompt growth rate of 20% from January to May 2025, followed by a decaying growth pattern from June to December.","772","queries","[""jegham2025""]","is_blank","is_blank","According to the context, the total estimated number of GPT-4o queries for 2025 is approximately 772 billion. This projection is based on a monthly prompt growth rate of 20% from January to May 2025, followed by a decaying growth pattern from June to December."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","According to Table 3 in the context, JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard across multiple benchmarks, which was the highest performance among the models compared.","53.0","score","[""shen2024""]","is_blank","is_blank","According to Table 3 in the context, JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard across multiple benchmarks, which was the highest performance among the models compared."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","Based on the provided context documents, there is no definitive global count of AI training runs conducted solely on renewable power in 2022. While the documents discuss renewable energy and AI's carbon footprint, they do not provide a specific number of training runs on renewable-only power.","is_blank","training runs","[""wu2021a"", ""luccioni2025a"", ""ebert2024"", ""han2024"", ""luccioni2025c"", ""morrison2025""]","is_blank","is_blank","Based on the provided context documents, there is no definitive global count of AI training runs conducted solely on renewable power in 2022. While the documents discuss renewable energy and AI's carbon footprint, they do not provide a specific number of training runs on renewable-only power."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context directly addresses the open-source exemption and proposes eliminating it. The authors argue that open-source models should adhere to the same reporting standards as proprietary models and that making parts of a model public does not justify exclusion from environmental accountability.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context directly addresses the open-source exemption and proposes eliminating it. The authors argue that open-source models should adhere to the same reporting standards as proprietary models and that making parts of a model public does not justify exclusion from environmental accountability."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context from Ebert et al. 2024 provides multiple references to the national datacenter average Power Usage Effectiveness (PUE) in 2023, which was 1.58 globally. While this is not specifically for the US in 2020, it is the closest available data point.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context from Ebert et al. 2024 provides multiple references to the national datacenter average Power Usage Effectiveness (PUE) in 2023, which was 1.58 globally. While this is not specifically for the US in 2020, it is the closest available data point."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","According to the context, when the batch size increases to 32 in the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB, which can potentially lead to GPU memory exhaustion.","5.312","GB","[""kim2025""]","is_blank","is_blank","According to the context, when the batch size increases to 32 in the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB, which can potentially lead to GPU memory exhaustion."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","According to the context, R&D staff costs (including equity) for the four models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) ranged between 29% and 49% of the total amortized model development cost.","29-49","percent","[""cottier2024""]","is_blank","is_blank","According to the context, R&D staff costs (including equity) for the four models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) ranged between 29% and 49% of the total amortized model development cost."
"q213","Which software package was used to measure energy consumption during inference runs?","Multiple context snippets consistently indicate that CodeCarbon was the primary software package used to measure energy consumption during inference runs across different research studies.","CodeCarbon","is_blank","[""morrison2025"", ""zschache2025"", ""rubei2025""]","is_blank","is_blank","Multiple context snippets consistently indicate that CodeCarbon was the primary software package used to measure energy consumption during inference runs across different research studies."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","According to the analysis of 100 news articles discussing ChatGPT's energy consumption, 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","According to the analysis of 100 news articles discussing ChatGPT's energy consumption, 53% of the articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The Compute Time Calibration Function (CTCF) is a proposed linear transformation function designed to adjust theoretical computation time to match actual GPU execution time in LLM inference. It aims to correct discrepancies between manufacturer-provided FLOPS values and real-world GPU performance by using scaling and offset parameters.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The Compute Time Calibration Function (CTCF) is a proposed linear transformation function designed to adjust theoretical computation time to match actual GPU execution time in LLM inference. It aims to correct discrepancies between manufacturer-provided FLOPS values and real-world GPU performance by using scaling and offset parameters."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context clearly indicates that increasing the number of GPU shards always increases the energy per second. Specifically, the text states that 'increasing the number of shards always increases the wattage' and provides an example of energy increasing when going from 16 to 32 shards at the same batch size.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context clearly indicates that increasing the number of GPU shards always increases the energy per second. Specifically, the text states that 'increasing the number of shards always increases the wattage' and provides an example of energy increasing when going from 16 to 32 shards at the same batch size."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context indicates that an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. One 12-inch silicon wafer produces about 63 H100s, which allows calculation of the water consumption from rare earth metal mining for a single H100 GPU.","2.2","kL","[""morrison2025""]","is_blank","is_blank","The context indicates that an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. One 12-inch silicon wafer produces about 63 H100s, which allows calculation of the water consumption from rare earth metal mining for a single H100 GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context indicates that the authors propose removing the open-source exemption from reporting obligations, specifically stating that open-source models should adhere to the same energy consumption reporting standards as proprietary models.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the authors propose removing the open-source exemption from reporting obligations, specifically stating that open-source models should adhere to the same energy consumption reporting standards as proprietary models."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","Multiple sources confirm that in 2020, Amazon, Microsoft, Meta (Facebook), and Google collectively accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","Multiple sources confirm that in 2020, Amazon, Microsoft, Meta (Facebook), and Google collectively accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","According to the context, U.S. data centers have resulted in a total public health cost of about $6.7 billion in 2023, based on the average attribution method of calculating electricity-related health impacts.","6.7","USD","[""han2024""]","is_blank","is_blank","According to the context, U.S. data centers have resulted in a total public health cost of about $6.7 billion in 2023, based on the average attribution method of calculating electricity-related health impacts."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","From the energy consumption table and analysis, the o3 model consumes 12.222 Wh for long prompts (10k input-1.5k output), while GPT-4.1 nano consumes 0.827 Wh for the same prompt length. By dividing these values, the o3 model consumes approximately 14.8 times more energy than GPT-4.1 nano for a long prompt.","14.8","multiplier","[""jegham2025""]","is_blank","is_blank","From the energy consumption table and analysis, the o3 model consumes 12.222 Wh for long prompts (10k input-1.5k output), while GPT-4.1 nano consumes 0.827 Wh for the same prompt length. By dividing these values, the o3 model consumes approximately 14.8 times more energy than GPT-4.1 nano for a long prompt."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","For the short-context Arena dataset with a 120ms SLO, the paper shows that Mélange achieves a cost reduction range of 15-77% compared to single-GPU baselines.","77","percent","[""griggs2024""]","is_blank","is_blank","For the short-context Arena dataset with a 120ms SLO, the paper shows that Mélange achieves a cost reduction range of 15-77% compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The paper provides a detailed breakdown of carbon emissions for FLM-101B in Table 3, which is referenced in the carbon footprint analysis section. The text explicitly states that their model yields only 1/10 of the pre-training carbon footprint of a typical large language model.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The paper provides a detailed breakdown of carbon emissions for FLM-101B in Table 3, which is referenced in the carbon footprint analysis section. The text explicitly states that their model yields only 1/10 of the pre-training carbon footprint of a typical large language model."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide a direct measurement of the total execution time for a sparse Mixtral model with a batch size of 1 on an NVIDIA A40-48 GB GPU. While the paper discusses fine-tuning execution times and throughput, it does not specify an exact total execution time matching the precise parameters of the question.","is_blank","seconds","[""xia2024""]","is_blank","is_blank","The context does not provide a direct measurement of the total execution time for a sparse Mixtral model with a batch size of 1 on an NVIDIA A40-48 GB GPU. While the paper discusses fine-tuning execution times and throughput, it does not specify an exact total execution time matching the precise parameters of the question."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context from the han2024 document clearly indicates that the public health costs of AI are NOT evenly distributed across U.S. communities. Specifically, the research shows that public health impacts are highly uneven, disproportionately affecting low-income communities, with per-household health costs potentially varying up to 200 times between different areas.","0","is_blank","[""han2024""]","is_blank","is_blank","The context from the han2024 document clearly indicates that the public health costs of AI are NOT evenly distributed across U.S. communities. Specifically, the research shows that public health impacts are highly uneven, disproportionately affecting low-income communities, with per-household health costs potentially varying up to 200 times between different areas."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context from the Wu et al. 2021 paper explicitly confirms that GPU theoretical performance per watt doubles approximately every 3-4 years, as stated in Figure 2 of the reference.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context from the Wu et al. 2021 paper explicitly confirms that GPU theoretical performance per watt doubles approximately every 3-4 years, as stated in Figure 2 of the reference."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","According to the context, Ollama was the specific open-source tool used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study. The paper explicitly states that they 'apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments'.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, Ollama was the specific open-source tool used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study. The paper explicitly states that they 'apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments'."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The authors chose Backblaze (B2), an independent S3 storage provider, to simulate a real-world deployment with a non-public dataset for their multi-cloud experiments on spot instances. They used WebDataset to access the datasets on-demand via shards in tar format, which allows for features like streaming decompression and preprocessing.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","is_blank","The authors chose Backblaze (B2), an independent S3 storage provider, to simulate a real-world deployment with a non-public dataset for their multi-cloud experiments on spot instances. They used WebDataset to access the datasets on-demand via shards in tar format, which allows for features like streaming decompression and preprocessing."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates that there is a strong correlation between inference energy consumption and model runtime, suggesting that the relationship is not exactly linear but highly correlated. The authors note that execution time can serve as a practical proxy for energy usage.","1","is_blank","[""zschache2025""]","is_blank","is_blank","The context indicates that there is a strong correlation between inference energy consumption and model runtime, suggesting that the relationship is not exactly linear but highly correlated. The authors note that execution time can serve as a practical proxy for energy usage."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context specifically states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024.","Senator Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","The context specifically states that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","Based on the context from Griggs et al. (2024), the price per hour for an NVIDIA H100 GPU is $7.5164, which was normalized from RunPod's pricing to match major cloud platform pricing structures.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","Based on the context from Griggs et al. (2024), the price per hour for an NVIDIA H100 GPU is $7.5164, which was normalized from RunPod's pricing to match major cloud platform pricing structures."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","Based on the context from Amazon's 2023 Sustainability Report, AWS announced in February 2024 that the average expected server lifetime had improved from five to six years. This suggests an estimated server/GPU lifetime of approximately 6 years in AI data centers in 2024.","6","years","[""amazon2023""]","is_blank","is_blank","Based on the context from Amazon's 2023 Sustainability Report, AWS announced in February 2024 that the average expected server lifetime had improved from five to six years. This suggests an estimated server/GPU lifetime of approximately 6 years in AI data centers in 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to the research paper, the minimum number of V100 32GB GPUs required to run LLaMA 13B inference without compression or quantization is 2 GPUs. This is clearly stated in Table II of the document, which lists the baseline configurations for different LLaMA model sizes.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","According to the research paper, the minimum number of V100 32GB GPUs required to run LLaMA 13B inference without compression or quantization is 2 GPUs. This is clearly stated in Table II of the document, which lists the baseline configurations for different LLaMA model sizes."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the context, Google reported that training their open source Gemma family of language models emitted 1247.61 tons of CO2 equivalent (tCO2e), which is over 4 times the original 'five cars' estimate of approximately 284 metric tons CO2e.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","According to the context, Google reported that training their open source Gemma family of language models emitted 1247.61 tons of CO2 equivalent (tCO2e), which is over 4 times the original 'five cars' estimate of approximately 284 metric tons CO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","According to the context, Peters et al. (2018) reported that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equates to 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","According to the context, Peters et al. (2018) reported that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equates to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context directly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh according to reference [8]. This figure is mentioned multiple times in the document, indicating a consistent estimate.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context directly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh according to reference [8]. This figure is mentioned multiple times in the document, indicating a consistent estimate."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","Multiple context sources confirm that Google reported a PUE of 1.10 for its hyperscale data centers in 2021. This represents a significant improvement from their 1.21 PUE in 2008 and is substantially more efficient than the global average data center PUE of 1.58.","1.10","PUE","[""wu2021b"", ""dodge2022"", ""cottier2024""]","is_blank","is_blank","Multiple context sources confirm that Google reported a PUE of 1.10 for its hyperscale data centers in 2021. This represents a significant improvement from their 1.21 PUE in 2008 and is substantially more efficient than the global average data center PUE of 1.58."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the context, AWS research shows that in North America, customers can lower their workload carbon footprints by up to 96% when moving from on-premises computing to AWS, specifically when AWS uses 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","is_blank","According to the context, AWS research shows that in North America, customers can lower their workload carbon footprints by up to 96% when moving from on-premises computing to AWS, specifically when AWS uses 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The paper specifically states that their model predicted fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU would cost $3,460. This cost estimation is derived from their analytical model for fine-tuning LLMs.","3460","USD","[""xia2024""]","is_blank","is_blank","The paper specifically states that their model predicted fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU would cost $3,460. This cost estimation is derived from their analytical model for fine-tuning LLMs."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","According to the context, GPUs account for a very significant portion of the total power consumption in data centers. Specifically, one study showed that in a single server setup, the GPU consumed 74% of the total electricity during a BERT model training experiment.","74","percent","[""dodge2022""]","is_blank","is_blank","According to the context, GPUs account for a very significant portion of the total power consumption in data centers. Specifically, one study showed that in a single server setup, the GPU consumed 74% of the total electricity during a BERT model training experiment."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context reveals that JetMoE-8B was trained on a cluster containing 12 nodes with 96 H100 GPUs. This is explicitly stated in section 4.1 of the paper, which details the training infrastructure.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The context reveals that JetMoE-8B was trained on a cluster containing 12 nodes with 96 H100 GPUs. This is explicitly stated in section 4.1 of the paper, which details the training infrastructure."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","According to the context from the Morrison 2025 paper, during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600 watts, which represents over 85% of an H100 GPU's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","is_blank","According to the context from the Morrison 2025 paper, during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600 watts, which represents over 85% of an H100 GPU's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","According to the Strubell et al. paper, the estimated carbon dioxide equivalent (CO2e) for an average human life in one year globally is 11,023 lbs.","11023","lbs","[""strubell2019""]","is_blank","is_blank","According to the Strubell et al. paper, the estimated carbon dioxide equivalent (CO2e) for an average human life in one year globally is 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","For the LLaMA-13B model, the context indicates that when comparing A100 and V100 GPUs, there was a 1.25 times increase in inference latency on the A100 compared to the V100, measured across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","For the LLaMA-13B model, the context indicates that when comparing A100 and V100 GPUs, there was a 1.25 times increase in inference latency on the A100 compared to the V100, measured across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides a precise measurement of energy consumption for a single short GPT-4o query. The study reports that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), which exceeds the energy footprint of a Google search (0.30 Wh) by approximately 40%.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context provides a precise measurement of energy consumption for a single short GPT-4o query. The study reports that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), which exceeds the energy footprint of a Google search (0.30 Wh) by approximately 40%."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","According to the context, the Max-Performance g6e.xlarge instance is about 280% more expensive than InferSave's top choice (g4dn.xlarge) for the 400 TPS SLO experiment.","280","percent","[""kim2025""]","is_blank","is_blank","According to the context, the Max-Performance g6e.xlarge instance is about 280% more expensive than InferSave's top choice (g4dn.xlarge) for the 400 TPS SLO experiment."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context from the Zschache et al. paper specifically indicates that for models generating a single token per inference, a V100 or A30 GPU is more efficient.","V100 and A30","is_blank","[""zschache2025""]","is_blank","is_blank","The context from the Zschache et al. paper specifically indicates that for models generating a single token per inference, a V100 or A30 GPU is more efficient."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","According to the Schwartz et al. (2019) paper, Green AI involves reporting the financial cost of finding, training, and running models as a key practice for transparency and efficiency. Specifically, the paper suggests 'reporting the computational price tag of finding, training, and running models is a key Green AI practice'.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","According to the Schwartz et al. (2019) paper, Green AI involves reporting the financial cost of finding, training, and running models as a key practice for transparency and efficiency. Specifically, the paper suggests 'reporting the computational price tag of finding, training, and running models is a key Green AI practice'."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","According to the context from the Luccioni et al. 2025 paper, the total amount of electronic waste generated worldwide in 2022 was 62 million metric tons. This is directly stated in the text discussing AI's contribution to electronic waste.","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","According to the context from the Luccioni et al. 2025 paper, the total amount of electronic waste generated worldwide in 2022 was 62 million metric tons. This is directly stated in the text discussing AI's contribution to electronic waste."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context does not provide a direct comparison of average system power per processor between the TPU v2 and V100 GPU. While various power and TDP details are mentioned for different hardware, there is no specific calculation of average system power per processor comparing these two specific devices.","is_blank","Watts","[""is_blank""]","is_blank","is_blank","The context does not provide a direct comparison of average system power per processor between the TPU v2 and V100 GPU. While various power and TDP details are mentioned for different hardware, there is no specific calculation of average system power per processor comparing these two specific devices."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to the context, training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater during the training process.","700000","liters","[""li2025b""]","is_blank","is_blank","According to the context, training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater during the training process."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","According to the context, Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021, as stated explicitly in multiple sections of the document.","20","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context, Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021, as stated explicitly in multiple sections of the document."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to the cross-efficiency DEA analysis in the paper, o3-mini achieved the highest cross-efficiency score of 0.884, indicating the best performance in balancing environmental sustainability with model capabilities.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","According to the cross-efficiency DEA analysis in the paper, o3-mini achieved the highest cross-efficiency score of 0.884, indicating the best performance in balancing environmental sustainability with model capabilities."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context from wu2021b directly confirms that current smartphone lifetimes average less than 3 years, specifically stating 'develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones'.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context from wu2021b directly confirms that current smartphone lifetimes average less than 3 years, specifically stating 'develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones'."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context shows that intra-zone scaling with T4 GPUs did achieve speedup, with a maximum speedup of up to 3.2x for CV and 2.75x for NLP at eight GPUs. While the per-GPU contribution decreased with more GPUs, the overall training throughput increased.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context shows that intra-zone scaling with T4 GPUs did achieve speedup, with a maximum speedup of up to 3.2x for CV and 2.75x for NLP at eight GPUs. While the per-GPU contribution decreased with more GPUs, the overall training throughput increased."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context specifies the context window size for the FLM-101B model in the FLM-101B Configurations section, which directly states the context window size.","2048","tokens","[""li2025a""]","is_blank","is_blank","The context specifies the context window size for the FLM-101B model in the FLM-101B Configurations section, which directly states the context window size."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context clearly indicates that LLMs have lower power draw during inference compared to diffusion models. This is because LLM decoding is memory-intensive and low compute-intensive, which means the GPU's compute resources are not fully utilized, resulting in lower power consumption.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context clearly indicates that LLMs have lower power draw during inference compared to diffusion models. This is because LLM decoding is memory-intensive and low compute-intensive, which means the GPU's compute resources are not fully utilized, resulting in lower power consumption."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The 'People Managers' section of the Amazon Representation by the Numbers table shows that in 2023, 31.6% of Amazon's global People Managers identified as women.","31.6","percent","[""amazon2023""]","is_blank","is_blank","The 'People Managers' section of the Amazon Representation by the Numbers table shows that in 2023, 31.6% of Amazon's global People Managers identified as women."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context indicates that when excluding equity, the percentage range of computing hardware costs for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) was 61-76%.","61-76","percent","[""cottier2024""]","is_blank","is_blank","The context indicates that when excluding equity, the percentage range of computing hardware costs for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) was 61-76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context indicates that accuracy and F1 scores do not always improve after optimization. In fact, the paper explicitly states that 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.'","0","is_blank","[""khan2025""]","is_blank","is_blank","The context indicates that accuracy and F1 scores do not always improve after optimization. In fact, the paper explicitly states that 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.'"
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context from the Strubell et al. 2019 paper specifically references the U.S. Environmental Protection Agency (EPA) as providing the average CO2 produced per kilowatt-hour for power consumed in the United States, with a conversion factor of 0.954 lbs/kWh.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","The context from the Strubell et al. 2019 paper specifically references the U.S. Environmental Protection Agency (EPA) as providing the average CO2 produced per kilowatt-hour for power consumed in the United States, with a conversion factor of 0.954 lbs/kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","According to the document, a recent study suggests that global AI could consume 85-134 TWh of electricity in 2027, based on GPU shipment projections.","85-134","TWh","[""li2025b""]","is_blank","is_blank","According to the document, a recent study suggests that global AI could consume 85-134 TWh of electricity in 2027, based on GPU shipment projections."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","According to the Amazon 2023 Sustainability Report, Amazon delivered 150 million packages via electric vehicles (EVs) in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","is_blank","According to the Amazon 2023 Sustainability Report, Amazon delivered 150 million packages via electric vehicles (EVs) in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","In the online inference workload, the study used 128 input tokens and 512 output tokens, evaluating a total of 3000 requests. In the offline inference workload, the study used 1024 input tokens and 128 output tokens, evaluating 1000 requests. Calculating the total tokens across both workloads: (128 + 512) * 3000 + (1024 + 128) * 1000 = (640 * 3000) + (1152 * 1000).","2592000","tokens","[""kim2025""]","is_blank","is_blank","In the online inference workload, the study used 128 input tokens and 512 output tokens, evaluating a total of 3000 requests. In the offline inference workload, the study used 1024 input tokens and 128 output tokens, evaluating 1000 requests. Calculating the total tokens across both workloads: (128 + 512) * 3000 + (1024 + 128) * 1000 = (640 * 3000) + (1152 * 1000)."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","Multiple context sources indicate that the AI Act does NOT mandate disclosure of greenhouse gas emissions from AI applications like oil and gas exploration. Specifically, the Ebert et al. paper notes that the AI Act 'fails to address the greenhouse gas (GHG) emissions generated by AI applications' and gives the example of Microsoft pitching AI models to ExxonMobile for fossil fuel exploration.","0","is_blank","[""ebert2024"", ""luccioni2025a""]","is_blank","is_blank","Multiple context sources indicate that the AI Act does NOT mandate disclosure of greenhouse gas emissions from AI applications like oil and gas exploration. Specifically, the Ebert et al. paper notes that the AI Act 'fails to address the greenhouse gas (GHG) emissions generated by AI applications' and gives the example of Microsoft pitching AI models to ExxonMobile for fossil fuel exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","According to Figure 3 in the Dodge et al. 2022 paper, the Flexible Start optimization for the DenseNet 201 experiment shows up to an 80% reduction in CO2 emissions for the West US region when shifting the start time by up to 24 hours.","80","percent","[""dodge2022""]","is_blank","is_blank","According to Figure 3 in the Dodge et al. 2022 paper, the Flexible Start optimization for the DenseNet 201 experiment shows up to an 80% reduction in CO2 emissions for the West US region when shifting the start time by up to 24 hours."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","According to the study's findings, image generation is the most energy-intensive task, consuming a mean of 2.9 kWh per 1,000 inferences, while text classification is the least energy-intensive task, consuming only 0.002 kWh per 1,000 inferences. This represents a factor difference of approximately 1,450 times between the most and least energy-intensive tasks.","1450","times","[""luccioni2024""]","is_blank","is_blank","According to the study's findings, image generation is the most energy-intensive task, consuming a mean of 2.9 kWh per 1,000 inferences, while text classification is the least energy-intensive task, consuming only 0.002 kWh per 1,000 inferences. This represents a factor difference of approximately 1,450 times between the most and least energy-intensive tasks."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","In the context of Yelp sentiment analysis, the document explicitly states that traditional models perform considerably worse than large language models (LLMs). The research indicates that LLMs justify their energy costs by significantly outperforming traditional models in sentiment analysis tasks.","0","is_blank","[""zschache2025""]","is_blank","is_blank","In the context of Yelp sentiment analysis, the document explicitly states that traditional models perform considerably worse than large language models (LLMs). The research indicates that LLMs justify their energy costs by significantly outperforming traditional models in sentiment analysis tasks."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","According to the Amazon Renewable Energy Projects table in the context, Amazon had announced 244 renewable energy projects in the United States as of January 2024, with a total MW capacity of 17,706.","244","projects","[""amazon2023""]","is_blank","is_blank","According to the Amazon Renewable Energy Projects table in the context, Amazon had announced 244 renewable energy projects in the United States as of January 2024, with a total MW capacity of 17,706."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","According to the context from Amazon's 2023 Sustainability Report, the company had 22 AWS data center regions powered 100% by renewable energy sources in 2022, an increase from 19 regions in the previous year.","100","percent","[""amazon2023""]","is_blank","is_blank","According to the context from Amazon's 2023 Sustainability Report, the company had 22 AWS data center regions powered 100% by renewable energy sources in 2022, an increase from 19 regions in the previous year."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting energy consumption at the cumulative server level to capture total computation-related power usage and help providers optimize AI models and algorithms for energy efficiency.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The authors recommend reporting energy consumption at the cumulative server level to capture total computation-related power usage and help providers optimize AI models and algorithms for energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","In their experiment training a BERT-base model, the authors specifically measured the electricity consumption of different hardware components. According to Table 1, the GPU consumed 187.1 watts, which represented 74% of the total electricity consumption (251.6 watts).","74","percent","[""dodge2022""]","is_blank","is_blank","In their experiment training a BERT-base model, the authors specifically measured the electricity consumption of different hardware components. According to Table 1, the GPU consumed 187.1 watts, which represented 74% of the total electricity consumption (251.6 watts)."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","According to the Griggs et al. paper, serving the Llama2-70b model at BF16 precision specifically requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","According to the Griggs et al. paper, serving the Llama2-70b model at BF16 precision specifically requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","According to the context, Facebook achieved a 28.5% operational energy footprint reduction over a two-year period through iterative hardware-software optimization. This reduction was achieved through optimization across model, platform, infrastructure, and hardware domains, with an average of 20% reduction in operational power consumption every six months.","28.5","percent","[""wu2021a""]","is_blank","is_blank","According to the context, Facebook achieved a 28.5% operational energy footprint reduction over a two-year period through iterative hardware-software optimization. This reduction was achieved through optimization across model, platform, infrastructure, and hardware domains, with an average of 20% reduction in operational power consumption every six months."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context from the Li 2025b paper provides a global projection for AI water withdrawal in 2027. Specifically, the paper estimates that global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in that year.","4.2 - 6.6","kilometers of fiberoptic cable","[""li2025b""]","is_blank","is_blank","The context from the Li 2025b paper provides a global projection for AI water withdrawal in 2027. Specifically, the paper estimates that global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in that year."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","According to the context, the estimated hardware acquisition cost to train GPT-4 is $800M, which is significantly higher than its amortized hardware and energy cost of $40M. This cost represents the up-front expense of purchasing all necessary hardware for training.","800","USD","[""cottier2024""]","is_blank","is_blank","According to the context, the estimated hardware acquisition cost to train GPT-4 is $800M, which is significantly higher than its amortized hardware and energy cost of $40M. This cost represents the up-front expense of purchasing all necessary hardware for training."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context indicates that 'Sustainable AI' was not proposed to only encompass climate-positive applications, but rather to include both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context indicates that 'Sustainable AI' was not proposed to only encompass climate-positive applications, but rather to include both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The paper provides specific details about the maximum batch size for a Mixtral model on an NVIDIA A100-40GB GPU. Figure 13 shows the projected maximum batch size for different GPU configurations, and the text directly indicates the ground truth batch size.","28","samples","[""xia2024""]","is_blank","is_blank","The paper provides specific details about the maximum batch size for a Mixtral model on an NVIDIA A100-40GB GPU. Figure 13 shows the projected maximum batch size for different GPU configurations, and the text directly indicates the ground truth batch size."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","According to the context, when an LLM inference server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Swapping copies data without running additional computation, while Recomputation performs extra computation when restoring requests.","1","is_blank","[""chung2025""]","is_blank","is_blank","According to the context, when an LLM inference server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Swapping copies data without running additional computation, while Recomputation performs extra computation when restoring requests."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to the McKinsey projections cited in the context from the Fernandez 2025 paper, data centers are anticipated to consume between 9.1% and 11.7% of total US energy demand by 2030.","9.1-11.7","percent","[""fernandez2025""]","is_blank","is_blank","According to the McKinsey projections cited in the context from the Fernandez 2025 paper, data centers are anticipated to consume between 9.1% and 11.7% of total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","According to Figure 4 in the context, the Pause and Resume optimization for the 6B parameter Transformer shows a maximum potential emissions saving of approximately 25% when increasing the job duration.","25","percent","[""dodge2022""]","is_blank","is_blank","According to Figure 4 in the context, the Pause and Resume optimization for the 6B parameter Transformer shows a maximum potential emissions saving of approximately 25% when increasing the job duration."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","According to multiple references from the JetMoE paper, the JetMoE-8B architecture reduces inference computation by approximately 70% compared to the Llama2-7B model by using sparse activation in both attention and feed-forward layers, with only 2B parameters being activated for each input token out of its total 8B parameters.","70","percent","[""shen2024""]","is_blank","is_blank","According to multiple references from the JetMoE paper, the JetMoE-8B architecture reduces inference computation by approximately 70% compared to the Llama2-7B model by using sparse activation in both attention and feed-forward layers, with only 2B parameters being activated for each input token out of its total 8B parameters."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT, a large language model, as 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT, a large language model, as 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","Multiple context sources consistently report that GPT-3's estimated training energy consumption was 1,287 MWh. This figure is specifically cited in the context by Li et al. and confirmed by Jegham et al.'s research.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","Multiple context sources consistently report that GPT-3's estimated training energy consumption was 1,287 MWh. This figure is specifically cited in the context by Li et al. and confirmed by Jegham et al.'s research."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context indicates that the Mixture of Experts (MoE) layer is often a prime target for optimization during LLM performance enhancement. Multiple sources specifically highlight the MoE layer as the most time-consuming component, accounting for 85% of overall execution time and contributing significantly to computational complexity.","1","is_blank","[""xia2024"", ""chen2024""]","is_blank","is_blank","The context indicates that the Mixture of Experts (MoE) layer is often a prime target for optimization during LLM performance enhancement. Multiple sources specifically highlight the MoE layer as the most time-consuming component, accounting for 85% of overall execution time and contributing significantly to computational complexity."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides details about the Mixtral fine-tuning experiment on an NVIDIA A40 GPU with 48GB memory. The authors performed fine-tuning with a maximum batch size dependent on GPU memory, but do not explicitly state the precise maximum batch size for a dense Mixtral setup on the Hellaswag dataset.","is_blank","samples","[""xia2024""]","is_blank","is_blank","The context provides details about the Mixtral fine-tuning experiment on an NVIDIA A40 GPU with 48GB memory. The authors performed fine-tuning with a maximum batch size dependent on GPU memory, but do not explicitly state the precise maximum batch size for a dense Mixtral setup on the Hellaswag dataset."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context specifically states that for high granularity tasks like computer vision (CV), distributing VMs over four continents only slows down performance by 7% compared to local training. This is directly referenced in the paper multiple times.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context specifically states that for high granularity tasks like computer vision (CV), distributing VMs over four continents only slows down performance by 7% compared to local training. This is directly referenced in the paper multiple times."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","Based on the provided context, there is no specific information about the exact number of hectares occupied by new AI data centers globally in 2022. While the context discusses data center growth and AI expansion, no precise land area measurement is provided.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","Based on the provided context, there is no specific information about the exact number of hectares occupied by new AI data centers globally in 2022. While the context discusses data center growth and AI expansion, no precise land area measurement is provided."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","According to the document, the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","According to the document, the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The research paper provides specific details about the CO2 emissions range for training a BERT model across different regions. According to Figure 1 and the associated text, there is a significant variation between the least carbon-intensive and most carbon-intensive regions, with emissions ranging from 7,000 to 26,000 grams.","19000","grams","[""dodge2022""]","is_blank","is_blank","The research paper provides specific details about the CO2 emissions range for training a BERT model across different regions. According to Figure 1 and the associated text, there is a significant variation between the least carbon-intensive and most carbon-intensive regions, with emissions ranging from 7,000 to 26,000 grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information for notable AI models peaked in 2022, with 10% of models that year releasing some degree of environmental information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to reduced transparency.","2022","year","[""luccioni2025c""]","is_blank","is_blank","According to the context, the direct release of environmental information for notable AI models peaked in 2022, with 10% of models that year releasing some degree of environmental information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to reduced transparency."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","Based on the context for an OLMo 60M model trained on 1.7 to 5.6 trillion tokens, the equivalent water usage is 5 days for one person in the US, according to the water consumption data in Table 2 of the Morrison et al. 2025 paper.","5","days","[""morrison2025""]","is_blank","is_blank","Based on the context for an OLMo 60M model trained on 1.7 to 5.6 trillion tokens, the equivalent water usage is 5 days for one person in the US, according to the water consumption data in Table 2 of the Morrison et al. 2025 paper."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","According to the document, one technology company's self-owned data centers directly consumed more than 23 billion liters of freshwater for on-site cooling in 2023, with nearly 80% being potable water.","23","liters of freshwater","[""li2025b""]","is_blank","is_blank","According to the document, one technology company's self-owned data centers directly consumed more than 23 billion liters of freshwater for on-site cooling in 2023, with nearly 80% being potable water."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context from Xia et al. indicates that adding compute resources to accelerate the MoE layers can indeed increase costs. Specifically, the paper discusses that fine-tuning LLMs is cost-intensive, and while adding compute resources can improve performance, this comes with additional expenses.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context from Xia et al. indicates that adding compute resources to accelerate the MoE layers can indeed increase costs. Specifically, the paper discusses that fine-tuning LLMs is cost-intensive, and while adding compute resources can improve performance, this comes with additional expenses."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","Based on the carbon footprint analysis in the context, the total energy consumption for training FLM-101B is 40 MkWh (Megawatt-hours). This is found in Table 3, which summarizes the carbon emissions and energy consumption statistics for various large language models.","40","kWh","[""li2025a""]","is_blank","is_blank","Based on the carbon footprint analysis in the context, the total energy consumption for training FLM-101B is 40 MkWh (Megawatt-hours). This is found in Table 3, which summarizes the carbon emissions and energy consumption statistics for various large language models."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to the study's projections, the total public health impact of U.S. data centers is estimated to reach between $11.7 billion and $20.9 billion in 2028, with the high-growth scenario potentially reaching more than $20 billion.","20.9","USD","[""han2024""]","is_blank","is_blank","According to the study's projections, the total public health impact of U.S. data centers is estimated to reach between $11.7 billion and $20.9 billion in 2028, with the high-growth scenario potentially reaching more than $20 billion."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","Based on Table IV in the context, the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset using an NVIDIA A40-48GB GPU is $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","Based on Table IV in the context, the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset using an NVIDIA A40-48GB GPU is $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides details about the maximum batch size for a Mixtral model fine-tuned on an NVIDIA A40-48 GB GPU. From Table III in the paper, the maximum batch size for sparse Mixtral on the MATH dataset is 3 samples.","3","samples","[""xia2024""]","is_blank","is_blank","The context provides details about the maximum batch size for a Mixtral model fine-tuned on an NVIDIA A40-48 GB GPU. From Table III in the paper, the maximum batch size for sparse Mixtral on the MATH dataset is 3 samples."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","According to Table IV in the paper, for a Mixtral model fine-tuned on the MATH dataset using a sparse setup with an NVIDIA A40-48GB GPU with a batch size of 10, the execution time can be estimated. From the provided context, the total execution time is related to the throughput and GPU characteristics.","32.7","seconds","[""xia2024""]","is_blank","is_blank","According to Table IV in the paper, for a Mixtral model fine-tuned on the MATH dataset using a sparse setup with an NVIDIA A40-48GB GPU with a batch size of 10, the execution time can be estimated. From the provided context, the total execution time is related to the throughput and GPU characteristics."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly advocates against using GPU-level power consumption tracking for overall energy measurements. The authors argue that despite GPU power consumption being a significant factor that correlates with total power usage, it substantially underrepresents the actual energy consumption since it only measures a single component.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly advocates against using GPU-level power consumption tracking for overall energy measurements. The authors argue that despite GPU power consumption being a significant factor that correlates with total power usage, it substantially underrepresents the actual energy consumption since it only measures a single component."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","According to the context from Luccioni et al.'s 2023 article on BLOOM's carbon footprint, training accounted for only half of the model's overall emissions, meaning training was not the entire carbon footprint as previously assumed in other studies.","50","percent","[""luccioni2025b""]","is_blank","is_blank","According to the context from Luccioni et al.'s 2023 article on BLOOM's carbon footprint, training accounted for only half of the model's overall emissions, meaning training was not the entire carbon footprint as previously assumed in other studies."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context from the Samsi et al. paper explicitly states that 8 V100 GPUs with 32GB RAM or 4 A100 GPUs with 80GB memory are the minimum hardware requirements for running meaningful inference on the 65B LLaMA model without compression or quantization.","8","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context from the Samsi et al. paper explicitly states that 8 V100 GPUs with 32GB RAM or 4 A100 GPUs with 80GB memory are the minimum hardware requirements for running meaningful inference on the 65B LLaMA model without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to the context, GPT-3 needs to 'consume' a 500ml bottle of water for approximately 10-50 medium-length responses. Specifically, the data varies by location, but for a data center in Arizona, it would take about 16.7 requests to consume a 500ml bottle of water.","16.7","requests","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs to 'consume' a 500ml bottle of water for approximately 10-50 medium-length responses. Specifically, the data varies by location, but for a data center in Arizona, it would take about 16.7 requests to consume a 500ml bottle of water."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","According to the context from the Dodge 2022 research paper, the estimated CO2 emission for average US home energy use is 8.3 metric tons per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","According to the context from the Dodge 2022 research paper, the estimated CO2 emission for average US home energy use is 8.3 metric tons per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","Unable to find specific information about JetMoE-8B's performance on the GSM8k benchmark in the provided context. While the document discusses the model's performance on various metrics, no specific GSM8k score is mentioned.","is_blank","score","[""is_blank""]","is_blank","is_blank","Unable to find specific information about JetMoE-8B's performance on the GSM8k benchmark in the provided context. While the document discusses the model's performance on various metrics, no specific GSM8k score is mentioned."
