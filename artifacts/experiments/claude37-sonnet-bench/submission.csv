"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context clearly identifies the name of the benchmark suite presented in the paper as the ML.ENERGY Benchmark. Multiple references throughout the document describe it as 'a benchmark suite and tool for measuring inference energy consumption under realistic service environments' along with the corresponding ML.ENERGY Leaderboard.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context clearly identifies the name of the benchmark suite presented in the paper as the ML.ENERGY Benchmark. Multiple references throughout the document describe it as 'a benchmark suite and tool for measuring inference energy consumption under realistic service environments' along with the corresponding ML.ENERGY Leaderboard."
"q009","What were the net CO2e emissions from training the GShard-600B model?","Figure 4 in the Wu2021a reference clearly shows the CO2e emissions for training the GShard-600B model. The figure displays emission data for various models, with GShard-600B specifically marked as having emissions of around 450,000 kg CO2e (or 450 tCO2e) for training only.","450","tCO2e","[""wu2021a""]","is_blank","is_blank","Figure 4 in the Wu2021a reference clearly shows the CO2e emissions for training the GShard-600B model. The figure displays emission data for various models, with GShard-600B specifically marked as having emissions of around 450,000 kg CO2e (or 450 tCO2e) for training only."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","According to the context, specifically from ref_id=chen2024, Table 3 explicitly lists LLaMA-33B as having 64.7 GB of parameters.","64.7","GB","[""chen2024""]","is_blank","is_blank","According to the context, specifically from ref_id=chen2024, Table 3 explicitly lists LLaMA-33B as having 64.7 GB of parameters."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context provides information about electricity consumption for various AI models and data centers, but it does not specify the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The documents discuss electricity consumption for training models like BERT, GPT-3, and Llama-3.1, as well as data center electricity consumption by state, but none mention Google Cloud TPU pods' total electricity consumption for 2023.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context provides information about electricity consumption for various AI models and data centers, but it does not specify the total electricity consumption of all Google Cloud TPU pods worldwide in 2023. The documents discuss electricity consumption for training models like BERT, GPT-3, and Llama-3.1, as well as data center electricity consumption by state, but none mention Google Cloud TPU pods' total electricity consumption for 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","According to wu2021b, 'between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1).' This directly confirms that hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020.","1","is_blank","[""wu2021b""]","is_blank","is_blank","According to wu2021b, 'between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1).' This directly confirms that hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","According to the context, GPT-3 needs to consume a 500ml bottle of water for roughly 10-50 medium-length responses, depending on when and where it is deployed.","1/10 to 1/50","500 mL bottles","[""li2025b""]","is_blank","is_blank","According to the context, GPT-3 needs to consume a 500ml bottle of water for roughly 10-50 medium-length responses, depending on when and where it is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","According to the context, 75% of CVPR papers target accuracy while only 20% of CVPR papers argue for efficiency. The difference between these percentages is 75% - 20% = 55%.","55","percent","[""schwartz2019""]","is_blank","is_blank","According to the context, 75% of CVPR papers target accuracy while only 20% of CVPR papers argue for efficiency. The difference between these percentages is 75% - 20% = 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context clearly states that energy consumption data is ""restricted to authorities and is not accessible to downstream providers or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1)"" of the AI Act. This is identified as a limitation that ""reduces transparency and accountability.""","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that energy consumption data is ""restricted to authorities and is not accessible to downstream providers or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1)"" of the AI Act. This is identified as a limitation that ""reduces transparency and accountability."""
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","According to Figure 13 and supporting text in the document, for a projected GPU capacity of 100GB, the maximum batch size supported for fine-tuning Mixtral will be 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","According to Figure 13 and supporting text in the document, for a projected GPU capacity of 100GB, the maximum batch size supported for fine-tuning Mixtral will be 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, specifically from document samsi2024, for LLaMA-7B model, the A100 GPUs provided a 2 times increase in inference throughput compared to V100 GPUs across words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","is_blank","According to the context, specifically from document samsi2024, for LLaMA-7B model, the A100 GPUs provided a 2 times increase in inference throughput compared to V100 GPUs across words per second, tokens per second, and responses per second."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides specific data on GPT-3's water consumption during training in U.S. data centers. According to Table 1, the total operational water consumption for training GPT-3 in U.S. Average conditions is 5.439 million liters, which includes both on-site water (0.708 million L) and off-site water (4.731 million L).","5.439","liters","[""li2025b""]","is_blank","is_blank","The context provides specific data on GPT-3's water consumption during training in U.S. data centers. According to Table 1, the total operational water consumption for training GPT-3 in U.S. Average conditions is 5.439 million liters, which includes both on-site water (0.708 million L) and off-site water (4.731 million L)."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context from document ebert2024 explicitly states that sustainability impact assessments 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' The authors justify this by explaining that 'the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.'","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context from document ebert2024 explicitly states that sustainability impact assessments 'should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.' The authors justify this by explaining that 'the carbon footprint of AI models is often unrelated to their classification as high or low risk under the Act.'"
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","According to the 2023 Amazon Sustainability Report, AWS improved its water use effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, representing a 5% improvement from 0.19 L/kWh in 2022 and a 28% improvement since 2021.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","According to the 2023 Amazon Sustainability Report, AWS improved its water use effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, representing a 5% improvement from 0.19 L/kWh in 2022 and a 28% improvement since 2021."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","According to the context, local inference is emphasized as a sustainability measure because it 'allows models to run directly on user devices while maintaining data privacy' and 'By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10]'. This explicitly states that local inference reduces both network overhead and carbon footprint.","1","is_blank","[""khan2025""]","is_blank","is_blank","According to the context, local inference is emphasized as a sustainability measure because it 'allows models to run directly on user devices while maintaining data privacy' and 'By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10]'. This explicitly states that local inference reduces both network overhead and carbon footprint."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context clearly supports that tracking runtime of training jobs is important for cost estimation in GPU environments. Strubell et al. (2019) provide detailed tables showing how training time (hours) directly factors into cloud compute costs and electricity costs for various models. Additionally, Kim et al. (2025) discuss how tracking inference time is critical for selecting optimal GPU instances based on cost efficiency and Service Level Objectives.","1","is_blank","[""strubell2019"", ""kim2025""]","is_blank","is_blank","The context clearly supports that tracking runtime of training jobs is important for cost estimation in GPU environments. Strubell et al. (2019) provide detailed tables showing how training time (hours) directly factors into cloud compute costs and electricity costs for various models. Additionally, Kim et al. (2025) discuss how tracking inference time is critical for selecting optimal GPU instances based on cost efficiency and Service Level Objectives."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to Chen et al. 2024, the LLaMA-65B model experienced a significant improvement in performance with automated resource utilization overlapping, achieving up to a 13.2% reduction in latency (as measured by Time Between Tokens). This maximum improvement was observed for larger batch sizes, which produce larger KV tensors and result in greater latency reduction.","13.2","percent","[""chen2024""]","is_blank","is_blank","According to Chen et al. 2024, the LLaMA-65B model experienced a significant improvement in performance with automated resource utilization overlapping, achieving up to a 13.2% reduction in latency (as measured by Time Between Tokens). This maximum improvement was observed for larger batch sizes, which produce larger KV tensors and result in greater latency reduction."
"q164","How much does an elephant weigh?","The context does not provide specific information about how much an elephant weighs. While document amazon2023 mentions that wild Asian elephants live in the Western Ghats in India, it does not provide any weight information for elephants.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about how much an elephant weighs. While document amazon2023 mentions that wild Asian elephants live in the Western Ghats in India, it does not provide any weight information for elephants."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context does not provide information about energy consumption comparisons between the specific models mentioned in the question: Meena, T5, GPT-3, GShard-600B, or Switch Transformer. While the documents mention energy consumption for various models like Llama, DeepSeek, GPT-4 variants, T5, BERT, and others, there is no direct comparison of energy consumption between the five models specified in the question.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about energy consumption comparisons between the specific models mentioned in the question: Meena, T5, GPT-3, GShard-600B, or Switch Transformer. While the documents mention energy consumption for various models like Llama, DeepSeek, GPT-4 variants, T5, BERT, and others, there is no direct comparison of energy consumption between the five models specified in the question."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","According to Strubell et al. (2019), training the BERT base model (110M parameters) contributes approximately 626,155 pounds of CO₂ emissions. This number can be used to calculate the equivalent in days of an average American's CO₂ emissions. Since an average American life produces 36,156 pounds of CO₂ per year, we can convert this to daily emissions: 36,156/365 = 99.06 pounds per day. Therefore, training BERT base is equivalent to 626,155/99.06 = 6321 days of average American CO₂ emissions.","6321","days","[""strubell2019""]","is_blank","is_blank","According to Strubell et al. (2019), training the BERT base model (110M parameters) contributes approximately 626,155 pounds of CO₂ emissions. This number can be used to calculate the equivalent in days of an average American's CO₂ emissions. Since an average American life produces 36,156 pounds of CO₂ per year, we can convert this to daily emissions: 36,156/365 = 99.06 pounds per day. Therefore, training BERT base is equivalent to 626,155/99.06 = 6321 days of average American CO₂ emissions."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The provided context does not contain any information comparing the performance of Transformer architecture versus Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as model sizes grow. While there are references to both Transformer and Evolved Transformer architectures in the documents, there is no specific comparison of their performance on this particular task.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information comparing the performance of Transformer architecture versus Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as model sizes grow. While there are references to both Transformer and Evolved Transformer architectures in the documents, there is no specific comparison of their performance on this particular task."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","Based on the provided context, there is no specific mention of a dataset with exactly 5,842 labeled entries used for testing energy-efficient large language models in the financial domain. The documents discuss various datasets used for energy efficiency testing, but none match the specific description in the question.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","Based on the provided context, there is no specific mention of a dataset with exactly 5,842 labeled entries used for testing energy-efficient large language models in the financial domain. The documents discuss various datasets used for energy efficiency testing, but none match the specific description in the question."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","According to the context, the researchers found that using eight T4 spot instances for distributed training could be more cost-efficient than using a DGX-2 node for computer vision tasks. While this was not the case for NLP models with low granularity, the paper explicitly states that for CV (computer vision) models, their 8xT4 setup was 58% cheaper than a DGX-2 while being 37% slower, representing a favorable cost-efficiency tradeoff.","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the context, the researchers found that using eight T4 spot instances for distributed training could be more cost-efficient than using a DGX-2 node for computer vision tasks. While this was not the case for NLP models with low granularity, the paper explicitly states that for CV (computer vision) models, their 8xT4 setup was 58% cheaper than a DGX-2 while being 37% slower, representing a favorable cost-efficiency tradeoff."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","According to the context, specifically from document luccioni2025b, the 2023 US Executive Order regarding AI 'did not mention AI's greenhouse gas emissions nor energy usage,' showing that these environmental factors were not included in the order.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context, specifically from document luccioni2025b, the 2023 US Executive Order regarding AI 'did not mention AI's greenhouse gas emissions nor energy usage,' showing that these environmental factors were not included in the order."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","According to the context, Germany's 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing to 100% by January 1, 2027. This is specifically mentioned in section 4.3 where it states 'it sets targets on energy efficiency and renewable energy use, requiring data centers... to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11)'.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, Germany's 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing to 100% by January 1, 2027. This is specifically mentioned in section 4.3 where it states 'it sets targets on energy efficiency and renewable energy use, requiring data centers... to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11)'."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","According to the context, the authors sampled 60 papers from top AI conferences including ACL and categorized them based on whether they targeted accuracy, efficiency, both, or other. From Figure 2, we can determine that 0% of ACL papers targeted both accuracy and efficiency because the text states that 90% of ACL papers target accuracy, and only a small portion (10%) argue for a new efficiency result, with no mention of papers targeting both.","0","papers","[""schwartz2019""]","is_blank","is_blank","According to the context, the authors sampled 60 papers from top AI conferences including ACL and categorized them based on whether they targeted accuracy, efficiency, both, or other. From Figure 2, we can determine that 0% of ACL papers targeted both accuracy and efficiency because the text states that 90% of ACL papers target accuracy, and only a small portion (10%) argue for a new efficiency result, with no mention of papers targeting both."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","According to the document jegham2025, recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use. This is also supported by luccioni2024, which cites AWS estimating that inference makes up 80-90% of total ML cloud computing demand.","90","percent","[""jegham2025"", ""luccioni2024""]","is_blank","is_blank","According to the document jegham2025, recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use. This is also supported by luccioni2024, which cites AWS estimating that inference makes up 80-90% of total ML cloud computing demand."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act requires reporting of energy consumption for GPAI model training but has a significant gap as it omits reporting requirements for the inference phase. The paper specifically notes that 'the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act requires reporting of energy consumption for GPAI model training but has a significant gap as it omits reporting requirements for the inference phase. The paper specifically notes that 'the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'"
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","According to the context, the AI Act currently does not mandate the disclosure of energy consumption during the inference phase of AI models. This is explicitly stated as 'a crucial omission' and a 'gap' in the regulatory framework, with authors proposing that inference energy consumption should be included in reporting requirements.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context, the AI Act currently does not mandate the disclosure of energy consumption during the inference phase of AI models. This is explicitly stated as 'a crucial omission' and a 'gap' in the regulatory framework, with authors proposing that inference energy consumption should be included in reporting requirements."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context explicitly states that ""new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities."" This directly contradicts the statement that they rely on air cooling.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context explicitly states that ""new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities."" This directly contradicts the statement that they rely on air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to the paper by Wu et al. (2021), platform-level caching improved power efficiency by 6.7× for the cross-lingual Transformer language model. This is explicitly stated in the context where they analyze optimization benefits and mention that 'application-level caching improves power efficiency by 6.7×' for language translation tasks.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","According to the paper by Wu et al. (2021), platform-level caching improved power efficiency by 6.7× for the cross-lingual Transformer language model. This is explicitly stated in the context where they analyze optimization benefits and mention that 'application-level caching improves power efficiency by 6.7×' for language translation tasks."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","According to Strubell et al. (2019), training BERT base model using 64 V100 GPUs for 79 hours resulted in CO2 emissions of 1438 pounds. This information is explicitly stated in Table 3 of their paper, which shows the estimated cost of training various models in terms of CO2 emissions.","1438","lbs","[""strubell2019""]","is_blank","is_blank","According to Strubell et al. (2019), training BERT base model using 64 V100 GPUs for 79 hours resulted in CO2 emissions of 1438 pounds. This information is explicitly stated in Table 3 of their paper, which shows the estimated cost of training various models in terms of CO2 emissions."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","According to multiple sources in the context, ML inference reportedly accounts for 80-90% of total ML compute demand. This figure is specifically cited in ref_id=chung2025 and ref_id=luccioni2024, both referring to estimates from AWS, the largest global cloud provider.","80-90","percent","[""chung2025"", ""luccioni2024""]","is_blank","is_blank","According to multiple sources in the context, ML inference reportedly accounts for 80-90% of total ML compute demand. This figure is specifically cited in ref_id=chung2025 and ref_id=luccioni2024, both referring to estimates from AWS, the largest global cloud provider."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","According to Dodge2022, training a 6.1 billion parameter language model to completion would consume approximately 103,500 kWh of electricity. The context does not provide information on how many household-years of electricity consumption this is equivalent to.","is_blank","household-years","[""dodge2022""]","is_blank","is_blank","According to Dodge2022, training a 6.1 billion parameter language model to completion would consume approximately 103,500 kWh of electricity. The context does not provide information on how many household-years of electricity consumption this is equivalent to."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","According to the document, when discussing geo-distributed NLP experiments, it explicitly states that 'For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).' This clearly confirms that egress costs can account for more than 90% of the total cost per VM in geo-distributed NLP experiments.","1","is_blank","[""erben2023""]","is_blank","is_blank","According to the document, when discussing geo-distributed NLP experiments, it explicitly states that 'For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).' This clearly confirms that egress costs can account for more than 90% of the total cost per VM in geo-distributed NLP experiments."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours, and the training was conducted on a cluster containing 12 nodes and 96 H100s. To calculate the wall-clock time, I need to divide the total GPU hours by the number of GPUs used: 30,000 / 96 ≈ 312.5 hours, which is approximately 13.02 days.","13.02","days","[""shen2024""]","is_blank","is_blank","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours, and the training was conducted on a cluster containing 12 nodes and 96 H100s. To calculate the wall-clock time, I need to divide the total GPU hours by the number of GPUs used: 30,000 / 96 ≈ 312.5 hours, which is approximately 13.02 days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","According to the context, water consumption is defined as 'water withdrawal minus water discharge', representing the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","According to the context, water consumption is defined as 'water withdrawal minus water discharge', representing the amount of water 'evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment'."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","According to the context, the energy per second for inference with LLaMA-65B ranges from 300 Watts to 1 Kilowatt depending on the GPU shard configuration, with lower energy consumption (300W) for the 8 GPU shard configuration and higher energy consumption (1KW) for the 32 GPU configuration.","300W","W","[""samsi2024""]","is_blank","is_blank","According to the context, the energy per second for inference with LLaMA-65B ranges from 300 Watts to 1 Kilowatt depending on the GPU shard configuration, with lower energy consumption (300W) for the 8 GPU shard configuration and higher energy consumption (1KW) for the 32 GPU configuration."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","According to the context, specifically from ref_id=zschache2025, when comparing Qwen models for zero-shot classification, the Qwen 2.5 72B model consumed seven times more energy than the Qwen 2.5 7B model, with only a minor accuracy reduction of 0.07 points.","7","multiplier","[""zschache2025""]","is_blank","is_blank","According to the context, specifically from ref_id=zschache2025, when comparing Qwen models for zero-shot classification, the Qwen 2.5 72B model consumed seven times more energy than the Qwen 2.5 7B model, with only a minor accuracy reduction of 0.07 points."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","According to Table III in the context, Qwen's carbon emissions decreased from 0.009 kg to 0.004 kg after optimization with quantization and local inference techniques. This represents a reduction of approximately 55.6%.","55.6","percent","[""khan2025""]","is_blank","is_blank","According to Table III in the context, Qwen's carbon emissions decreased from 0.009 kg to 0.004 kg after optimization with quantization and local inference techniques. This represents a reduction of approximately 55.6%."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","According to the context, the latest iteration (early 2025) of the ML.ENERGY Benchmark included 40 widely used model architectures across 6 different tasks. This information is explicitly stated in the abstract of the paper.","40","models","[""chung2025""]","is_blank","is_blank","According to the context, the latest iteration (early 2025) of the ML.ENERGY Benchmark included 40 widely used model architectures across 6 different tasks. This information is explicitly stated in the abstract of the paper."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","According to the provided context, when training an AI model of the Llama-3.1 scale, the health cost varies widely depending on location. The text specifically states that the total health cost in Iowa would be $2.5 million due to factors such as wind direction and pollutant emission rates for electricity generation.","2500000","USD","[""han2024""]","is_blank","is_blank","According to the provided context, when training an AI model of the Llama-3.1 scale, the health cost varies widely depending on location. The text specifically states that the total health cost in Iowa would be $2.5 million due to factors such as wind direction and pollutant emission rates for electricity generation."
