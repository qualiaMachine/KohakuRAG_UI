"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context clearly mentions the 'ML.ENERGY Benchmark' as a benchmark suite and tool for measuring inference energy consumption under realistic service environments.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context clearly mentions the 'ML.ENERGY Benchmark' as a benchmark suite and tool for measuring inference energy consumption under realistic service environments."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context provides a figure (Figure 4) that illustrates the operational carbon emissions for model training and inference across various ML tasks, including the GShard-600B model. The figure shows the carbon footprint in CO2e (kg) for different models, and it is mentioned that the average carbon footprint for ML training tasks at Facebook is 0.3 times that of GPT-3's carbon footprint.","0.55","tCO2e","[""wu2021a""]","is_blank","is_blank","The context provides a figure (Figure 4) that illustrates the operational carbon emissions for model training and inference across various ML tasks, including the GShard-600B model. The figure shows the carbon footprint in CO2e (kg) for different models, and it is mentioned that the average carbon footprint for ML training tasks at Facebook is 0.3 times that of GPT-3's carbon footprint."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides a table (Table 3) that lists the model size in gigabytes (GB) for various LLaMA models, including LLaMA-33B, which is reported to be 64.7 GB.","64.7","GB","[""chen2024""]","is_blank","is_blank","The context provides a table (Table 3) that lists the model size in gigabytes (GB) for various LLaMA models, including LLaMA-33B, which is reported to be 64.7 GB."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","is_blank","is_blank","MWh","[""is_blank""]","is_blank","is_blank","is_blank"
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context provides a comparison between the efficiency of hyperscale data centers and traditional data centers in 2020, stating that hyperscale data centers achieved more than 40% higher efficiency.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context provides a comparison between the efficiency of hyperscale data centers and traditional data centers in 2020, stating that hyperscale data centers achieved more than 40% higher efficiency."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context provides information about GPT-3's water consumption, stating that it needs to 'drink' a 500ml bottle of water for roughly 10 – 50 medium-length responses.","30","500 mL bottles","[""li2025b""]","is_blank","is_blank","The context provides information about GPT-3's water consumption, stating that it needs to 'drink' a 500ml bottle of water for roughly 10 – 50 medium-length responses."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context provides the percentage of CVPR papers that target accuracy (75%) and the percentage that target efficiency (20%). The difference between these two percentages is 55%.","55","percent","[""schwartz2019""]","is_blank","is_blank","The context provides the percentage of CVPR papers that target accuracy (75%) and the percentage that target efficiency (20%). The difference between these two percentages is 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context indicates that the AI Act does not make energy consumption data from providers publicly available to NGOs, analysts, and the general public. It is restricted to authorities due to confidentiality clauses.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not make energy consumption data from providers publicly available to NGOs, analysts, and the general public. It is restricted to authorities due to confidentiality clauses."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context provides a figure (Fig. 13) and text that describe the projected maximum batch size for fine-tuning a Mixtral model on GPUs with different memory capacities. According to the text and figure, for a GPU memory capacity of 100GB, the projected maximum batch size is 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","The context provides a figure (Fig. 13) and text that describe the projected maximum batch size for fine-tuning a Mixtral model on GPUs with different memory capacities. According to the text and figure, for a GPU memory capacity of 100GB, the projected maximum batch size is 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides a comparison of inference performance between LLaMA 7B, 13B, and 65B models on V100 and A100 GPUs. It states that for LLaMA 7B, there is a 2 times increase in inference throughput on A100 compared to V100 GPUs.","2","multiplier","[""samsi2024""]","is_blank","is_blank","The context provides a comparison of inference performance between LLaMA 7B, 13B, and 65B models on V100 and A100 GPUs. It states that for LLaMA 7B, there is a 2 times increase in inference throughput on A100 compared to V100 GPUs."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides a table (Table 1) that estimates GPT-3's operational water consumption footprint for various Microsoft data center locations. The 'U.S. Average' row gives the total water consumption for training GPT-3 as 16.904 million liters.","16.904","liters","[""li2025b""]","is_blank","is_blank","The context provides a table (Table 1) that estimates GPT-3's operational water consumption footprint for various Microsoft data center locations. The 'U.S. Average' row gives the total water consumption for training GPT-3 as 16.904 million liters."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The authors explicitly state that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The authors explicitly state that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context provides the water use effectiveness (WUE) for AWS data centers as of 2023, stating it was 0.18 L/kWh.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context provides the water use effectiveness (WUE) for AWS data centers as of 2023, stating it was 0.18 L/kWh."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context clearly supports the statement that local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models. The provided text states that local inference 'significantly reduces both network overhead and carbon footprint' by minimizing data transmission between clients and remote servers.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context clearly supports the statement that local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models. The provided text states that local inference 'significantly reduces both network overhead and carbon footprint' by minimizing data transmission between clients and remote servers."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context supports the answer by discussing the importance of training time in estimating carbon emissions and compute cost. According to [ref_id=luccioni2023], training time is a crucial factor in calculating CO2 emissions, and [ref_id=strubell2019] emphasizes the need for authors to report training time and computational resources.","1","is_blank","[""luccioni2023"", ""strubell2019""]","is_blank","is_blank","The context supports the answer by discussing the importance of training time in estimating carbon emissions and compute cost. According to [ref_id=luccioni2023], training time is a crucial factor in calculating CO2 emissions, and [ref_id=strubell2019] emphasizes the need for authors to report training time and computational resources."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context provides information about the performance improvement achieved by enabling the automated resource utilization overlapping feature in the LLaMA-65B model. According to the text, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% improvement through automated resource utilization overlapping.","13.2","percent","[""chen2024""]","is_blank","is_blank","The context provides information about the performance improvement achieved by enabling the automated resource utilization overlapping feature in the LLaMA-65B model. According to the text, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% improvement through automated resource utilization overlapping."
"q164","How much does an elephant weigh?","is_blank","is_blank","lbs","[""is_blank""]","is_blank","is_blank","is_blank"
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides a table (Table 4) that lists the energy consumption of various models, including some of the models mentioned in the question. DeepSeek-R1 (DS) has the highest energy consumption at 29.078 Wh for the longest prompt size.","DeepSeek-R1 (DS)","is_blank","[""jegham2025""]","is_blank","is_blank","The context provides a table (Table 4) that lists the energy consumption of various models, including some of the models mentioned in the question. DeepSeek-R1 (DS) has the highest energy consumption at 29.078 Wh for the longest prompt size."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context provides the CO₂ emissions from training BERT base as 626,155 pounds and the average American life CO₂ emissions as 36,156 pounds per year. To find the equivalent days, we need to calculate the daily CO₂ emissions of an average American and then divide the CO₂ emissions from training BERT base by this daily amount.","is_blank","days","[""strubell2019"", ""luccioni2025b""]","is_blank","is_blank","The context provides the CO₂ emissions from training BERT base as 626,155 pounds and the average American life CO₂ emissions as 36,156 pounds per year. To find the equivalent days, we need to calculate the daily CO₂ emissions of an average American and then divide the CO₂ emissions from training BERT base by this daily amount."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not mention a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not mention a dataset of 5,842 labeled entries used to test energy-efficient large language models in the financial domain."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context states that 'By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.' This directly supports the statement that eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that 'By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.' This directly supports the statement that eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context clearly states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, supporting the answer '0' for False.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context clearly states that the 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage, supporting the answer '0' for False."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context clearly states that the German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing to 100% by January 1, 2027 (Sec. 11).","1","is_blank","[""ebert2024""]","is_blank","is_blank","The context clearly states that the German 2023 Energy Efficiency Act requires data centers to run on 50% renewable energy, increasing to 100% by January 1, 2027 (Sec. 11)."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context provides information about a sample of 60 papers from top AI conferences (ACL, NeurIPS, and CVPR). For ACL papers, 90% targeted accuracy, and 10% argued for a new efficiency result. The context does not directly state how many ACL papers targeted both accuracy and efficiency.","is_blank","papers","[""schwartz2019""]","is_blank","is_blank","The context provides information about a sample of 60 papers from top AI conferences (ACL, NeurIPS, and CVPR). For ACL papers, 90% targeted accuracy, and 10% argued for a new efficiency result. The context does not directly state how many ACL papers targeted both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context snippets provide various estimates and discussions about the percentage of a model's total lifecycle energy use that inference can account for. Multiple sources, including 'jegham2025' and 'luccioni2024', mention that recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use.","90","percent","[""jegham2025"", ""luccioni2024""]","is_blank","is_blank","The context snippets provide various estimates and discussions about the percentage of a model's total lifecycle energy use that inference can account for. Multiple sources, including 'jegham2025' and 'luccioni2024', mention that recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act does not explicitly mandate reporting of both training and inference energy consumption for general-purpose AI models. However, the authors propose an interpretation to include inference energy consumption in the reporting obligations.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not explicitly mandate reporting of both training and inference energy consumption for general-purpose AI models. However, the authors propose an interpretation to include inference energy consumption in the reporting obligations."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context indicates that the AI Act does not currently mandate the disclosure of energy consumption during the inference phase of AI models. The authors propose an interpretation to bring reporting on energy consumption from AI inferences back into the scope, suggesting it is not currently required.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does not currently mandate the disclosure of energy consumption during the inference phase of AI models. The authors propose an interpretation to bring reporting on energy consumption from AI inferences back into the scope, suggesting it is not currently required."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context clearly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities, contradicting the statement that they rely on air cooling.","0","is_blank","[""[\""li2025b\""]""]","is_blank","is_blank","The context clearly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities, contradicting the statement that they rely on air cooling."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context provides a detailed analysis of the optimizations applied to the cross-lingual Transformer language model described in Wu et al. (2021). Specifically, it mentions that platform-level caching improves power efficiency by 6.7×.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","The context provides a detailed analysis of the optimizations applied to the cross-lingual Transformer language model described in Wu et al. (2021). Specifically, it mentions that platform-level caching improves power efficiency by 6.7×."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a table (Table 3) that lists the estimated CO2 emissions from training various models, including BERT base on 64 V100 GPUs for 79 hours. The CO2 emissions are given as 1438 lbs.","1438","lbs","[""strubell2019""]","is_blank","is_blank","The context provides a table (Table 3) that lists the estimated CO2 emissions from training various models, including BERT base on 64 V100 GPUs for 79 hours. The CO2 emissions are given as 1438 lbs."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context snippets from multiple references (chung2025, luccioni2024, fernandez2025) consistently report that ML inference accounts for 80-90% of total ML cloud computing demand according to AWS, and similar percentages are attributed by other companies like Google and Meta, though with some variation.","85","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","The context snippets from multiple references (chung2025, luccioni2024, fernandez2025) consistently report that ML inference accounts for 80-90% of total ML cloud computing demand according to AWS, and similar percentages are attributed by other companies like Google and Meta, though with some variation."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context provides information about the energy consumption of training a 6.1 billion parameter language model. According to the document [ref_id=dodge2022], training this model consumed 13.8 MWh of electricity, and a full training run would consume approximately 103,500 kWh. However, it does not directly provide the equivalent U.S. household-years of electricity consumption.","is_blank","household-years","[""is_blank""]","is_blank","is_blank","The context provides information about the energy consumption of training a 6.1 billion parameter language model. According to the document [ref_id=dodge2022], training this model consumed 13.8 MWh of electricity, and a full training run would consume approximately 103,500 kWh. However, it does not directly provide the equivalent U.S. household-years of electricity consumption."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context provides evidence that egress costs in geo-distributed NLP experiments can account for more than 90% of the total cost per VM, specifically mentioning that for NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).","1","is_blank","[""[\""erben2023\""]""]","is_blank","is_blank","The context provides evidence that egress costs in geo-distributed NLP experiments can account for more than 90% of the total cost per VM, specifically mentioning that for NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h)."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The total pre-training GPU hours for JetMoE-8B is 30,000 hours, and it was trained on 96 H100 GPUs. By dividing the total GPU hours by the number of GPUs, we get the total wall-clock time in hours, which is then converted to days.","13.02","days","[""shen2024""]","is_blank","is_blank","The total pre-training GPU hours for JetMoE-8B is 30,000 hours, and it was trained on 96 H100 GPUs. By dividing the total GPU hours by the number of GPUs, we get the total wall-clock time in hours, which is then converted to days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The term 'water consumption' is defined as 'water withdrawal minus water discharge', and it refers to the amount of water evaporated, transpired, or incorporated into products. This definition is provided in the context snippets.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The term 'water consumption' is defined as 'water withdrawal minus water discharge', and it refers to the amount of water evaporated, transpired, or incorporated into products. This definition is provided in the context snippets."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context provides a clear statement about the observed range of inference energy per second for LLaMA-65B across GPU shard configurations. It is mentioned that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","300-1000","W","[""samsi2024""]","is_blank","is_blank","The context provides a clear statement about the observed range of inference energy per second for LLaMA-65B across GPU shard configurations. It is mentioned that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context snippet from [ref_id=zschache2025] states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This directly answers the question about the energy consumption comparison between the 7B and 72B versions of Qwen models.","7","multiplier","[""zschache2025""]","is_blank","is_blank","The context snippet from [ref_id=zschache2025] states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This directly answers the question about the energy consumption comparison between the 7B and 72B versions of Qwen models."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context provides a table (TABLE III) comparing the carbon emissions of various LLMs before and after optimization. For Qwen, the CO2 emissions decreased from 0.009 kg to 0.004 kg after applying quantization and local inference.","55.56","percent","[""khan2025""]","is_blank","is_blank","The context provides a table (TABLE III) comparing the carbon emissions of various LLMs before and after optimization. For Qwen, the CO2 emissions decreased from 0.009 kg to 0.004 kg after applying quantization and local inference."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration.","40","models","[""chung2025""]","is_blank","is_blank","The context mentions that the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context provides a table (Table 2) that lists the estimated health costs for training a Llama-3.1 scale model in different locations. According to the table, the total health cost in Iowa is $2.5 million.","2.5","USD","[""han2024""]","is_blank","is_blank","The context provides a table (Table 2) that lists the estimated health costs for training a Llama-3.1 scale model in different locations. According to the table, the total health cost in Iowa is $2.5 million."
