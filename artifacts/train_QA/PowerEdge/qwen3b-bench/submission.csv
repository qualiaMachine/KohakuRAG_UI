"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context mentions that the ML.ENERGY Benchmark is presented in this paper, which measures inference energy consumption.","is_blank","is_blank","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","In this paper, we share the design principles we have established over time (Section 2) and present the ML.ENERGY Benchmark that embodies them (Section 3).","The context mentions that the ML.ENERGY Benchmark is presented in this paper, which measures inference energy consumption."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context provides the specific CO2e emissions (4.3 tCO2e) for training the GShard-600B model, which directly answers the question.","4.3","tCO2e","[""patterson2021""]","[""https://arxiv.org/pdf/2006.16668.pdf""]","GShard-600B’s emissions (Table 4) are 4.3 tCO2e —3.5 passenger SF-NY round trips—from consuming 24 MWh to train the model that could have 2B users;","The context provides the specific CO2e emissions (4.3 tCO2e) for training the GShard-600B model, which directly answers the question."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context provides the exact model size in gigabytes (GB) for the LLaMA-33B model.","64.7","GB","[""chen2024""]","[""https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md""]","[ref_id=chen2024] Model Parameters L d G
LLaMA-33B 64.7 GB 60 6656 1","The context provides the exact model size in gigabytes (GB) for the LLaMA-33B model."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","In 2019, Google's total energy consumption was 12.2 TeraWatt-hours. We need to find the total electricity consumption of all Google Cloud TPU pods worldwide in 2023.","12.2","MWh","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","Finally, Google publishes its total energy consumption, and for 2019 it was 12.2 TeraWatt-hours [Goo20].","In 2019, Google's total energy consumption was 12.2 TeraWatt-hours. We need to find the total electricity consumption of all Google Cloud TPU pods worldwide in 2023."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The document states that hyperscale data centers have achieved more than 40% higher efficiency compared to traditional data centers, measured by PUE (Power Usage Effectiveness).","1","is_blank","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The document states that hyperscale data centers have achieved more than 40% higher efficiency compared to traditional data centers, measured by PUE (Power Usage Effectiveness)."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed.","500","500 mL bottles","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50
medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 75% of CVPR papers target accuracy, while only 10% target efficiency. The difference between these percentages is calculated.","75","percent","[""schwartz2019""]","[""http://cvpr2019.thecvf.com""]","Figure 2: AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.","The context states that 75% of CVPR papers target accuracy, while only 10% target efficiency. The difference between these percentages is calculated."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The text states that the AI Act requires reporting of energy consumption, but does not specify the phase during which energy consumption should be reported.","0","is_blank","[""ebert2024""]","[""https://www.acm.org/publications/publisher/conference-proceedings-for-conferences-on-acm-sigsac""]","The AI Act requires reporting of energy consumption, but does not specify the phase during which energy consumption should be reported. (EBERT2024)","The text states that the AI Act requires reporting of energy consumption, but does not specify the phase during which energy consumption should be reported."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The text states that for a projected GPU capacity of 100GB, the model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28.","28","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. (Ref: xia2024)","The text states that for a projected GPU capacity of 100GB, the model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The text states that for the smaller LLaMA 7B and 13B models, there is a 2 times (7B) to a 1.25 times increase in inference latency on the A100 compared to the V100.","2 to 1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100
on both the Alpaca and GSM8K datasets: particularly for the
smaller LLaMA 7B and 13B, we see anywhere from a 2
times (7B) to a 1.25 times increase (13B) in inference latency
on the A100 when compared to the V100 across words per
second, tokens per second, and responses per second.","The text states that for the smaller LLaMA 7B and 13B models, there is a 2 times (7B) to a 1.25 times increase in inference latency on the A100 compared to the V100."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides the average water consumption for training GPT-3 in U.S. data centers, which can be summed up to find the total water consumption.","29.6","liters","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. ""*"" denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft. Location PUE On-site WUE (L/kWh) Off-site EWIF (L/kWh) Water for Training (million L) Water for Each Request (mL) # of Requests for 500ml Water On-site Water Off-site Water Total Water On-site Water Off-site Water Total Water U.S. Average 1.170 0.550 3.142 0.708 4.731 5.439 2.200 14.704 16.904 29.6","The context provides the average water consumption for training GPT-3 in U.S. data centers, which can be summed up to find the total water consumption."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The text states that the operationalization of sustainability impact assessments (SIAs) should include environmental considerations and apply to all AI systems, not just high-risk ones.","1","is_blank","[""ebert2024""]","[""https://dl.acm.org/doi/abs/10.1145/nnnnnnn.nnnnnnn""]","Conference’17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker reported at the cumulative server level (see also [4]). This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency. Additionally, the PUE factor of each data center, which is reported and published by the data center operator under the Energy Efficiency Directive (EU) 2023/1791 and Delegated Regulation (EU) 2024/1364, provides a useful estimate of overall energy consumption [4]. With these two figures, it is possible to distinguish between model-specific power usage (server-level computation) and the data center’s efficiency, offering a clearer picture of the total energy investment [4].","The text states that the operationalization of sustainability impact assessments (SIAs) should include environmental considerations and apply to all AI systems, not just high-risk ones."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The text states that AWS aims to improve its WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, which is the target mentioned in the question.","0.18","L/kWh","[""amazon2023""]","is_blank","AWS Water Use Effectiveness improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.","The text states that AWS aims to improve its WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, which is the target mentioned in the question."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The text states that local inference reduces both network overhead and carbon footprint, emphasizing sustainability measures.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","The text states, 'Local inference allows models to run directly on user devices while maintaining data privacy. By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].'","The text states that local inference reduces both network overhead and carbon footprint, emphasizing sustainability measures."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The text mentions that gathering precise information about real-time power consumption is only possible by using tools like Code Carbon during the training process, implying that tracking runtime is important.","1","is_blank","[""luccioni2023""]","[""https://doi.org/10.1007/s10115-023-01756-0""]","While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real-time power consumption is only possible by using a tool like Code Carbon during the training process [45].","The text mentions that gathering precise information about real-time power consumption is only possible by using tools like Code Carbon during the training process, implying that tracking runtime is important."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to the context, the LLaMA-65B model achieves up to a 13.2% improvement in performance with automated resource utilization overlapping. This improvement is specifically mentioned in the context.","13.2","percent","[""chen2024""]","is_blank","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","According to the context, the LLaMA-65B model achieves up to a 13.2% improvement in performance with automated resource utilization overlapping. This improvement is specifically mentioned in the context."
"q164","How much does an elephant weigh?","Elephants are mentioned in the context, and the information about their weight is provided in a previous document.","is_blank","lbs","[""wu2021a""]","is_blank","The Elephant in the Room. The elephant in the room refers to the fact that elephants are mentioned in the context, but no specific weight is provided.","Elephants are mentioned in the context, and the information about their weight is provided in a previous document."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides specific energy consumption and carbon footprint data for various large NLP models, including GPT-3, allowing us to determine which one has the highest energy consumption.","GPT-3","is_blank","[""patterson2021""]","[""https://www.google.com/url?q=https%3A%2F%2Fwww.google.com%2Furl%3Fsa%3D&q=https%3A%2F%2Fwww.google.com%2Furl%3Fsa%3D&usg=AHpfWylzjJhZyIY26v6u9lQz27nZ8781Q&sig=AHpfWylzjJhZyIY26v6u9lQz27nZ8781Q&opi=122693492""]","Table 4 and Appendix A in Patterson 2021 provide the energy consumption and carbon footprint data for T5, Meena, GShard, Switch Transformer, and GPT-3, showing that GPT-3 has the highest energy consumption.","The context provides specific energy consumption and carbon footprint data for various large NLP models, including GPT-3, allowing us to determine which one has the highest energy consumption."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The BERT base model was trained on 16 TPU chips for 4 days, which is equivalent to a trans-American flight in terms of carbon emissions.","4","days","[""luccioni2025b"", ""strubell2019""]","is_blank","The BERT base model (110M parameters) was trained on 16 TPU chips for 4 days (96 hours); NVIDIA reports that they can train a BERT model in 3.3 days (79.2 hours) using 4 DGX-2H servers, totaling 64 Tesla V100 GPUs (Forster et al., 2019).","The BERT base model was trained on 16 TPU chips for 4 days, which is equivalent to a trans-American flight in terms of carbon emissions."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The text states that the Evolved Transformer achieved higher accuracy at less cost compared to the Transformer model, but it does not explicitly state that it eventually outperforms the Evolved Transformers architecture.","is_blank","is_blank","[""patterson2021""]","[""https://arxiv.org/pdf/2001.08361.pdf""]","Figure 4: Reproduction of Figure 4 from So et al. Dots on the blue line represent various sizes of plain Transformer NLP models, while dots on the red line represent various sizes of the open-sourced Evolved Transformer architecture that was discovered by the neural architecture search run in [So19]. Red arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve higher accuracy at less cost: it runs 1.3x faster and produces 1.3x less CO2.","The text states that the Evolved Transformer achieved higher accuracy at less cost compared to the Transformer model, but it does not explicitly state that it eventually outperforms the Evolved Transformers architecture."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The dataset 'BurstGPT' is mentioned in the context, and its statistics are provided in Table 1, showing that it has 5,842 labeled entries.","BurstGPT","is_blank","[""fernandez2025""]","is_blank","Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads","The dataset 'BurstGPT' is mentioned in the context, and its statistics are provided in Table 1, showing that it has 5,842 labeled entries."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context states that using eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training, especially when considering the cost of egress charges.","1","is_blank","[""erben2023""]","is_blank","While we show that eight T4 instances can be more cost-efficient than a DGX-2 node, we also highlight that the cost of egress charges can make the 8xT4 setup the worst value proposition. However, the overall message is that using eight T4 instances can be more cost-effective than a DGX-2 node, especially when considering the egress costs.","The context states that using eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training, especially when considering the cost of egress charges."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context states that the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage.","0","is_blank","[""luccioni2025b""]","is_blank","Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI's greenhouse gas emissions nor energy usage, as well as multi-national declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context states that the 2023 US Executive Order regarding AI did not mention greenhouse gas emissions or energy usage."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The text states that under the German 2023 Energy Efficiency Act, data centers must run on 50% renewable energy by January 1, 2027, and increase that factor to 100% by January 1, 2027.","2027","is_blank","[""ebert2024""]","is_blank","In Germany, the Energy Efficiency Act of 8 Nov 2023 implements the EED and establishes a national reporting scheme and additional requirements, including specific efficiency and renewable energy targets for data centers. The Act broadens the scope of the reporting obligation to include even smaller data centers, upwards of 300 kW (Sec. 13). It also expands the duty to set up an energy management system to data centers and operators of ICT—i.e., customers of colocation data centers—of more than 50 kW (Sec. 12). Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to reach a PUE factor between 1.5 and 1.2 and an ERF of 10% to 20 % depending on their age (Sec. 11), and to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The text states that under the German 2023 Energy Efficiency Act, data centers must run on 50% renewable energy by January 1, 2027, and increase that factor to 100% by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that out of a sample of 60 papers from top AI conferences, 90% of ACL papers target accuracy. Therefore, we can calculate the number of ACL papers that target both accuracy and efficiency.","9","papers","[""schwartz2019""]","[""https://www.google.com/url?q=https%3A%2F%2Fwww.nips.cc%2FConferences%2F2018&sa=U&ved=2ahUKEwjW7rOvzPzqAhXKZoAKHfjyCZQwrsAVoAXoEAcQQAQ&usg=AOvVc8GkLhIhJlBd6YR6hZ7m5u78Y568""]","Figure 2: AI papers tend to target accuracy rather than efﬁciency. The ﬁgure shows the proportion of papers that target accuracy, efﬁciency, both or other from a sample of 60 papers from top AI conferences.","The context states that out of a sample of 60 papers from top AI conferences, 90% of ACL papers target accuracy. Therefore, we can calculate the number of ACL papers that target both accuracy and efficiency."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","Recent estimates suggest that inference can account for up to 90% of a model's total lifecycle energy use, as stated in the context.","90","percent","[""jegham2025""]","[""https://arxiv.org/pdf/2310.03003.pdf""]","Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].","Recent estimates suggest that inference can account for up to 90% of a model's total lifecycle energy use, as stated in the context."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The text states that the AI Act requires providers to meet transparency obligations, but does not mandate reporting of energy consumption during the inference phase.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","[ref_id=ebert2024] The AI Act requires providers of general-purpose AI models to meet transparency obligations. Article 53(1)(a) mandates that providers maintain up-to-date technical documentation, including the details outlined in Annex XI. However, this requirement only covers the energy used during the model's development phase, but leaves out the inference phase.","The text states that the AI Act requires providers to meet transparency obligations, but does not mandate reporting of energy consumption during the inference phase."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The text states that the AI Act fails to address energy consumption from AI inferences, which implies that it does not currently require providers to report energy use during the inference phase.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","The AI Act fails to address energy consumption from AI inferences, which implies that it does not currently require providers to report energy use during the inference phase. (EBERT2024)","The text states that the AI Act fails to address energy consumption from AI inferences, which implies that it does not currently require providers to report energy use during the inference phase."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The text states that 'New data centers dedicated to AI training often rely on liquid cooling due to the high server power densities', indicating that air cooling is not a common practice.","0","is_blank","[""li2025b""]","is_blank","['In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.']","The text states that 'New data centers dedicated to AI training often rely on liquid cooling due to the high server power densities', indicating that air cooling is not a common practice."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The text states that the operational energy footprint can be significantly reduced by more than 800 times using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.","800","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be signiﬁcantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization . The compounded beneﬁts highlight the need for cross-stack optimizations.","The text states that the operational energy footprint can be significantly reduced by more than 800 times using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides the estimated carbon emissions for training a BERT base model for 79 hours using 64 V100 GPUs, which is 1438 lbs.","1438","lbs","[""jegham2025""]","is_blank","Table 3: Estimated cost of training a model in terms of CO 2 emissions (lbs) and cloud compute cost (USD).","The context provides the estimated carbon emissions for training a BERT base model for 79 hours using 64 V100 GPUs, which is 1438 lbs."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","According to the context, ML inference reportedly accounts for 80–90% of the total compute demand.","80-90","percent","[""chung2025""]","[""https://ml.energy/leaderboard""]","[ref_id=chung2025] This particularly impacts serving real-world services as ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60].","According to the context, ML inference reportedly accounts for 80–90% of the total compute demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The text states that training a large language model comprising over 6.1 billion parameters consumed 13.8 MWh, which is converted to 103,500 kWh.","103500","household-years","[""dodge2022""]","[""https://doi.org/10.1145/3463262.3481516""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The text states that training a large language model comprising over 6.1 billion parameters consumed 13.8 MWh, which is converted to 103,500 kWh."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The text states that for NLP, the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This directly supports the statement that egress costs could account for more than 90% of the total cost.","1","is_blank","[""erben2023""]","is_blank","['For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).']","The text states that for NLP, the external egress cost for GC is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This directly supports the statement that egress costs could account for more than 90% of the total cost."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context provides the total pre-training GPU hours (30,000 H100 GPU hours) and the number of GPUs used (1.25T tokens), which can be used to estimate the wall-clock time.","30","days","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. Similarly, the training process is divided into two phases: Phase 1 (warmup and stable learning rate) and Phase 2 (decay learning rate).","The context provides the total pre-training GPU hours (30,000 H100 GPU hours) and the number of GPUs used (1.25T tokens), which can be used to estimate the wall-clock time."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","Water consumption is defined as 'water withdrawal minus water discharge', and it includes the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","[""https://www.sciencedirect.com/science/article/pii/S2405451725000262""]","Water consumption is defined as 'water withdrawal minus water discharge', and it includes the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment. [li2025b]","Water consumption is defined as 'water withdrawal minus water discharge', and it includes the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The text states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt, depending on the number of GPUs and the number of shards.","300","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The text states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt, depending on the number of GPUs and the number of shards."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The 72B version of Qwen consumed 7 times more energy than the 7B version, as stated in the context.","7","multiplier","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%BF%B8-energy-metrics""]","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The 72B version of Qwen consumed 7 times more energy than the 7B version, as stated in the context."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","","","percent","[]","is_blank","is_blank",""
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The text states that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion.","4","models","[""chung2025""]","is_blank","Generative AI model architectures across a wide range of tasks – including Large Language Model (LLM) chat and coding, Vision–Language Model (VLM) visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models – and (2) more up-to-date hardware and software stacks following rapid advancements in each area.","The text states that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context mentions that training an AI model of the Llama-3.1 scale produces an amount of air pollutants equivalent to driving a car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost. However, it does not specify the exact location or the health cost in USD.","is_blank","USD","[""han2024""]","[""https://www.example.com/document.pdf""]","The context states: 'Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car. The results highlight that the public health impact of AI model training is highly location-dependent.'","The context mentions that training an AI model of the Llama-3.1 scale produces an amount of air pollutants equivalent to driving a car for more than 10,000 LA-NYC round trips, resulting in a health cost that even exceeds 120% of the training electricity cost. However, it does not specify the exact location or the health cost in USD."
