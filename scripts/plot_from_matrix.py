#!/usr/bin/env python3
"""
Plot Results from Matrix

Generates comparison charts (Overall Score, Question Type Breakdown)
using the 'Results Matrix' CSV generated by scripts/generate_results_matrix.py.

Usage:
    python scripts/plot_from_matrix.py --matrix artifacts/results_matrix.csv
"""

import argparse
import sys
from pathlib import Path
import pandas as pd

try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    import numpy as np
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    from scipy import stats
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

import json
import glob


def wilson_ci(successes, n, confidence=0.95):
    """Wilson score interval for binomial proportion.

    More accurate than normal approximation for small sample sizes.
    Returns (lower, upper) bounds for the confidence interval.
    """
    if n == 0:
        return 0.0, 0.0

    if not HAS_SCIPY:
        # Fallback: simple normal approximation (less accurate for small n)
        p = successes / n
        se = np.sqrt(p * (1 - p) / n) if n > 0 else 0
        z = 1.96 if confidence == 0.95 else 1.645  # 95% or 90%
        return max(0, p - z * se), min(1, p + z * se)

    z = stats.norm.ppf(1 - (1 - confidence) / 2)
    p = successes / n
    denom = 1 + z**2 / n
    center = (p + z**2 / (2 * n)) / denom
    spread = z * np.sqrt(p * (1 - p) / n + z**2 / (4 * n**2)) / denom
    return max(0, center - spread), min(1, center + spread)

def load_costs(experiments_dir="artifacts/experiments", datafile=None):
    """Load cost data from summary.json files.

    When *datafile* is given (e.g. ``"train_QA"``), only summaries whose
    path contains that subfolder are included.
    """
    costs = {} # model_name -> cost_usd

    # search patterns
    patterns = [
        f"{experiments_dir}/**/summary.json",
        f"artifacts/**/summary.json"
    ]

    for pattern in patterns:
        for p in glob.glob(pattern, recursive=True):
            path = Path(p)
            if datafile is not None and datafile not in path.parts:
                continue
            try:
                with open(path, "r") as f:
                    data = json.load(f)

                # Try to derive model name from path or data
                # Path likely: .../experiments/<datafile>/model_name/summary.json
                model_name = path.parent.name.replace("-v1", "").replace("bedrock_", "")

                cost = data.get("estimated_cost_usd") or data.get("total_cost_usd") or data.get("cost_usd") or 0.0
                if cost > 0:
                     costs[model_name] = cost
            except Exception as e:
                print(f"Warning: Failed to load cost from {p}: {e}")

    return costs

def main():
    if not HAS_MATPLOTLIB:
        print("Error: matplotlib is required. Please install it with 'pip install matplotlib'.")
        sys.exit(1)

    parser = argparse.ArgumentParser(description="Plot results from matrix")
    parser.add_argument(
        "--matrix", "-m",
        default="artifacts/results_matrix.csv",
        help="Path to results matrix CSV"
    )
    parser.add_argument(
        "--output-dir", "-o",
        default="artifacts/plots",
        help="Directory to save plots"
    )
    parser.add_argument(
        "--datafile", "-d",
        default=None,
        help="Filter cost data to this datafile subfolder "
             "(e.g. 'train_QA', 'test_solutions'). Default: include all.",
    )

    args = parser.parse_args()

    # Load Costs
    if args.datafile:
        print(f"Filtering costs to datafile: {args.datafile}")
    model_costs = load_costs(datafile=args.datafile)
    print(f"Loaded costs for {len(model_costs)} models: {list(model_costs.keys())}")
    
    matrix_path = Path(args.matrix)
    if not matrix_path.exists():
        print(f"Error: Matrix file not found: {matrix_path}")
        print("Run scripts/generate_results_matrix.py first.")
        sys.exit(1)
        
    print(f"Loading matrix from {matrix_path}...")
    df = pd.read_csv(matrix_path)
    
    # Identify models by finding columns ending in "_Value"
    raw_models = [c.replace("_Value", "") for c in df.columns if c.endswith("_Value") and c != "GT_Value"]
    
    # Filter and Deduplicate
    IGNORED_MODELS = {"test", "new", "v2", "fixed", "jinav4", "haiku-baseline"}
    
    model_map = {} # normalized_name -> original_name_in_csv
    
    for m in raw_models:
        # Normalize: llama4-scout -> llama4_scout
        norm = m.replace("-", "_").lower()
        if any(ign in norm for ign in IGNORED_MODELS):
            continue
        
        # Prefer the one with underscores if conflict (arbitrary but consistent)
        model_map[norm] = m
        
    models = sorted(list(model_map.values()))
    print(f"Selected {len(models)} models after filtering: {models}")
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Helper for plot aesthetics
    def setup_plot(ax, title, ylabel):
        ax.set_title(title, fontsize=14, pad=20)
        ax.set_ylabel(ylabel, fontsize=12)
        ax.grid(axis='y', linestyle='--', alpha=0.3)
        plt.xticks(rotation=45, ha='right')

    
    # -------------------------------------------------------------------------
    # 1. Overall Score Comparison
    # -------------------------------------------------------------------------
    print("Generating overall score comparison...")
    
    overall_scores = {}
    
    # Calculate scores for each model
    # Score = 0.75 * Val + 0.15 * Ref + 0.10 * NA
    # Note: Using simplified scoring here based on the boolean correct columns if available
    # But generate_results_matrix produces: {model}_ValCorrect (bool/1/0)
    # We might need to handle Ref scoring if we want exact WattBot score.
    # Ref score usually needs overlap calculation. generate_results_matrix calls row_bits which does it.
    
    for model in models:
        val_col = f"{model}_ValCorrect"
        ref_col = f"{model}_RefScore"
        na_col = f"{model}_NACorrect"

        if val_col in df.columns and ref_col in df.columns:
             val_acc = df[val_col].mean()
             ref_acc = df[ref_col].mean()

             # NA component: recall over truly-NA questions only
             # GT_Value == "is_blank" (or similar) indicates a truly-NA question
             if na_col in df.columns and "GT_Value" in df.columns:
                 na_gt_mask = df["GT_Value"].apply(
                     lambda v: str(v).strip().lower() in ("", "na", "n/a", "is_blank", "nan")
                 )
                 if na_gt_mask.any():
                     na_recall = df.loc[na_gt_mask, na_col].mean()
                 else:
                     na_recall = 1.0
             elif na_col in df.columns:
                 na_recall = df[na_col].mean()  # fallback if no GT_Value column
             else:
                 na_recall = 1.0

             # WattBot Score = 0.75*Val + 0.15*Ref + 0.10*NA_recall
             overall_score = 0.75 * val_acc + 0.15 * ref_acc + 0.10 * na_recall

             # Compute 95% CI via error propagation across weighted components
             n = len(df)
             val_se = np.sqrt(val_acc * (1 - val_acc) / n) if n > 0 else 0
             ref_se = df[ref_col].std() / np.sqrt(n) if n > 1 else 0
             if na_col in df.columns and "GT_Value" in df.columns and na_gt_mask.any():
                 n_na = na_gt_mask.sum()
                 na_se = np.sqrt(na_recall * (1 - na_recall) / n_na) if n_na > 1 else 0
             else:
                 na_se = 0
             overall_se = np.sqrt((0.75 * val_se)**2 + (0.15 * ref_se)**2 + (0.10 * na_se)**2)

             overall_scores[model] = {
                 "Value Accuracy": val_acc,
                 "Ref Overlap": ref_acc,
                 "NA Recall": na_recall,
                 "Overall": overall_score,
                 "n_questions": n,
                 "overall_ci": 1.96 * overall_se,  # 95% CI half-width
             }
        else:
            print(f"Warning: Score columns missing for {model}")

    # Plot Overall
    fig, ax = plt.subplots(figsize=(12, 7))
    
    model_names = list(overall_scores.keys())
    scores = [overall_scores[m]["Overall"] for m in model_names]
    
    # Sort by score descending
    sorted_pairs = sorted(zip(model_names, scores), key=lambda x: x[1], reverse=True)
    model_names = [p[0] for p in sorted_pairs]
    scores = [p[1] for p in sorted_pairs]
    
    ci_widths = [overall_scores[m]["overall_ci"] for m in model_names]
    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models)))
    bars = ax.bar(model_names, scores, color=colors, width=0.6,
                  yerr=ci_widths, capsize=4, error_kw={'linewidth': 1.5, 'color': '#333'})

    ax.set_ylim(0, 1.15)
    n_questions = len(df)
    setup_plot(ax, f"Model Performance (WattBot Score, n={n_questions}, 95% CI)", "WattBot Score (0.75*Val + 0.15*Ref + 0.10*NA)")
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 5),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
                    
    plt.tight_layout()
    plt.savefig(output_dir / "overall_scores.png", dpi=300)
    print(f"Saved {output_dir / 'overall_scores.png'}")
    
    # -------------------------------------------------------------------------
    # 2. Question Type Breakdown
    # -------------------------------------------------------------------------
    # Derive types if explanation exists (logic from plot_results.py)
    if "GT_Explanation" in df.columns:
        print("Generating question type breakdown...")
        
        def get_types(row):
            expl = str(row.get("GT_Explanation", "")).lower()
            types = []
            if "table" in expl: types.append("Table")
            if "figure" in expl or "fig" in expl: types.append("Figure")
            if "quote" in expl: types.append("Quote")
            if "math" in expl or "calculation" in expl: types.append("Math")
            
            # Check GT Value for NA
            gt_val = str(row.get("GT_Value", "")).lower()
            if "unable" in gt_val or "is_blank" in gt_val:
                types.append("is_NA")
                
            return types

        df["Derived_Types"] = df.apply(get_types, axis=1)
        
        all_types = ["Table", "Figure", "Quote", "Math", "is_NA"]
        type_scores = {m: {} for m in models}
        type_ci = {m: {} for m in models}  # Store confidence intervals
        type_counts = {}  # Store N per type

        for qtype in all_types:
            # Filter rows having this type
            mask = df["Derived_Types"].apply(lambda t: qtype in t)
            subset = df[mask]
            n_type = len(subset)
            type_counts[qtype] = n_type

            if n_type == 0:
                continue

            for model in models:
                # For NA questions, use NACorrect (ValCorrect is always True
                # when GT is blank, so it's meaningless for NA detection)
                if qtype == "is_NA":
                    score_col = f"{model}_NACorrect"
                else:
                    score_col = f"{model}_ValCorrect"
                if score_col in subset.columns:
                    successes = subset[score_col].sum()
                    acc = successes / n_type if n_type > 0 else 0
                    type_scores[model][qtype] = acc

                    # Calculate confidence interval
                    ci_low, ci_high = wilson_ci(successes, n_type, confidence=0.95)
                    type_ci[model][qtype] = (ci_low, ci_high)
        
        # Plotting with error bars
        fig, ax = plt.subplots(figsize=(14, 7))

        x = np.arange(len(all_types))
        width = 0.8 / len(models)

        for i, model in enumerate(models):
            scores = [type_scores[model].get(t, 0) for t in all_types]
            # Calculate error bar sizes (distance from mean to CI bounds)
            yerr_lower = []
            yerr_upper = []
            for t in all_types:
                if t in type_ci[model]:
                    ci_low, ci_high = type_ci[model][t]
                    score = type_scores[model].get(t, 0)
                    yerr_lower.append(max(0, score - ci_low))
                    yerr_upper.append(max(0, ci_high - score))
                else:
                    yerr_lower.append(0)
                    yerr_upper.append(0)

            ax.bar(x + i*width, scores, width, label=model, color=colors[i],
                   yerr=[yerr_lower, yerr_upper], capsize=2, error_kw={'linewidth': 1})

        # Add N counts to labels with warning for small samples
        labels_with_counts = []
        for t in all_types:
            count = type_counts.get(t, 0)
            warning = " (!)" if count < 10 else ""  # Flag small samples
            labels_with_counts.append(f"{t}\n(n={count}{warning})")

        ax.set_xticks(x + width * (len(models) - 1) / 2)
        ax.set_xticklabels(labels_with_counts)
        
        setup_plot(ax, "Performance by Question Type (95% CI error bars)", "Value Accuracy")
        ax.legend(loc='upper right', bbox_to_anchor=(1.18, 1), fontsize=9)
        ax.set_ylim(0, 1.15)  # Extra room for error bars

        # Add note about small sample sizes
        ax.text(0.02, 0.98, "Note: (!) indicates n<10, interpret with caution",
                transform=ax.transAxes, fontsize=8, verticalalignment='top',
                style='italic', color='gray')
        
        plt.tight_layout()
        plt.savefig(output_dir / "accuracy_by_type.png", dpi=300)
        print(f"Saved {output_dir / 'accuracy_by_type.png'}")

    # -------------------------------------------------------------------------
    # 3. Model Agreement Heatmap
    # -------------------------------------------------------------------------
    print("Generating agreement heatmap...")
    # Calculate agreement (Jaccard or simple Overlap)
    # We'll use simple agreement on "ValCorrect" status
    
    n_models = len(models)
    agreement_matrix = np.zeros((n_models, n_models))
    
    for i, m1 in enumerate(models):
        for j, m2 in enumerate(models):
            if i == j:
                agreement_matrix[i, j] = 1.0
            else:
                col1 = f"{m1}_ValCorrect"
                col2 = f"{m2}_ValCorrect"
                if col1 in df.columns and col2 in df.columns:
                    # Agreement: Fraction of questions where both are Correct or both are Wrong
                     matches = (df[col1] == df[col2]).sum()
                     agreement_matrix[i, j] = matches / len(df)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(agreement_matrix, cmap="YlGnBu", vmin=0, vmax=1)
    
    # We want to show all ticks...
    ax.set_xticks(np.arange(n_models))
    ax.set_yticks(np.arange(n_models))
    # ... and label them with the respective list entries
    ax.set_xticklabels(models, rotation=45, ha="right")
    ax.set_yticklabels(models)
    
    # Loop over data dimensions and create text annotations.
    for i in range(n_models):
        for j in range(n_models):
            text = ax.text(j, i, f"{agreement_matrix[i, j]:.2f}",
                           ha="center", va="center", color="black")
                           
    ax.set_title("Model Agreement (Correctness Correlation)")
    fig.tight_layout()
    plt.savefig(output_dir / "agreement_heatmap.png")
    print(f"Saved {output_dir / 'agreement_heatmap.png'}")

    # -------------------------------------------------------------------------
    # 4. Unique Wins Analysis
    # -------------------------------------------------------------------------
    print("Generating unique wins analysis...")
    unique_wins = {}
    
    for m_target in models:
        # Check correct for target
        target_col = f"{m_target}_ValCorrect"
        if target_col not in df.columns: continue
        
        # Check correct for ALL others
        other_cols = [f"{m}_ValCorrect" for m in models if m != m_target and f"{m}_ValCorrect" in df.columns]
        if not other_cols: continue
        
        # Rows where Target is True AND All Others are False
        wins = df[
            (df[target_col] == True) & 
            (df[other_cols] == False).all(axis=1)
        ]
        unique_wins[m_target] = len(wins)
        
    fig, ax = plt.subplots(figsize=(12, 7))
    bars = ax.bar(unique_wins.keys(), unique_wins.values(), color='orange', width=0.6)
    
    setup_plot(ax, "Unique Wins (Questions only THIS model got right)", "Count of Questions")
    
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 5),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
                    
    plt.tight_layout()
    plt.savefig(output_dir / "unique_wins.png", dpi=300)
    print(f"Saved {output_dir / 'unique_wins.png'}")

    # -------------------------------------------------------------------------
    # 5. Refusal Rate Comparison
    # -------------------------------------------------------------------------
    print("Generating refusal rate comparison...")
    refusal_rates = {}
    
    for model in models:
        val_col = f"{model}_Value"
        if val_col in df.columns:
            # Check for strings like "is_blank" or "unable"
            # We treat everything as string first
            vals = df[val_col].fillna("is_blank").astype(str).str.lower()
            refusals = vals.apply(lambda x: "is_blank" in x or "unable" in x or "refuse" in x)
            rate = refusals.mean() * 100
            refusal_rates[model] = rate
            
    fig, ax = plt.subplots(figsize=(12, 7))
    bars = ax.bar(refusal_rates.keys(), refusal_rates.values(), color='gray', width=0.6)
    
    # Scale y-axis
    y_max = max(refusal_rates.values() or [10]) * 1.2
    ax.set_ylim(0, y_max)
    setup_plot(ax, "Refusal Rate (% of questions answered as 'Unable')", "Percentage (%)")
    
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.1f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 5),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
                    
    plt.tight_layout()
    plt.savefig(output_dir / "refusal_rates.png", dpi=300)
    print(f"Saved {output_dir / 'refusal_rates.png'}")

    # -------------------------------------------------------------------------
    # 6. Cost vs. Score (Pareto Frontier)
    # -------------------------------------------------------------------------
    print("Generating Cost vs. Score comparison...")
    
    # Prepare data
    plot_models = []
    plot_scores = []
    plot_costs = []
    
    for model in models:
        # Match model name with cost keys (fuzzy match might be needed)
        # Try exact match, then replace _ with -
        cost = model_costs.get(model)
        if cost is None:
            cost = model_costs.get(model.replace("_", "-"))
            
        if cost is not None:
             score = overall_scores.get(model, {}).get("Overall", 0)
             plot_models.append(model)
             plot_scores.append(score)
             plot_costs.append(cost)
        else:
             print(f"Skipping cost plot for {model}: No cost data found")
             
    if len(plot_models) > 1:
        fig, ax = plt.subplots(figsize=(10, 8))
        ax.scatter(plot_costs, plot_scores, color='blue', alpha=0.7, s=100)
        
        # Annotate points
        for i, txt in enumerate(plot_models):
            ax.annotate(txt, (plot_costs[i], plot_scores[i]), 
                        xytext=(5, 5), textcoords='offset points')
                        
        ax.set_xlabel("Total Cost (USD)")
        ax.set_ylabel("Overall Score")
        ax.set_title("Cost vs. Performance Trade-off")
        ax.grid(True, linestyle='--', alpha=0.6)
        
        plt.tight_layout()
        plt.savefig(output_dir / "cost_vs_score.png")
        print(f"Saved {output_dir / 'cost_vs_score.png'}")
    else:
        print("Not enough models with cost data to generate cost plot.")

if __name__ == "__main__":
    main()
