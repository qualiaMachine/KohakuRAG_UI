"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context explicitly states that the ML.ENERGY Benchmark is a benchmark suite and tool for measuring inference energy consumption under realistic service environments. It is described as the first inference energy benchmark for modern generative AI models, and it provides an extensible benchmark suite for measuring energy consumption across various tasks and hardware configurations.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","['The ML.ENERGY Benchmark is the first inference energy benchmark for modern generative AI models, and empowers users to not only measure but also optimize the energy consumption of their models.']","The context explicitly states that the ML.ENERGY Benchmark is a benchmark suite and tool for measuring inference energy consumption under realistic service environments. It is described as the first inference energy benchmark for modern generative AI models, and it provides an extensible benchmark suite for measuring energy consumption across various tasks and hardware configurations."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context explicitly states that the net CO2e emissions from training the GShard-600B model are 4.3 tCO2e, as confirmed in multiple references including Table 4 and direct statements in the text.","4.3","tCO2e","[""patterson2021""]","is_blank","['GShard-600B’s emissions (Table 4) are 4.3 tCO2e —3.5 passenger SF-NY round trips—from consuming 24 MWh to train the model that could have 2B users; the amortized per-user CO2e impact of model training would be less than the CO2e impact of sending one text message 26.', 'Training GShard-600B used 24 MWh and produced 4.3 net tCO2e.']","The context explicitly states that the net CO2e emissions from training the GShard-600B model are 4.3 tCO2e, as confirmed in multiple references including Table 4 and direct statements in the text."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context explicitly states the model size for LLaMA-33B in Table 3, where 'Parameters' is listed as 64.7 GB. This directly answers the question about the model size in gigabytes.","64.7","GB","[""chen2024""]","[""https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md""]","LLaMA-33B 64.7 GB 60 6656 1","The context explicitly states the model size for LLaMA-33B in Table 3, where 'Parameters' is listed as 64.7 GB. This directly answers the question about the model size in gigabytes."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","The context provides information about Google's total energy consumption in 2019, which was 12.2 Terawatt-hours, but does not provide any data on the electricity consumption of Google Cloud TPU pods specifically in 2023. While some references mention TPU energy usage in general (e.g., ~75 MWh for a hypothetical system), no figure is given for all TPU pods worldwide in 2023. Therefore, the requested value cannot be determined from the provided documents.","Unable to answer with confidence based on the provided documents.","MWh","[""patterson2021"", ""li2025b"", ""han2024""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","[""Google's total energy consumption in 2019 was 12.2 Terawatt-hours."", 'The total electricity consumption for AI training in the U.S. is projected to reach 150–300 TWh in 2028.', ""Meta's Llama-3.1 training consumed approximately 30 GWh, excluding non-GPU overheads.""]","The context provides information about Google's total energy consumption in 2019, which was 12.2 Terawatt-hours, but does not provide any data on the electricity consumption of Google Cloud TPU pods specifically in 2023. While some references mention TPU energy usage in general (e.g., ~75 MWh for a hypothetical system), no figure is given for all TPU pods worldwide in 2023. Therefore, the requested value cannot be determined from the provided documents."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context explicitly states that between traditional and highly optimized hyperscale data centers, there is a stark difference in power usage effectiveness (PUE), with hyperscale data centers achieving more than 40% higher efficiency. This is directly supported by the reference [ref_id=wu2021b], which confirms the claim.","1","is_blank","[""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/""]","Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efﬁciency for hyperscale data centers (Figure 1).","The context explicitly states that between traditional and highly optimized hyperscale data centers, there is a stark difference in power usage effectiveness (PUE), with hyperscale data centers achieving more than 40% higher efficiency. This is directly supported by the reference [ref_id=wu2021b], which confirms the claim."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many 500 mL bottles are consumed per medium-length response.","1","500 mL bottles","[""li2025b"", ""luccioni2025a""]","is_blank","['GPT-3 needs to “drink” (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.', 'Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].']","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500 mL bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. This directly answers the question about how many 500 mL bottles are consumed per medium-length response."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","The context states that 75% of CVPR papers target accuracy, and 20% argue for a new efficiency result. The difference between these percentages is 75% - 20% = 55%.","55","percent","[""schwartz2019""]","[""http://cvpr2019.thecvf.com""]","75% of CVPR papers... only a small portion (10% and 20% respectively) argue for a new efficiency result.","The context states that 75% of CVPR papers target accuracy, and 20% argue for a new efficiency result. The difference between these percentages is 75% - 20% = 55%."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context states that the AI Act does not mandate the disclosure of energy consumption during the inference phase and that when disclosure is required, it is restricted to authorities and not accessible to the general public due to confidentiality clauses. The authors propose a specific interpretation to make such data publicly available, but this is not currently part of the Act. Therefore, the claim that the AI Act makes energy consumption data publicly available to NGOs, analysts, and the general public is false.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['Where the Act does mandate the disclosure of energy consumption, this information is restricted to authorities and is not accessible to downstream providers (unless the proposed interpretation from 2) is applied) or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1) [4].', 'The AI Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.']","The context states that the AI Act does not mandate the disclosure of energy consumption during the inference phase and that when disclosure is required, it is restricted to authorities and not accessible to the general public due to confidentiality clauses. The authors propose a specific interpretation to make such data publicly available, but this is not currently part of the Act. Therefore, the claim that the AI Act makes energy consumption data publicly available to NGOs, analysts, and the general public is false."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","The context explicitly states that for GPU memory capacities of 100GB and 120GB, the analytical model predicts the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Since the question asks for the projected maximum batch size at a GPU capacity of 100GB, the answer is 28.","28","samples","[""xia2024""]","is_blank","['For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.']","The context explicitly states that for GPU memory capacities of 100GB and 120GB, the analytical model predicts the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Since the question asks for the projected maximum batch size at a GPU capacity of 100GB, the answer is 28."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context explicitly states that for the LLaMA 7B model, there is a 2 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup of approximately 2 times in inference throughput when using A100 GPUs over V100 GPUs.","2","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context explicitly states that for the LLaMA 7B model, there is a 2 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup of approximately 2 times in inference throughput when using A100 GPUs over V100 GPUs."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","The context provides a detailed estimate of GPT-3’s operational water consumption footprint in Table 1. The 'U.S. Average' row in the table lists the 'Water for Training' as 5.439 million liters, which represents the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers.","5.439","liters","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. U.S. Average: Water for Training(million L) = 5.439","The context provides a detailed estimate of GPT-3’s operational water consumption footprint in Table 1. The 'U.S. Average' row in the table lists the 'Water for Training' as 5.439 million liters, which represents the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The context explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of their risk to health or safety. This is because the carbon footprint of AI models is often unrelated to their risk classification under the AI Act.","1","is_blank","[""ebert2024""]","is_blank","['Importantly, these assessments should not be limited to high-risk AI models but should also apply to all AI systems, regardless of the associated risk to health or safety.']","The context explicitly states that sustainability impact assessments (SIAs) should not be limited to high-risk AI models but should also apply to all AI systems, regardless of their risk to health or safety. This is because the carbon footprint of AI models is often unrelated to their risk classification under the AI Act."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context explicitly states that AWS improved its Water Use Effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, a 5% improvement from 0.19 L/kWh in 2022. This figure is directly cited in multiple instances within the provided documents.","0.18","L/kWh","[""amazon2023""]","is_blank","['improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.']","The context explicitly states that AWS improved its Water Use Effectiveness (WUE) to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023, a 5% improvement from 0.19 L/kWh in 2022. This figure is directly cited in multiple instances within the provided documents."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context explicitly states that local inference reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers. This is presented as a key sustainability measure in the deployment of large language models, directly supporting the claim in the question.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10].","The context explicitly states that local inference reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers. This is presented as a key sustainability measure in the deployment of large language models, directly supporting the claim in the question."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context explicitly states that training time is a critical factor in estimating both carbon emissions and cloud compute costs. Equation (1) in the context defines carbon emissions as a function of power consumption (P), training time (T), and carbon intensity (I), directly linking training time to cost estimation. Additionally, multiple studies cited in the context, such as Strubell et al. (2019), emphasize the importance of reporting training time to assess computational resources and cost-benefit trade-offs.","1","is_blank","[""luccioni2023"", ""strubell2019""]","[""https://arxiv.org/pdf/2304.01653.pdf"", ""https://arxiv.org/pdf/1906.02214.pdf""]","['C = P × T × I = E × I . (1)', 'Authors should report training time and sensitivity to hyperparameters.']","The context explicitly states that training time is a critical factor in estimating both carbon emissions and cloud compute costs. Equation (1) in the context defines carbon emissions as a function of power consumption (P), training time (T), and carbon intensity (I), directly linking training time to cost estimation. Additionally, multiple studies cited in the context, such as Strubell et al. (2019), emphasize the importance of reporting training time to assess computational resources and cost-benefit trade-offs."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","The context explicitly states that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% latency reduction with automated resource utilization overlapping enabled. This is directly supported by the text in multiple instances, including the description of Figure 14 and the discussion of batch size effects.","13.2","percent","[""chen2024""]","is_blank","As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.","The context explicitly states that the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% latency reduction with automated resource utilization overlapping enabled. This is directly supported by the text in multiple instances, including the description of Figure 14 and the discussion of batch size effects."
"q164","How much does an elephant weigh?","The context does not provide specific information about the weight of an elephant. While it mentions that the Western Ghats, a UNESCO World Heritage Site, is home to wild Asian elephants, it does not include any details about their weight.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the weight of an elephant. While it mentions that the Western Ghats, a UNESCO World Heritage Site, is home to wild Asian elephants, it does not include any details about their weight."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","The context provides energy consumption values for the five large NLP DNNs: T5 (85.7 MWh), Meena (232 MWh), GShard-600B (24.1 MWh), Switch Transformer (179 MWh), and GPT-3 (1,287 MWh). Among these, GPT-3 has the highest energy consumption at 1,287 MWh.","GPT-3","is_blank","[""patterson2021""]","is_blank","Energy Consumption (MWh) ... GPT-3 1,287","The context provides energy consumption values for the five large NLP DNNs: T5 (85.7 MWh), Meena (232 MWh), GShard-600B (24.1 MWh), Switch Transformer (179 MWh), and GPT-3 (1,287 MWh). Among these, GPT-3 has the highest energy consumption at 1,287 MWh."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context states that the BERT base model was trained on 16 TPU chips for 4 days (96 hours), as reported in multiple sources including Strubell et al. (2019). This directly answers the question about the duration of training BERT base.","4","days","[""strubell2019""]","[""https://arxiv.org/abs/1906.02214""]","The BERT base model (110M parameters) was trained on 16 TPU chips for 4 days (96 hours).","The context states that the BERT base model was trained on 16 TPU chips for 4 days (96 hours), as reported in multiple sources including Strubell et al. (2019). This directly answers the question about the duration of training BERT base."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","The context states that the Evolved Transformer, discovered via NAS, achieves the same accuracy as the vanilla Transformer (Big) model on the WMT English-to-German translation task while using 37% fewer parameters and 25% less energy expenditure. However, there is no information provided about the performance of the Transformer architecture compared to the Evolved Transformer as model sizes grow. Therefore, the claim about the Transformer eventually outperforming the Evolved Transformer as model sizes grow cannot be confirmed or denied based on the given context.","is_blank","is_blank","[""patterson2021"", ""so19""]","[""https://www.nature.com/articles/s41467-018-04068-0"", ""https://arxiv.org/pdf/1901.11117.pdf""]","['Figure 4 shows that the Evolved Transformer, found by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation.', 'The Evolved Transformer (Medium) reached the same accuracy as Transformer (Big) in [So19].']","The context states that the Evolved Transformer, discovered via NAS, achieves the same accuracy as the vanilla Transformer (Big) model on the WMT English-to-German translation task while using 37% fewer parameters and 25% less energy expenditure. However, there is no information provided about the performance of the Transformer architecture compared to the Evolved Transformer as model sizes grow. Therefore, the claim about the Transformer eventually outperforming the Evolved Transformer as model sizes grow cannot be confirmed or denied based on the given context."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context mentions that the Burst-GPT dataset was used for short-form conversations in real-world LLM workloads, and it references the dataset as being used in the study (Wang et al., 2024). However, the context does not specify that this dataset contains 5,842 labeled entries. No other dataset in the provided context is described with this exact number of labeled entries. Therefore, the specific dataset with 5,842 labeled entries cannot be identified from the given information.","Unable to answer with confidence based on the provided documents.","is_blank","[""fernandez2025""]","is_blank","['is_blank']","The context mentions that the Burst-GPT dataset was used for short-form conversations in real-world LLM workloads, and it references the dataset as being used in the study (Wang et al., 2024). However, the context does not specify that this dataset contains 5,842 labeled entries. No other dataset in the provided context is described with this exact number of labeled entries. Therefore, the specific dataset with 5,842 labeled entries cannot be identified from the given information."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The context explicitly states that using eight T4 spot instances can be more cost-efficient than a DGX-2 node, particularly for computer vision models. This is supported by cost analysis showing that the 8xT4 setup is 58% cheaper than the DGX-2 while being 37% slower, indicating a favorable cost-to-performance tradeoff. The conclusion section reiterates this finding, confirming that distributed training with eight T4 instances can be more cost-efficient than using a DGX-2.","1","is_blank","[""erben2023""]","is_blank","['Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs.', 'However, for CV, we manage to provide two new offerings: First, the 8xA10, which is both 50% faster and 49% cheaper than the DGX-2, and 8xT4, which is 58% cheaper than DGX-2, while being 37% slower (Figure 1).']","The context explicitly states that using eight T4 spot instances can be more cost-efficient than a DGX-2 node, particularly for computer vision models. This is supported by cost analysis showing that the 8xT4 setup is 58% cheaper than the DGX-2 while being 37% slower, indicating a favorable cost-to-performance tradeoff. The conclusion section reiterates this finding, confirming that distributed training with eight T4 instances can be more cost-efficient than using a DGX-2."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage. This is directly supported by multiple references, including [ref_id=luccioni2025b], which confirms the absence of sustainability considerations in the order.","0","is_blank","[""luccioni2025b""]","is_blank","Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI’s greenhouse gas emissions nor energy usage, as well as multi-national declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.","The context explicitly states that the 2023 US Executive Order regarding AI did not mention AI’s greenhouse gas emissions nor energy usage. This is directly supported by multiple references, including [ref_id=luccioni2025b], which confirms the absence of sustainability considerations in the order."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","The context explicitly states that under Germany's 2023 Energy Efficiency Act, data centers are required to run on 50% renewable energy, with that factor increasing to 100% by January 1, 2027. This directly supports the claim in the question.","1","is_blank","[""ebert2024""]","is_blank","Most importantly, it sets targets on energy efficiency and renewable energy use, requiring data centers to reach a PUE factor between 1.5 and 1.2 and an ERF of 10% to 20 % depending on their age (Sec. 11), and to run on 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027 (Sec. 11).","The context explicitly states that under Germany's 2023 Energy Efficiency Act, data centers are required to run on 50% renewable energy, with that factor increasing to 100% by January 1, 2027. This directly supports the claim in the question."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","The context states that 90% of ACL papers target accuracy, and only a small portion (10%) argue for a new efficiency result. Since the sample size is 60 papers, and the papers targeting both accuracy and efficiency are not explicitly mentioned, the number of papers targeting both cannot be determined from the provided information.","is_blank","papers","[""schwartz2019""]","[""https://acl2018.org"", ""https://nips.cc/Conferences/2018"", ""http://cvpr2019.thecvf.com""]","As shown in Figure 2, in all conferences we considered, a large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efficiency result.","The context states that 90% of ACL papers target accuracy, and only a small portion (10%) argue for a new efficiency result. Since the sample size is 60 papers, and the papers targeting both accuracy and efficiency are not explicitly mentioned, the number of papers targeting both cannot be determined from the provided information."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","The context explicitly states that recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use. This is supported by multiple sources within the provided documents, including Jegham et al. (2025) and Luccioni et al. (2024), which cite inference as making up 80–90% of total ML cloud computing demand and a significant portion of energy use.","90","percent","[""jegham2025"", ""luccioni2024""]","[""https://arxiv.org/abs/2505.09598v6"", ""https://arxiv.org/abs/2406.02547""]","['Recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use [14, 15].', 'According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28]']","The context explicitly states that recent estimates suggest inference can account for up to 90% of a model’s total lifecycle energy use. This is supported by multiple sources within the provided documents, including Jegham et al. (2025) and Luccioni et al. (2024), which cite inference as making up 80–90% of total ML cloud computing demand and a significant portion of energy use."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The context indicates that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is described as a crucial omission. While the Act requires reporting of energy consumption for model development, it fails to address inference energy consumption. The authors propose a novel interpretation to include inference in reporting, but this is not currently mandated by the Act.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.']","The context indicates that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is described as a crucial omission. While the Act requires reporting of energy consumption for model development, it fails to address inference energy consumption. The authors propose a novel interpretation to include inference in reporting, but this is not currently mandated by the Act."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","The context explicitly states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is described as a crucial omission. This directly contradicts the claim in the question that the Act currently requires such reporting.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['For example, the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.']","The context explicitly states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, which is described as a crucial omission. This directly contradicts the claim in the question that the Act currently requires such reporting."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context explicitly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. This directly contradicts the claim that they rely on air cooling. Therefore, the statement is false.","0","is_blank","[""li2025b""]","is_blank","['In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.']","The context explicitly states that new data centers dedicated to AI training often rely on liquid cooling due to high server power densities. This directly contradicts the claim that they rely on air cooling. Therefore, the statement is false."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","The context states that platform-level caching improves power efficiency by 6.7× when starting from a CPU server baseline. This value is explicitly mentioned in the section discussing the cross-lingual Transformer language model optimization.","6.7","multiplier","[""wu2021a""]","is_blank","Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7 ×.","The context states that platform-level caching improves power efficiency by 6.7× when starting from a CPU server baseline. This value is explicitly mentioned in the section discussing the cross-lingual Transformer language model optimization."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","The context provides a direct estimate of CO2 emissions for training a BERT base model using 64 V100 GPUs for 79 hours. According to Table 3 in reference [ref_id=strubell2019], the CO2 emissions for this configuration are listed as 1438 lbs. This value is explicitly derived from the power draw of the hardware, training duration, and PUE adjustments.","1438","lbs","[""strubell2019""]","[""https://doi.org/10.18653/v1/2019/acl-ijcnlp-19/papers/A19-1047.pdf""]","BERTbase V100x64 12,041.51 79 1507 1438 $3751–$12,571","The context provides a direct estimate of CO2 emissions for training a BERT base model using 64 V100 GPUs for 79 hours. According to Table 3 in reference [ref_id=strubell2019], the CO2 emissions for this configuration are listed as 1438 lbs. This value is explicitly derived from the power draw of the hardware, training duration, and PUE adjustments."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","The context consistently states that ML inference accounts for 80–90% of total compute demand. Multiple sources, including Chung et al. (2025), Luccioni et al. (2024), and Fernandez et al. (2025), cite this range. The most direct evidence comes from Chung et al., who state that ML inference reportedly accounts for 80–90% of total compute demand.","80–90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","[""https://ml.energy/leaderboard"", ""https://arxiv.org/abs/2505.06371v2"", ""https://arxiv.org/abs/2504.17674v1""]","['ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60].', 'inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28]', 'between 80 to 90% of ML AWS cloud computing demand (Barr, 2019; Leopold, 2019).']","The context consistently states that ML inference accounts for 80–90% of total compute demand. Multiple sources, including Chung et al. (2025), Luccioni et al. (2024), and Fernandez et al. (2025), cite this range. The most direct evidence comes from Chung et al., who state that ML inference reportedly accounts for 80–90% of total compute demand."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","The context states that training a 6.1B-parameter language model for 8 days on 256 NVIDIA A100s consumed 13.8 MWh of energy, which is only 13% of the full training run. The full training run is estimated to take 60 days, and the total energy consumption is calculated as (60/8) × 13.8 MWh = 103.5 MWh, or 103,500 kWh. This energy consumption is equivalent to approximately 103,500 kWh, which can be compared to U.S. household electricity consumption. The average U.S. household consumes about 10,000 kWh per year, so 103,500 kWh corresponds to roughly 10.35 household-years. However, the context does not provide a direct conversion or explicit statement of household-years, so the exact value cannot be confirmed from the given data.","is_blank","household-years","[""dodge2022""]","is_blank","['We estimate a full training run would consume approximately 103,593 kWh.']","The context states that training a 6.1B-parameter language model for 8 days on 256 NVIDIA A100s consumed 13.8 MWh of energy, which is only 13% of the full training run. The full training run is estimated to take 60 days, and the total energy consumption is calculated as (60/8) × 13.8 MWh = 103.5 MWh, or 103,500 kWh. This energy consumption is equivalent to approximately 103,500 kWh, which can be compared to U.S. household electricity consumption. The average U.S. household consumes about 10,000 kWh per year, so 103,500 kWh corresponds to roughly 10.35 household-years. However, the context does not provide a direct conversion or explicit statement of household-years, so the exact value cannot be confirmed from the given data."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context explicitly states that in NLP experiments, egress costs can account for more than 90% of the total cost per VM. For example, in Google Cloud (GC), the external egress cost for NLP is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This directly supports the claim in the question.","1","is_blank","[""erben2023""]","is_blank","For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h).","The context explicitly states that in NLP experiments, egress costs can account for more than 90% of the total cost per VM. For example, in Google Cloud (GC), the external egress cost for NLP is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). This directly supports the claim in the question."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. The model was trained on a cluster with 96 H100 GPUs. To estimate the wall-clock time in days, divide the total GPU hours by the number of GPUs and then by 24 hours per day. This yields 30,000 / 96 / 24 = 12.5 days.","12.5","days","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. The model was trained on a cluster with 96 H100 GPUs. To estimate the wall-clock time in days, divide the total GPU hours by the number of GPUs and then by 24 hours per day. This yields 30,000 / 96 / 24 = 12.5 days."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context explicitly defines 'water consumption' as 'water withdrawal minus water discharge', and describes it as the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment. This matches the definition provided in the question.","Water consumption","is_blank","[""li2025b""]","is_blank","['Water consumption:It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13].']","The context explicitly defines 'water consumption' as 'water withdrawal minus water discharge', and describes it as the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment. This matches the definition provided in the question."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context explicitly states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt across GPU shard configurations, from 8 GPUs (lower end) to 32 GPUs (higher end). This range is directly mentioned in the text and supported by the discussion of energy costs across different shard configurations in Figures 4 and 5.","300 to 1000","W","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context explicitly states that the energy per second for inference with LLaMA 65B ranges from 300 Watts to 1 Kilowatt across GPU shard configurations, from 8 GPUs (lower end) to 32 GPUs (higher end). This range is directly mentioned in the text and supported by the discussion of energy costs across different shard configurations in Figures 4 and 5."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This directly implies that the 72B version consumes seven times more energy than the 7B version.","7","multiplier","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context states that the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B). This directly implies that the 72B version consumes seven times more energy than the 7B version."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","The context provides carbon emission values for Qwen before and after optimization. Before optimization, Qwen's carbon emissions were 0.009 kg CO2 per inference task. After applying quantization and local inference, the emissions decreased to 0.004 kg CO2 per task. The percentage reduction is calculated as ((0.009 - 0.004) / 0.009) * 100 = 55.56%, which aligns with the claim in the text that reductions of up to 45% were achieved across models, with some exceeding this threshold.","55.56","percent","[""khan2025""]","is_blank","Qwen 0.77 0.79 0.76 0.79 0.009
After Optimization
Qwen 0.80 0.81 0.80 0.81 0.004","The context provides carbon emission values for Qwen before and after optimization. Before optimization, Qwen's carbon emissions were 0.009 kg CO2 per inference task. After applying quantization and local inference, the emissions decreased to 0.004 kg CO2 per task. The percentage reduction is calculated as ((0.009 - 0.004) / 0.009) * 100 = 55.56%, which aligns with the claim in the text that reductions of up to 45% were achieved across models, with some exceeding this threshold."
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion, and that it benchmarks across a wide range of tasks including LLM chat, VLM visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models. However, the exact number of widely used model architectures included in the latest iteration is not specified in the provided text.","Unable to answer with confidence based on the provided documents.","models","[""chung2025""]","[""https://github.com/ml-energy/benchmark""]","Model. The ML.ENERGY Benchmark already supports various popular architectures like Llama [73], LLaV A [43], Stable Diffusion [25], and Stable Video Diffusion [14] (See Appendix A for a full list).","The context mentions that the ML.ENERGY Benchmark supports various popular architectures like Llama, LLaVA, Stable Diffusion, and Stable Video Diffusion, and that it benchmarks across a wide range of tasks including LLM chat, VLM visual chat, and text-to-image, text-to-video, and image-to-video generation using Diffusion models. However, the exact number of widely used model architectures included in the latest iteration is not specified in the provided text."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context provides a table (Table 2) that lists the public health cost of training a large AI model in various U.S. data centers. For Altoona, Iowa, the health cost is listed as $2.51 million, which exceeds 120% of the training electricity cost. This value is explicitly stated in the table and directly answers the question.","2.51","USD","[""han2024""]","is_blank","Table 2: The public health cost of training a large AI model in selected U.S. data centers. Location: Altoona, IA; Health Cost (million$): 2.51(1.84, 3.17)","The context provides a table (Table 2) that lists the public health cost of training a large AI model in various U.S. data centers. For Altoona, Iowa, the health cost is listed as $2.51 million, which exceeds 120% of the training electricity cost. This value is explicitly stated in the table and directly answers the question."
