"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014. However, it does not provide specific information about the average increase in U.S. data center electricity consumption between 2010 and 2014.","is_blank","percent","[""wu2021b""]","is_blank","is_blank","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014. However, it does not provide specific information about the average increase in U.S. data center electricity consumption between 2010 and 2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context does not provide information on the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid in 2023.","is_blank","cars","[""is_blank""]","is_blank","is_blank","The context does not provide information on the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid in 2023."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, but it does not provide a specific number of data centers that use recycled water for cooling in relation to the total number of data centers. Therefore, the exact number of data centers using recycled water for cooling in 2023 cannot be determined.","is_blank","data centers","[""is_blank""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore. [ref_id=amazon2023]","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, but it does not provide a specific number of data centers that use recycled water for cooling in relation to the total number of data centers. Therefore, the exact number of data centers using recycled water for cooling in 2023 cannot be determined."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context does not provide specific information about the embodied carbon emissions of GPUs in kg per GPU.","is_blank","kg/GPU","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the embodied carbon emissions of GPUs in kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context does not provide information about the estimated amortized training cost of GPT-4 or the total training budget for FLM-101B, making it impossible to compare the two.","is_blank","ratio","[""is_blank""]","is_blank","is_blank","The context does not provide information about the estimated amortized training cost of GPT-4 or the total training budget for FLM-101B, making it impossible to compare the two."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","","","tCO2e","[]","is_blank","is_blank",""
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context does not provide the final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard. It only mentions that FLM-101B outperforms GLM-130B and obtains competitive results compared to GPT-3 in some tasks with much lower costs, but it does not specify the exact scores.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not provide the final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard. It only mentions that FLM-101B outperforms GLM-130B and obtains competitive results compared to GPT-3 in some tasks with much lower costs, but it does not specify the exact scores."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context provides the information about the first commercially-produced microprocessor - Intel 4004, which was manufactured in 10,000 nm process technology in 1971 and ran at 740kHZ with 2,250 transistors. The typical microprocessor in 2021 is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz with more than 3.9 billion transistors.","5000000","fold","[""wu2021b""]","is_blank","[Intel, 2019, AMD, 2020]","The context provides the information about the first commercially-produced microprocessor - Intel 4004, which was manufactured in 10,000 nm process technology in 1971 and ran at 740kHZ with 2,250 transistors. The typical microprocessor in 2021 is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz with more than 3.9 billion transistors."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context mentions that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS for GPT-3 training. The question asks for the number of days, so we can use the provided information.","14.8","days","[""patterson2021""]","is_blank","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS for GPT-3 training. [patterson2021]","The context mentions that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS for GPT-3 training. The question asks for the number of days, so we can use the provided information."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context does not provide specific information about GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model.","is_blank","kWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context does not provide specific information about the total permitted annual emission limits for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024.","is_blank","tons","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total permitted annual emission limits for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated). This implies that training from scratch would take 21.54 * (1 - 0.72) = 5.98 days, or approximately 6 days.","6","percent","[""li2025a""]","is_blank","[ref_id=li2025a] Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated). This implies that training from scratch would take 21.54 * (1 - 0.72) = 5.98 days, or approximately 6 days."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context does not provide specific information about the number of premature deaths in 2030 caused by scope-2 pollutants from U.S. data centers alone.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the number of premature deaths in 2030 caused by scope-2 pollutants from U.S. data centers alone."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context does not provide specific information about the training time for the 6.1 billion parameter model, but it does mention that a 6 billion parameter transformer was trained for 8 days on 256 NVIDIA A100s. Assuming similar parameters, a full training run of the 6.1 billion parameter model would take approximately 103,593 kWh.","103593","days","[""dodge2022""]","is_blank","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context does not provide specific information about the training time for the 6.1 billion parameter model, but it does mention that a 6 billion parameter transformer was trained for 8 days on 256 NVIDIA A100s. Assuming similar parameters, a full training run of the 6.1 billion parameter model would take approximately 103,593 kWh."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context does not provide information about the specific year the One Hundred Year Study on Artificial Intelligence was launched.","is_blank","year","[""is_blank""]","is_blank","is_blank","The context does not provide information about the specific year the One Hundred Year Study on Artificial Intelligence was launched."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","According to the UN's Global E-Waste Monitor 2024, about 22% of e-waste has been shown to be formally collected and recycled.","0.22","percent","[""luccioni2025a""]","is_blank","The global e-waste monitor 2024. United Nations University (UNU), International Telecommunication Union (ITU) & International Solid Waste Associ- ation (ISW A), Bonn/Geneva/Vienna(2024), 1–109.","According to the UN's Global E-Waste Monitor 2024, about 22% of e-waste has been shown to be formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context does not provide specific information about the energy consumption in MWh for pre-training the BLOOM model.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the energy consumption in MWh for pre-training the BLOOM model."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context does not provide information about the percentage of the Switch Transformer's 1500 billion parameters that are activated per token.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about the percentage of the Switch Transformer's 1500 billion parameters that are activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B is based on a Sparsely-gated Mixture-of-Experts (SMoE) architecture, which consists of attention and feedforward experts, both of which are sparsely activated. However, the exact number of experts in each layer is not specified in the provided context.","is_blank","experts","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B is based on a Sparsely-gated Mixture-of-Experts (SMoE) architecture, which consists of attention and feedforward experts, both of which are sparsely activated. However, the exact number of experts in each layer is not specified in the provided context."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide specific information about the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs attributed to the English portion of the model. However, the context does not explicitly state the cost required to train only the English portion.","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","According to the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs attributed to the English portion of the model. However, the context does not explicitly state the cost required to train only the English portion."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context does not provide information on the number of machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context mentions that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3x, but it does not specify the initial carbon footprint or the unit of the multiplier.","is_blank","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context mentions that increasing GPU utilization up to 80% decreases the overall carbon footprint by 3x, but it does not specify the initial carbon footprint or the unit of the multiplier."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is 1.2x to 4x larger than the final training run, and that this factor was sampled from a log-normal distribution. However, it does not provide a specific value for the total compute for model development for any specific model.","is_blank","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context states that the total compute for model development is 1.2x to 4x larger than the final training run, and that this factor was sampled from a log-normal distribution. However, it does not provide a specific value for the total compute for model development for any specific model."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides the energy consumption in kWh for training a 6 billion parameter transformer model, which is 103,593 kWh. Since the question asks for the energy consumption in MWh, we divide the given value by 1000 to get the answer.","103.593","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context provides the energy consumption in kWh for training a 6 billion parameter transformer model, which is 103,593 kWh. Since the question asks for the energy consumption in MWh, we divide the given value by 1000 to get the answer."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper argues that understanding the rebound effects of AI requires an interdisciplinary approach that includes both qualitative and quantitative methods drawn from computer science, economics, and the social sciences. It mentions Jevons' Paradox as a concept that explains how efficiency gains in AI may not lead to net environmental benefits due to increased consumption spurred by these efficiency gains.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI’s impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI’s true climate footprint, limiting the scope for meaningful interventions.","The paper argues that understanding the rebound effects of AI requires an interdisciplinary approach that includes both qualitative and quantitative methods drawn from computer science, economics, and the social sciences. It mentions Jevons' Paradox as a concept that explains how efficiency gains in AI may not lead to net environmental benefits due to increased consumption spurred by these efficiency gains."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in 2027. However, it does not explicitly specify this value in terms of the answer unit (billion cubic meters).","is_blank","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom. This is concerning, as freshwater scarcity has become one of the most pressing challenges. To respond to the global water challenges, AI can, and also must, take social responsibility and lead by example by addressing its own water footprint. In this paper, we provide a principled methodology to estimate the water footprint of AI, and also discuss the unique spatial-temporal diversities of AI’s runtime water efficiency. Finally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to enable truly sustainable AI.","The context states that the global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in 2027. However, it does not explicitly specify this value in terms of the answer unit (billion cubic meters)."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise, and that it is associated with the use of massive computational power to achieve state-of-the-art results. It also mentions that there are diminishing returns of increased cost in AI research. However, it does not directly state that Red AI is declining.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context states that Red AI is on the rise, and that it is associated with the use of massive computational power to achieve state-of-the-art results. It also mentions that there are diminishing returns of increased cost in AI research. However, it does not directly state that Red AI is declining."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context states that the total time cost for training FLM-101B is 21.54 days, which is a 3.56x speedup compared to training a 101B model from scratch.","21.54","days","[""li2025a""]","is_blank","[ref_id=li2025a] Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days, which is a 3.56x speedup compared to training a 101B model from scratch."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","0","is_blank","[""wu2021a""]","is_blank","[ref_id=wu2021a] A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","The context states that a vast majority of model experimentation utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippet from [ref_id=jegham2025] states that GPT-3 consumes 1,287 MWh of electricity during its training phase. This is the only directly relevant context found.","1287","MWh","[""jegham2025""]","is_blank","GPT-3 consumes 1,287 MWh of electricity during its training phase. [jegham2025]","The context snippet from [ref_id=jegham2025] states that GPT-3 consumes 1,287 MWh of electricity during its training phase. This is the only directly relevant context found."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context discusses the AI Energy Score project, which provides a standardized methodology for comparing models across different tasks, including inference efficiency. However, it does not provide a specific name for the collaborative project.","is_blank","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context discusses the AI Energy Score project, which provides a standardized methodology for comparing models across different tasks, including inference efficiency. However, it does not provide a specific name for the collaborative project."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context does not provide the execution time for the longest kernel of the MoE layer specifically for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context does not provide the execution time for the longest kernel of the MoE layer specifically for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B is based on a Sparsely-gated Mixture-of-Experts (SMoE) architecture, which consists of attention and feedforward experts, and both layers are sparsely activated. However, the specifics of how many experts are selected for activation per token are not provided in the context.","is_blank","experts","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B is based on a Sparsely-gated Mixture-of-Experts (SMoE) architecture, which consists of attention and feedforward experts, and both layers are sparsely activated. However, the specifics of how many experts are selected for activation per token are not provided in the context."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context from Schwartz et al. (2019) states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, which supports the claim of increasingly large and computationally-intensive deep learning models.","1","is_blank","[""schwartz2019""]","is_blank","[ref_id=schwartz2019] Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context from Schwartz et al. (2019) states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, which supports the claim of increasingly large and computationally-intensive deep learning models."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from [ref_id=wu2021b] states that the global carbon emissions for 2020 dropped by 6.4%.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context snippet from [ref_id=wu2021b] states that the global carbon emissions for 2020 dropped by 6.4%."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources. However, it does not specify the year in which this was achieved.","is_blank","data centers","[""amazon2023""]","is_blank","Data Centers Powered with Renewable Energy  Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022. Across Amazon, we work with utilities and regulators on green tariffs so that more companies can buy clean energy directly from renewable energy projects.","The context states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources. However, it does not specify the year in which this was achieved."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context does not provide specific information about the age of the field of Artificial Intelligence in 2025. However, it does mention that the field of AI was officially born and christened at a 1956 workshop, and that the report is a reflection on progress in the past fifteen years and anticipates developments in the coming fifteen years.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the age of the field of Artificial Intelligence in 2025. However, it does mention that the field of AI was officially born and christened at a 1956 workshop, and that the report is a reflection on progress in the past fifteen years and anticipates developments in the coming fifteen years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context provides information about a study by Strubell et al. that estimated the carbon emissions of training AI models, including the NAS approach that emitted '5 times the emissions of a car during its lifetime, including fuel'. This NAS training workload is the basis for the 'five cars' estimate that has been misinterpreted as a proxy for the carbon footprint of training AI models at large.","NAS approach for model development","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context provides information about a study by Strubell et al. that estimated the carbon emissions of training AI models, including the NAS approach that emitted '5 times the emissions of a car during its lifetime, including fuel'. This NAS training workload is the basis for the 'five cars' estimate that has been misinterpreted as a proxy for the carbon footprint of training AI models at large."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context does not provide specific information about the energy use decrease percentage for the Llama 3.1 8B model when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the energy use decrease percentage for the Llama 3.1 8B model when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context does not provide information about the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide information about the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that as of January 2024, Amazon had announced 270 solar rooftops at their facilities and stores, but it does not provide specific information about the energy storage capacity of these rooftops.","is_blank","GW","[""is_blank""]","is_blank","is_blank","The context states that as of January 2024, Amazon had announced 270 solar rooftops at their facilities and stores, but it does not provide specific information about the energy storage capacity of these rooftops."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual emissions of GPT-4o are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London. It also mentions that these figures are based on electricity usage.","272","flights","[""jegham2025""]","is_blank","We further examine GPT-4o’s environmental footprint through estimated carbon emissions from electricity usage, as seen in Figure 5. Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual emissions of GPT-4o are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London. It also mentions that these figures are based on electricity usage."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide information about the percentage of AI inference workloads in Asia powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about the percentage of AI inference workloads in Asia powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context does not provide specific information about the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023. It does mention that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but it does not specify if this includes AI-dedicated data centers.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023. It does mention that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but it does not specify if this includes AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","","","parameters","[]","is_blank","is_blank",""
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context does not provide specific GHG emissions for the pre-training of the Llama 7B model in tCO2e. However, it does provide a range of GHG emissions for pre-training various models, including the Llama 7B model, in MWh which can be converted to tCO2e using a carbon intensity factor.","is_blank","tCO2e","[""luccioni2025c""]","is_blank","Range of Pre-Training Environmental Impacts (Representative Models Displayed) Table in [luccioni2025c]. Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e) Llama 7B 63 Meta 14","The context does not provide specific GHG emissions for the pre-training of the Llama 7B model in tCO2e. However, it does provide a range of GHG emissions for pre-training various models, including the Llama 7B model, in MWh which can be converted to tCO2e using a carbon intensity factor."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context mentions that the Amazon fleet included 11,800 electric delivery vans from Rivian in the U.S. in 2023, and 2,600 in 2022. The difference between these two numbers gives the total number of electric delivery vans added in the U.S. across 2022 and 2023.","9200","electric delivery vans","[""amazon2023""]","is_blank","Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context mentions that the Amazon fleet included 11,800 electric delivery vans from Rivian in the U.S. in 2023, and 2,600 in 2022. The difference between these two numbers gives the total number of electric delivery vans added in the U.S. across 2022 and 2023."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This implies that GHG emissions from servers and data centers cooling are indeed included in operational environmental impacts.","1","is_blank","[""morrison2025""]","is_blank","3.1 O PERATIONAL IMPACTS Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This implies that GHG emissions from servers and data centers cooling are indeed included in operational environmental impacts."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides the energy consumption of various models, including LLaMA-3.1-8B which consumes 0.443 Wh for long prompts. However, the specific prompt size for 'long' is not defined, so the answer is based on the closest available definition in the context.","0.443","Wh","[""jegham2025""]","is_blank","[ref_id=jegham2025] LLaMA-3.1-8B is the most efficient, requiring only 0.443 Wh for long prompts (approximately 7,000 words of input and 1,000 words of output), followed by LLaMA-3.2 1B and LLaMA-3.2 3B at 0.552 Wh and 0.707 Wh, respectively.","The context provides the energy consumption of various models, including LLaMA-3.1-8B which consumes 0.443 Wh for long prompts. However, the specific prompt size for 'long' is not defined, so the answer is based on the closest available definition in the context."
"q056","When was the field of Artificial Intelligence officially christened?","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop, as stated in the proposal co-authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. This is mentioned in multiple parts of the context.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. (stone2022)","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop, as stated in the proposal co-authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. This is mentioned in multiple parts of the context."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people worldwide do not have access to a stable supply of electricity.","770000000","is_blank","[""wu2021b""]","is_blank","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states that approximately 770 million people worldwide do not have access to a stable supply of electricity."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context provides information about the energy per output token estimates of LLaMA 65B across batch sizes and maximum generation lengths. Specifically, it mentions that for a maximum generation length of 512, it takes about 3-4 Joules for an output token. However, it does not explicitly state the energy per token for a specific batch size.","is_blank","joules per token","[""samsi2024""]","is_blank","Fig. 6: Energy per output token estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512 : inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.","The context provides information about the energy per output token estimates of LLaMA 65B across batch sizes and maximum generation lengths. Specifically, it mentions that for a maximum generation length of 512, it takes about 3-4 Joules for an output token. However, it does not explicitly state the energy per token for a specific batch size."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the RM2 model size was reduced by 15%.","15","percent","[""wu2021a""]","is_blank","[ref_id=wu2021a] By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the RM2 model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context includes a statement from a 2021 Boston Consulting Group (BCG) report that AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030, but the reasoning behind this estimate is unclear and the underlying calculations are not detailed. The same number appears in a more recent BCG report from 2023, but the report does not offer specific calculations translating individual project numbers to a global scale.","is_blank","is_blank","[""luccioni2025c"", ""luccioni2025c""]","is_blank","is_blank","The context includes a statement from a 2021 Boston Consulting Group (BCG) report that AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030, but the reasoning behind this estimate is unclear and the underlying calculations are not detailed. The same number appears in a more recent BCG report from 2023, but the report does not offer specific calculations translating individual project numbers to a global scale."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from Patterson et al. (2021) states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This implies that sparsely activated DNNs are more energy-efficient than their dense counterparts.","1","is_blank","[""patterson2021""]","is_blank","We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): ● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context from Patterson et al. (2021) states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This implies that sparsely activated DNNs are more energy-efficient than their dense counterparts."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. However, the exact percentage is not provided, so the answer is_blank.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. However, the exact percentage is not provided, so the answer is_blank."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides a table with water consumption data for training and making a request to GPT-3. The table includes on-site and off-site water usage, PUE, WUE, and the number of requests for 500ml. By adding up the operational and embodied water footprints, we can obtain the total water footprint.","29.6","MWh","[""luccioni2024""]","is_blank","Table 1 in the context","The context provides a table with water consumption data for training and making a request to GPT-3. The table includes on-site and off-site water usage, PUE, WUE, and the number of requests for 500ml. By adding up the operational and embodied water footprints, we can obtain the total water footprint."
"q067","What was the average global data center PUE in 2023?","The context provides the average global data center PUE in 2023 as 1.58. The Power Usage Effectiveness (PUE) metric reflects the energy efficiency of a data center, indicating the ratio of the total energy needed by a data center to the energy used solely by computational devices. A PUE of 1.0 would imply ideal efficiency, meaning that the data center uses only the energy necessary to power the computational devices.","1.58","PUE","[""ebert2024""]","is_blank","[ref_id=ebert2024] The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context provides the average global data center PUE in 2023 as 1.58. The Power Usage Effectiveness (PUE) metric reflects the energy efficiency of a data center, indicating the ratio of the total energy needed by a data center to the energy used solely by computational devices. A PUE of 1.0 would imply ideal efficiency, meaning that the data center uses only the energy necessary to power the computational devices."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","","","wind turbines","[]","is_blank","is_blank",""
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context provides a breakdown of total amortized model development costs for selected models, including Gemini Ultra. The R&D staff costs, including equity, are mentioned to be 49% for Gemini Ultra.","49","percent","[""cottier2024""]","is_blank","Breakdown of total amortized model development costs for selected models, with equity included in the R&D staff cost. (a) Hardware costs are amortized to the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based on median estimates.","The context provides a breakdown of total amortized model development costs for selected models, including Gemini Ultra. The R&D staff costs, including equity, are mentioned to be 49% for Gemini Ultra."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context mentions the formation of a 17-member Study Panel for the One Hundred Year Study on AI in mid-fall 2015.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry , and AI-savvy scholars in law , political science, policy , and economics, was launched in mid-fall 2015.","The context mentions the formation of a 17-member Study Panel for the One Hundred Year Study on AI in mid-fall 2015."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. However, it does not provide a specific percentage for the manufacturing carbon footprint of a client device's total carbon footprint.","is_blank","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. However, it does not provide a specific percentage for the manufacturing carbon footprint of a client device's total carbon footprint."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context does not provide information about the relationship between the number of model parameters and energy consumption during inference.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about the relationship between the number of model parameters and energy consumption during inference."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide specific information about OpenAI's API requests CO2 emissions in January 2024.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about OpenAI's API requests CO2 emissions in January 2024."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context mentions that training the Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate. However, it does not explicitly state the GHG emissions (tCO2e) from the pre-training process specifically.","is_blank","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.","The context mentions that training the Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate. However, it does not explicitly state the GHG emissions (tCO2e) from the pre-training process specifically."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI has driven 2.9x capacity increases for AI training at Facebook over the recent 18 months. However, it does not provide a specific time period or a comparison factor for the increase in AI infrastructure capacity.","is_blank","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that the explosive growth in AI has driven 2.9x capacity increases for AI training at Facebook over the recent 18 months. However, it does not provide a specific time period or a comparison factor for the increase in AI infrastructure capacity."
"q079","How many miles is the Earth from the Sun?","The context does not provide information about the distance between the Earth and the Sun.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide information about the distance between the Earth and the Sun."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context mentions that AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match. This implies that AlphaGo defeated the human Go champion.","1","is_blank","[""stone2022""]","is_blank","For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game. --- AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning.","The context mentions that AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match. This implies that AlphaGo defeated the human Go champion."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process takes 60 H100 GPU hours, which includes both dSFT and dDPO fine-tuning for the JetMoE-8B model.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process takes 60 H100 GPU hours, which includes both dSFT and dDPO fine-tuning for the JetMoE-8B model."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context provides a table (Table VI) comparing instance selection results for an offline workload with a 100 TPS SLO. It shows that the Max-Performance policy selected the g6e.xlarge instance, which is 2.699$ while InferSave selected the g4dn.xlarge instance, which is 2.13$. Therefore, the Max-Performance policy selected an instance that was approximately (2.699-2.13)/2.13 * 100% = 26.67% more expensive.","26.67","percent","[""kim2025""]","is_blank","TABLE VI Comparison of Instance Selection Results by SLO Constraints (100 TPS and 200 TPS)","The context provides a table (Table VI) comparing instance selection results for an offline workload with a 100 TPS SLO. It shows that the Max-Performance policy selected the g6e.xlarge instance, which is 2.699$ while InferSave selected the g4dn.xlarge instance, which is 2.13$. Therefore, the Max-Performance policy selected an instance that was approximately (2.699-2.13)/2.13 * 100% = 26.67% more expensive."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context provides the exact value for the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, which generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","[ref_id=luccioni2024] For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences,","The context provides the exact value for the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, which generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides a table (Table 1) that lists the energy consumption of GPT-4o in kWh for a short prompt, which is 0.42 Wh (±0.13 Wh). However, the context does not provide the energy consumption for performing 1,000 inference queries specifically.","is_blank","Wh","[""jegham2025""]","is_blank","Table 1 from the document: 'Energy consumption of GPT-4o for a short prompt: 0.42 Wh (±0.13 Wh)'","The context provides a table (Table 1) that lists the energy consumption of GPT-4o in kWh for a short prompt, which is 0.42 Wh (±0.13 Wh). However, the context does not provide the energy consumption for performing 1,000 inference queries specifically."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context discusses the interdependence of AI ethics and sustainability, and the necessity of integrating the two for a holistic societal impact. It does not explicitly state whether a universal, one-size-fits-all approach can be developed, but it does imply that a nuanced and context-specific approach is needed.","is_blank","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context discusses the interdependence of AI ethics and sustainability, and the necessity of integrating the two for a holistic societal impact. It does not explicitly state whether a universal, one-size-fits-all approach can be developed, but it does imply that a nuanced and context-specific approach is needed."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The carbon intensity of energy according to the U.S. average mix in 2021 is 0.429 kg of CO2e/KWh, as stated in the context from [USE21].","0.429","kg of CO2e/KWh","[""USE21""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The carbon intensity of energy according to the U.S. average mix in 2021 is 0.429 kg of CO2e/KWh, as stated in the context from [USE21]."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context does not provide information about a specific decentralized PyTorch-based framework used for enabling distributed spot instance training across clouds and continents.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about a specific decentralized PyTorch-based framework used for enabling distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context discusses the concept of 'social transparency' in AI systems, which involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint. This aligns with the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system.","Social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [ 56]. Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.","The context discusses the concept of 'social transparency' in AI systems, which involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint. This aligns with the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that for emotion classification tasks, the linear model with sentence embeddings is among the top-performing models.","is_blank","is_blank","[""zschache2025""]","is_blank","Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models.","The context states that for emotion classification tasks, the linear model with sentence embeddings is among the top-performing models."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context does not provide the name of the LLM inference system developed in the 2025 Chen et al. paper.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide the name of the LLM inference system developed in the 2025 Chen et al. paper."
"q093","How many parameters does the largest T5 model have?","The context mentions a T5 model with 6 billion parameters that emits a significant amount of CO2, even when only partially trained. However, it does not provide specific information about the number of parameters of the largest T5 model.","is_blank","parameters","[""dodge2022""]","is_blank","is_blank","The context mentions a T5 model with 6 billion parameters that emits a significant amount of CO2, even when only partially trained. However, it does not provide specific information about the number of parameters of the largest T5 model."
"q094","What is the total number of parameters in the JetMoE-8B model?","","","parameters","[]","is_blank","is_blank",""
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by ~20% from 2021 to 2022. However, it does not provide specific numerical values for the water consumption in liters or gallons. Therefore, the answer cannot be calculated directly.","is_blank","percent","[""li2025b""]","is_blank","Importantly, the company’s data center water consumption increased by ~20% from 2021 to 2022 and by ~17% from 2022 to 2023 [4]","The context states that Google's data center water consumption increased by ~20% from 2021 to 2022. However, it does not provide specific numerical values for the water consumption in liters or gallons. Therefore, the answer cannot be calculated directly."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context does not provide information on the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context does not provide specific information about the FLOPs utilization percentage in the final growth stage of the FLM-101B training.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the FLOPs utilization percentage in the final growth stage of the FLM-101B training."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context does not provide specific information about the amortized training costs for OpenAI's GPT-4.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the amortized training costs for OpenAI's GPT-4."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context provides information about a Transformer-based universal translation model that achieved an 810x operational carbon footprint reduction through full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes.","810","multiplier","[""wu2021a""]","is_blank","Efﬁciency Optimization: Optimization across the axes of al-gorithms, platforms, infrastructures, hardware can signiﬁcantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","The context provides information about a Transformer-based universal translation model that achieved an 810x operational carbon footprint reduction through full-stack optimization, including platform-level caching, GPU acceleration, and algorithmic changes."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context does not provide specific information about the fraction of local throughput achieved for NLP when training was spread across four continents instead of remaining local.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the fraction of local throughput achieved for NLP when training was spread across four continents instead of remaining local."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. However, it does not specify how much of this was returned to communities from Amazon's replenishment projects in 2023 specifically.","is_blank","liters","[""is_blank""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. (amazon2023)","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. However, it does not specify how much of this was returned to communities from Amazon's replenishment projects in 2023 specifically."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context discusses a study on the impact of custom tags on the energy consumption of LLMs during the inference phase for code completion tasks. The study found that using custom tags can reduce energy consumption across three prompt engineering techniques: zero-shot, one-shot, and few-shots.","1","is_blank","[""rubei2025""]","is_blank","Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. [ref_id=rubei2025]","The context discusses a study on the impact of custom tags on the energy consumption of LLMs during the inference phase for code completion tasks. The study found that using custom tags can reduce energy consumption across three prompt engineering techniques: zero-shot, one-shot, and few-shots."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context includes a statement from a 2025 paper that directly provides the number of data center GPUs NVIDIA shipped in the year 2024.","3.76","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","Nvidia Shipped 3.76 Million Data-center GPUs in 2023, According to Study.","The context includes a statement from a 2025 paper that directly provides the number of data center GPUs NVIDIA shipped in the year 2024."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","","","percent","[]","is_blank","is_blank",""
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context does not provide a specific PUE value for Facebook's data centers other than 1.10, which is from 2020. Therefore, the PUE for Facebook's data centers is 1.10.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","['Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.', 'Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].']","The context does not provide a specific PUE value for Facebook's data centers other than 1.10, which is from 2020. Therefore, the PUE for Facebook's data centers is 1.10."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is ETAIROS.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","[ref_id=luccioni2025b] From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is ETAIROS."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context provides the estimated hardware acquisition costs to train frontier models, including the cost breakdown of amortized hardware CapEx + energy. However, it does not explicitly provide the estimated amortized training costs for Google's Gemini Ultra. Therefore, the answer is 'is_blank'.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context provides the estimated hardware acquisition costs to train frontier models, including the cost breakdown of amortized hardware CapEx + energy. However, it does not explicitly provide the estimated amortized training costs for Google's Gemini Ultra. Therefore, the answer is 'is_blank'."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, but it does not explicitly mention that these assessments should include environmental risks such as greenhouse gas emissions or water consumption. However, the authors argue that environmental risks should be included in these assessments as they relate to fundamental rights within the AI Act.","1","is_blank","[""ebert2024""]","is_blank","Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5]. In Art. 1(1) and Recital 1, the purpose of the AI Act is defined as protecting health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection.","The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, but it does not explicitly mention that these assessments should include environmental risks such as greenhouse gas emissions or water consumption. However, the authors argue that environmental risks should be included in these assessments as they relate to fundamental rights within the AI Act."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The EPA’s recently tightened standard for PM2.5 is 9µg/m3, which is higher than the WHO’s recommended level of 5µg/m3. This information is provided in the context [ref_id=han2024].","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The EPA’s recently tightened standard for PM2.5 is 9µg/m3, which is higher than the WHO’s recommended level of 5µg/m3. This information is provided in the context [ref_id=han2024]."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment (LCA) found that 115 books produce the same amount of CO2 as one Amazon Kindle device. This suggests that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books, which is 115 books.","115","books","[""luccioni2025a""]","is_blank","[ref_id=luccioni2025a] For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context states that a life cycle assessment (LCA) found that 115 books produce the same amount of CO2 as one Amazon Kindle device. This suggests that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books, which is 115 books."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the context, the per-household health burden of air pollutants in the most affected, economically-disadvantaged communities could exceed that in less-impacted communities by up to 200x. However, the context does not provide a specific numerical factor by which the burden could exceed. Therefore, the answer is is_blank.","is_blank","multiplier","[""han2024""]","is_blank","In particular, some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.","According to the context, the per-household health burden of air pollutants in the most affected, economically-disadvantaged communities could exceed that in less-impacted communities by up to 200x. However, the context does not provide a specific numerical factor by which the burden could exceed. Therefore, the answer is is_blank."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context does not provide the energy consumption of the DS Llama 70B model for inference on the FKTG dataset.","is_blank","Wh","[""is_blank""]","is_blank","is_blank","The context does not provide the energy consumption of the DS Llama 70B model for inference on the FKTG dataset."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not provide the total number of parameters in the large language model analyzed in the 2022 paper by Dodge et al.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide the total number of parameters in the large language model analyzed in the 2022 paper by Dodge et al."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context discusses the Jevons Paradox, where increased efficiency in resource usage can lead to overall increased consumption due to systemic demand. This is described in the context as a phenomenon that applies to AI systems.","1","is_blank","[""jegham2025""]","is_blank","Although large language models consume significantly less energy, water, and carbon per task than human labor [ 75], these efficiency gains do not inherently reduce overall environmental impact. As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand.","The context discusses the Jevons Paradox, where increased efficiency in resource usage can lead to overall increased consumption due to systemic demand. This is described in the context as a phenomenon that applies to AI systems."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context does not provide information on the energy consumption of Meena training runs compared to GPT-3 training runs.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context does not provide information on the energy consumption of Meena training runs compared to GPT-3 training runs."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides the energy consumption of the default ChatGPT model, which is the GPT-4o model, as 0.34 Wh per query. The study also mentions that this is for a short prompt, and our framework estimates 0.42 Wh (±0.13 Wh) for a short GPT-4o prompt.","0.42","kWh","[""jegham2025""]","is_blank","In June 2025, OpenAI CEO Sam Altman reported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing that GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level inference. Our framework estimates 0.42 Wh ( ±0.13 Wh) for a short GPT-4o prompt (0.37 Wh without datacenter overhead), within 19% of Altman’s figure.","The context provides the energy consumption of the default ChatGPT model, which is the GPT-4o model, as 0.34 Wh per query. The study also mentions that this is for a short prompt, and our framework estimates 0.42 Wh (±0.13 Wh) for a short GPT-4o prompt."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides the CO2 emissions for an average American life in a year, which is 36,156 lbs. This value is listed under the 'Consumption CO2e (lbs)' section in the table.","36156","lbs","[""strubell2019""]","is_blank","Air travel, 1 passenger, NY ↔SF 1984
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156
Car, avg incl. fuel, 1 lifetime 126,000
Training one model (GPU)
NLP pipeline (parsing, SRL) 39
w/ tuning & experimentation 78,468
Transformer (big) 192
w/ neural architecture search 626,155
Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption. 1
Sources: (1) Air travel and per-capita consumption: https://bit.ly/2Hw0xWc; (2) car lifetime: https://bit.ly/2Qbr0w1.","The context provides the CO2 emissions for an average American life in a year, which is 36,156 lbs. This value is listed under the 'Consumption CO2e (lbs)' section in the table."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context does not provide the specific per-household health cost for each county in West Virginia in 2030. It only provides data up to 2023.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide the specific per-household health cost for each county in West Virginia in 2030. It only provides data up to 2023."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context does not provide specific information about the change in emissions for Mistral-small after optimization in the financial sentiment classification task.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the change in emissions for Mistral-small after optimization in the financial sentiment classification task."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context from [ref_id=luccioni2024] provides the training energy cost for the BLOOMz-7B model, which is 51,686 kWh. The context also provides the finetuning energy cost, which is 7,571 kWh. Adding these two values gives the combined training and fine-tuning energy costs.","59257","kWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543","The context from [ref_id=luccioni2024] provides the training energy cost for the BLOOMz-7B model, which is 51,686 kWh. The context also provides the finetuning energy cost, which is 7,571 kWh. Adding these two values gives the combined training and fine-tuning energy costs."
"q125","What is the total number of parameters in the final FLM-101B model?","The context does not provide specific information about the total number of parameters in the final FLM-101B model. It mentions the model's performance on various tasks and its cost, but not the number of parameters.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total number of parameters in the final FLM-101B model. It mentions the model's performance on various tasks and its cost, but not the number of parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the energy cost per inference for the BLOOMz-7B model, which is 1.0 × 10−4 kWh. However, it does not provide the energy cost for training a 6.1B parameter model. Therefore, we cannot directly answer the question without additional information.","is_blank","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides the energy cost per inference for the BLOOMz-7B model, which is 1.0 × 10−4 kWh. However, it does not provide the energy cost for training a 6.1B parameter model. Therefore, we cannot directly answer the question without additional information."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' is explicitly stated as 754.66 kWh.","754.66","kWh","[""luccioni2024""]","is_blank","The total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' is explicitly stated as 754.66 kWh.","The total amount of energy consumed for all model experimentation and evaluation in the 2024 study 'Power Hungry Processing' is explicitly stated as 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the energy cost per inference for the BLOOMz-7B model and the total energy cost of training and fine-tuning it. By dividing the total energy cost by the energy cost per inference, we can estimate the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The context provides the energy cost per inference for the BLOOMz-7B model and the total energy cost of training and fine-tuning it. By dividing the total energy cost by the energy cost per inference, we can estimate the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide information on the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide information on the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide specific information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context from Patterson et al. (2021) states that the CO2e equivalent of the Evolved Transformer NAS is approximately 3 passengers taking a round trip between San Francisco and New York. This is based on the average direct round trip flights between SFO and JFK being 180.4t CO2e and the Boeing 767 having 175 seats with 84.5% seat occupancy, resulting in 1.2t CO2e per passenger round trip.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO 2 e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. (patterson2021)","The context from Patterson et al. (2021) states that the CO2e equivalent of the Evolved Transformer NAS is approximately 3 passengers taking a round trip between San Francisco and New York. This is based on the average direct round trip flights between SFO and JFK being 180.4t CO2e and the Boeing 767 having 175 seats with 84.5% seat occupancy, resulting in 1.2t CO2e per passenger round trip."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure. Therefore, 84% of the token usage occurred through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","Figure 2. Environmental Impact Transparency of LLM Usage – OpenRouter28 (May 2025)","The context states that 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure. Therefore, 84% of the token usage occurred through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for the 65B LLaMA model, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. However, it does not explicitly state the minimum number of GPUs required for LLaMA-13B.","is_blank","A100_80GB_GPU","[""is_blank""]","is_blank","is_blank","The context states that for the 65B LLaMA model, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. However, it does not explicitly state the minimum number of GPUs required for LLaMA-13B."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides the energy consumption in kWh for training a 6 billion parameter transformer model, which is 103,593 kWh. Since the context does not provide the exact time it took to train the model, we cannot directly calculate the CO2 emissions in metric tons. However, the context mentions that this is for approximately 13% of the total training time. Therefore, we can estimate the CO2 emissions for a complete training run by dividing the provided energy consumption by 13% and using the conversion factor of 1 kWh = 0.000283495 metric tons of CO2e (as per the question's additional info).","781215","metric tons","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context provides the energy consumption in kWh for training a 6 billion parameter transformer model, which is 103,593 kWh. Since the context does not provide the exact time it took to train the model, we cannot directly calculate the CO2 emissions in metric tons. However, the context mentions that this is for approximately 13% of the total training time. Therefore, we can estimate the CO2 emissions for a complete training run by dividing the provided energy consumption by 13% and using the conversion factor of 1 kWh = 0.000283495 metric tons of CO2e (as per the question's additional info)."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide specific information about the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context mentions a specific scenario where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","24","percent","[""griggs2024""]","is_blank","9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context mentions a specific scenario where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context does not provide information on the price per hour for an NVIDIA H20.","is_blank","USD per hour","[""is_blank""]","is_blank","is_blank","The context does not provide information on the price per hour for an NVIDIA H20."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses gather information manually by writing to authors, with Luccioni and Hernandez-Garcia reaching out to over 500 authors and only receiving 95 responses. This indicates that automatic data collection is not common.","0","is_blank","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-mation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].","The context states that most carbon footprint analyses gather information manually by writing to authors, with Luccioni and Hernandez-Garcia reaching out to over 500 authors and only receiving 95 responses. This indicates that automatic data collection is not common."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context does not provide specific information about the percentage of the data centers' total electricity cost that is equivalent to their public health cost in 2023 using the average attribution method.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage of the data centers' total electricity cost that is equivalent to their public health cost in 2023 using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context mentions that for the 65B LLaMA model, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. Since each A100 GPU has 80GB of memory, 1 such GPU is sufficient to run the model.","A100_80GB_GPU","A100_80GB_GPU","[""samsi2024"", ""rubei2025""]","is_blank","samsi2024: 'For this configuration, we ran on four A100 GPUs...'; rubei2025: 'The study reveals that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model'","The context mentions that for the 65B LLaMA model, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. Since each A100 GPU has 80GB of memory, 1 such GPU is sufficient to run the model."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context mentions that sustainable deployment techniques for large language models have demonstrated up to a 45% reduction in carbon emissions after quantization.","45","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization","The context mentions that sustainable deployment techniques for large language models have demonstrated up to a 45% reduction in carbon emissions after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors for their carbon footprint analysis and were only able to collect 95 answers. This implies that they received 95 answers out of over 500, meaning they got answers from less than 20% of the authors they contacted.","Less than 20%","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-mation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors for their carbon footprint analysis and were only able to collect 95 answers. This implies that they received 95 answers out of over 500, meaning they got answers from less than 20% of the authors they contacted."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B is trained with a budget of $100k, using 1.25T tokens and 30,000 H100 GPU hours. However, it does not provide specific information about the cost per GPU hour.","is_blank","USD per hour","[""is_blank""]","is_blank","is_blank","The context states that JetMoE-8B is trained with a budget of $100k, using 1.25T tokens and 30,000 H100 GPU hours. However, it does not provide specific information about the cost per GPU hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context does not provide specific information about the health cost as a percentage of the electricity cost for training a Llama-3.1 scale model in Altoona, Iowa. Therefore, the answer is_blank.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the health cost as a percentage of the electricity cost for training a Llama-3.1 scale model in Altoona, Iowa. Therefore, the answer is_blank."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context does not provide information on the number of tokens used to pre-train the JetMoE-8B model.","is_blank","tokens","[""is_blank""]","is_blank","is_blank","The context does not provide information on the number of tokens used to pre-train the JetMoE-8B model."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context does not provide specific information about the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024.","is_blank","projects","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the number of Amazon Renewable Energy Projects announced in the United Kingdom as of January 2024."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","","","percent","[]","is_blank","is_blank",""
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple's supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","[ref_id=li2025b] For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple's supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide information about the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide information about the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context does not provide information on which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year. This information is provided in the context, but the specific numbers are not directly cited, so the answer_value is set to 'is_blank'.","is_blank","times","[""luccioni2025a""]","is_blank","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119].","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year. This information is provided in the context, but the specific numbers are not directly cited, so the answer_value is set to 'is_blank'."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is referred to as 'water withdrawal' in the context.","Water withdrawal","is_blank","[""li2025b""]","is_blank","Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporar-ily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is referred to as 'water withdrawal' in the context."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. Therefore, the Standing Committee forms a Study Panel once every five years.","5","years","[""stone2022""]","is_blank","['As its core activity , the Standing Committee that oversees the One Hundred  Y ear Study forms a Study Panel  every five years to assess the  current state of  AI.']","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. Therefore, the Standing Committee forms a Study Panel once every five years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context states that 'at the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]'. This directly answers the question about the average number of connected devices per U.S. household reported in 2021.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context states that 'at the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]'. This directly answers the question about the average number of connected devices per U.S. household reported in 2021."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training large language models (LLMs), from 0.8 MWh to 3,500 MWh. However, it does not provide a specific range for the energy consumption to train a large language model.","is_blank","MWh","[""is_blank""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout) - [ref_id=luccioni2025c]","The context provides a range of energy consumption for pre-training large language models (LLMs), from 0.8 MWh to 3,500 MWh. However, it does not provide a specific range for the energy consumption to train a large language model."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context mentions IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011. It states that Watson was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","Watson won","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.159","The context mentions IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011. It states that Watson was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context from [ref_id=luccioni2025a] states that 10-50 queries on GPT-3 consumes around half a liter of water. However, the context from [ref_id=li2025b] suggests that the estimate of inference water consumption for GPT-3 is on the conservative side, and the actual water consumption could be several times higher.","is_blank","queries","[""luccioni2025a"", ""li2025b""]","is_blank","Luccioni et al. (2023) suggests that 10-50 queries on GPT-3 consumes around half a liter of water. Li et al. (2025b) states that the estimate of inference water consumption for GPT-3 is on the conservative side, and the actual water consumption could be several times higher.","The context from [ref_id=luccioni2025a] states that 10-50 queries on GPT-3 consumes around half a liter of water. However, the context from [ref_id=li2025b] suggests that the estimate of inference water consumption for GPT-3 is on the conservative side, and the actual water consumption could be several times higher."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","","","score","[]","is_blank","is_blank",""
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses. This implies that one medium-length response would require 1/10 to 1/50 of a bottle of water, or 0.1 to 0.02 liters of water.","is_blank","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses. This implies that one medium-length response would require 1/10 to 1/50 of a bottle of water, or 0.1 to 0.02 liters of water."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings. This is stated in the abstract and the 'Cost Savings Analysis' section of the paper.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting. ... In evaluations on a range of GPUs, request sizes, request rates, and latency SLOs, Mélange consistently demonstrates significant reductions in deployment costs (up to 77%) while providing high SLO attainment.","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings. This is stated in the abstract and the 'Cost Savings Analysis' section of the paper."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at least 4 A100 GPUs with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, it does not explicitly state the minimum number of GPUs required without compression or quantization.","is_blank","A100_80GB_GPUs","[""is_blank""]","is_blank","is_blank","The context states that at least 4 A100 GPUs with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, it does not explicitly state the minimum number of GPUs required without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. However, it does not provide a specific number of round trips for the scale of the model in question.","is_blank","round trips","[""han2024""]","is_blank","Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.
Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City (LA-NYC), resulting in a health cost that even exceeds 120% of the training electricity cost.
Importantly, although the public health impact of data centers may be modest at the national level, it is geographically concentrated, with certain regions and communities bearing a disproportionate share.
Furthermore, to highlight scope-1 health impacts, we examine data center backup generators in Virginia, which hosts one of the largest concentrations of data centers in the world [42]. Our analysis shows that, assuming the actual emissions are only 10% of the permitted level based on the historical reports and future projections [42–44], the data center backup generators registered in Virginia (mostly in Loudoun, Prince William, and Fairfax) could already cause 14,000 asthma symptom cases among other health outcomes and a total public health burden of$220-300 million per year, impacting residents in multiple surrounding states and as far as Florida (Section 3.1). If these data centers emit air pollutants at the maximum permitted level, the total public health cost will become 10-fold and reach$2.2-3.0 billion per year.","The context states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. However, it does not provide a specific number of round trips for the scale of the model in question."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from NVIDIA's report states that 80-90% of the ML workload is inference processing. This information is provided in the context snippet [ref_id=patterson2021] and is directly related to the question.","80-90","percent","[""patterson2021""]","is_blank","[ref_id=patterson2021]: For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context from NVIDIA's report states that 80-90% of the ML workload is inference processing. This information is provided in the context snippet [ref_id=patterson2021] and is directly related to the question."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context includes a statement that the total energy used for all model experimentation and evaluation was 754.66 kWh and the total CO2 emissions were 178.97 kg CO2eq. However, this includes both training and inference. The context does not provide a breakdown of the energy and emissions specifically for the inference stage.","is_blank","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context includes a statement that the total energy used for all model experimentation and evaluation was 754.66 kWh and the total CO2 emissions were 178.97 kg CO2eq. However, this includes both training and inference. The context does not provide a breakdown of the energy and emissions specifically for the inference stage."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context from chung2025 states that estimations using TDP are nearly always an overestimation and can lead to a worst-case overestimation of energy consumption by a factor of 4.1. Therefore, estimating GPU energy consumption based on its TDP is not a reliable or accurate method.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context from chung2025 states that estimations using TDP are nearly always an overestimation and can lead to a worst-case overestimation of energy consumption by a factor of 4.1. Therefore, estimating GPU energy consumption based on its TDP is not a reliable or accurate method."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","According to the context, GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to its reliance on older A100 GPU nodes. This implies that GPT-4o mini does indeed consume more energy per query than GPT-4o.","1","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","According to the context, GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries due to its reliance on older A100 GPU nodes. This implies that GPT-4o mini does indeed consume more energy per query than GPT-4o."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context does not provide the specific throughput value for a dense Mixtral-CS-A100-40GB model with a batch size of 1.","is_blank","queries/sec","[""is_blank""]","is_blank","is_blank","The context does not provide the specific throughput value for a dense Mixtral-CS-A100-40GB model with a batch size of 1."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context does not provide information on the trend of AI developers directly disclosing environmental information for notable models after 2022.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on the trend of AI developers directly disclosing environmental information for notable models after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context does not provide the normalized on-demand hourly price for an H100 GPU.","is_blank","USD per hour","[""is_blank""]","is_blank","is_blank","The context does not provide the normalized on-demand hourly price for an H100 GPU."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not provide specific information about the amount of water used for cooling during OpenAI's GPT-4 training run.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the amount of water used for cooling during OpenAI's GPT-4 training run."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context does not provide specific information about the hourly cost of running the Llama-2-70B model using 2 NVIDIA A100 GPUs. However, it does mention that serving the model costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days/month, the hourly cost can be estimated.","370.37","USD per hour","[""griggs2024""]","is_blank","The context mentions that serving Llama-2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days/month, the hourly cost can be estimated as $5,200 / 30 / 2 = $370.37","The context does not provide specific information about the hourly cost of running the Llama-2-70B model using 2 NVIDIA A100 GPUs. However, it does mention that serving the model costs over $5,200 per month in on-demand rental costs on major cloud platforms. Assuming 30 days/month, the hourly cost can be estimated."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model 1,000x larger in size.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model 1,000x larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context from Luccioni2023 states that training and fine-tuning a large Transformer model with Neural Architecture Search produced 626,155 lbs of CO2. However, the context does not provide any information about the emissions-to-driving-distance ratio, so the exact driving distance cannot be determined.","is_blank","miles","[""luccioni2023""]","is_blank","The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars. [48]","The context from Luccioni2023 states that training and fine-tuning a large Transformer model with Neural Architecture Search produced 626,155 lbs of CO2. However, the context does not provide any information about the emissions-to-driving-distance ratio, so the exact driving distance cannot be determined."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context provides the energy consumption per inference for the BLOOMz-7B model, which is 1.0 × 10−4 kWh. The context also mentions that the BLOOMz-7B model has been downloaded hundreds of thousands of times. However, the context does not provide information on the number of inferences per download.","is_blank","MWh","[""luccioni2024""]","is_blank","The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost. If we compare the amount of energy used per inference for each of the models with the total amount of energy used for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried out with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this varies depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over 590 million inferences for the biggest model, BLOOMz-7B.","The context provides the energy consumption per inference for the BLOOMz-7B model, which is 1.0 × 10−4 kWh. The context also mentions that the BLOOMz-7B model has been downloaded hundreds of thousands of times. However, the context does not provide information on the number of inferences per download."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","","","H100 GPU hours","[]","is_blank","is_blank",""
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context provides information about the increasing costs of training frontier AI models, with the amortized cost to train the most compute-intensive models growing at a rate of 2.4x per year since 2016. If this trend continues, the largest training runs will cost more than a billion dollars by 2027.","is_blank","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context provides information about the increasing costs of training frontier AI models, with the amortized cost to train the most compute-intensive models growing at a rate of 2.4x per year since 2016. If this trend continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context does not provide specific information about the total number of floating point operations to train GPT-3, but it does mention that OpenAI published this value as 3.14E+23 FLOPS.","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context does not provide specific information about the total number of floating point operations to train GPT-3, but it does mention that OpenAI published this value as 3.14E+23 FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, it does not explicitly mention the number of V100_32GB_GPUs required without compression or quantization.","is_blank","V100_32GB_GPUs","[""is_blank""]","is_blank","The context does not provide specific information on the number of V100_32GB_GPUs required without compression or quantization.","The context states that at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. However, it does not explicitly mention the number of V100_32GB_GPUs required without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context does not provide information on the throughput or FLOPs for the final 101B training stage.","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","The context does not provide information on the throughput or FLOPs for the final 101B training stage."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide the top-1 accuracy on ImageNet associated with AlexNet 2012.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide the top-1 accuracy on ImageNet associated with AlexNet 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, but it does not explicitly state the number of A800 GPUs used. However, since DGX-A800 servers typically come with 8 GPUs, and there are 24 servers, it can be inferred that 24 x 8 = 192 GPUs were used.","192","GPUs","[""li2025a""]","is_blank","['FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.']","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, but it does not explicitly state the number of A800 GPUs used. However, since DGX-A800 servers typically come with 8 GPUs, and there are 24 servers, it can be inferred that 24 x 8 = 192 GPUs were used."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context from [ref_id=patterson2021] provides the information that the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation is 626,155 pounds (284 tCO2 e). However, it does not provide information on how many average American lifetimes this is equivalent to.","is_blank","lifetimes","[""patterson2021""]","is_blank","[ref_id=patterson2021] estimates the CO2 emissions from NAS to train a Transformer-based model for machine translation as 626,155 pounds (284 tCO2 e).","The context from [ref_id=patterson2021] provides the information that the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation is 626,155 pounds (284 tCO2 e). However, it does not provide information on how many average American lifetimes this is equivalent to."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","","","metric tons","[]","is_blank","is_blank",""
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context does not provide information on the framework used to deploy large language models across multiple GPUs and nodes.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on the framework used to deploy large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides a table (Table 4) that compares the energy consumption of Llama 3.1 8B model when deployed on different node configurations. It shows that deploying the model on two nodes instead of one increases energy consumption. However, the exact factor of increase is not explicitly stated in the table.","is_blank","multiplier","[""zschache2025""]","is_blank","Table 4: Energy per generation (Joules) and the percentage of decode energy consumption with PD disaggregation. Model and deployment Request dataset Input mean 512 Output mean 512 Input mean 512 Output mean 4096 Input mean 4096 Output mean 512 Llama 3.1 8B (TP=1, 1P3D) 37.71, 77.2% 665.77, 98.7% 208.34, 67.2% Llama 3.1 8B (TP=1, 2P2D) 36.22, 76.7% 706.27, 98.8% 151.75, 55.2% Llama 3.1 8B (TP=1, 3P1D) 37.26, 77.0% 748.45, 98.9% 158.85, 56.0% Llama 3.1 70B (TP=4, 1P1D) 276.93, 64.8% 907.60, 89.2% 1492.59, 50.0%","The context provides a table (Table 4) that compares the energy consumption of Llama 3.1 8B model when deployed on different node configurations. It shows that deploying the model on two nodes instead of one increases energy consumption. However, the exact factor of increase is not explicitly stated in the table."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific information about the amount of water consumed per ChatGPT user session in 2023. However, it does mention that 10-50 queries on GPT-3 consume around half a liter of water. Assuming each user session consists of one query, this would suggest that each session consumes around 0.5 liters of water.","0.5","gallons of water","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context does not provide specific information about the amount of water consumed per ChatGPT user session in 2023. However, it does mention that 10-50 queries on GPT-3 consume around half a liter of water. Assuming each user session consists of one query, this would suggest that each session consumes around 0.5 liters of water."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context provides the estimated 2025 annual energy consumption of GPT-4o, which is 391,509 MWh at minimum and 463,269 MWh at maximum. The question asks for the number of U.S. homes this energy usage is comparable to, but the context does not provide this information.","is_blank","homes","[""jegham2025""]","is_blank","['6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries per day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].\nGiven GPT-4o’s status as the default model, we conservatively attribute 700 million daily queries to GPT-4o. To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20% from January to May 2025, reflecting the documented increase in ChatGPT’s weekly active user base from 300 million to 800 million between December 2024 and April 2025 [72]. This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].\nWithin these queries, we conservatively assume an 80%/20% split between short and medium-length prompts based on typical usage patterns. Scaling the per-query energy estimates accordingly, we find that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, as seen in Figure 5.']","The context provides the estimated 2025 annual energy consumption of GPT-4o, which is 391,509 MWh at minimum and 463,269 MWh at maximum. The question asks for the number of U.S. homes this energy usage is comparable to, but the context does not provide this information."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. However, the context does not specify the exact quantity of water consumed in 2021 and 2022.","is_blank","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. However, the context does not specify the exact quantity of water consumed in 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context mentions that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the energy costs of LLM deployment.","1","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context mentions that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the energy costs of LLM deployment."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11. This is a factor of 1.4X better than the US national datacenter average in 2018 and 2020.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11. This is a factor of 1.4X better than the US national datacenter average in 2018 and 2020."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","","","queries","[]","is_blank","is_blank",""
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context does not provide specific information about the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context discusses the need for open-source models to adhere to the same reporting standards as proprietary models, indicating that open-source models are not exempt from reporting obligations. However, it does not explicitly mention the AI Act or energy consumption.","is_blank","is_blank","[""is_blank""]","is_blank","Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. - ebert2024","The context discusses the need for open-source models to adhere to the same reporting standards as proprietary models, indicating that open-source models are not exempt from reporting obligations. However, it does not explicitly mention the AI Act or energy consumption."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context does not provide the US national datacenter average Power Usage Effectiveness (PUE) in 2020.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide the US national datacenter average Power Usage Effectiveness (PUE) in 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides a figure (Fig. 1) that shows the KV Cache size growth for the OPT-2.7B model with a batch size of 32. At this batch size, the KV Cache becomes 5.312GB.","5.312","GB","[""kim2025""]","is_blank","KV Cache Size (GB) for OPT-2.7B model with batch size 32: 5.312","The context provides a figure (Fig. 1) that shows the KV Cache size growth for the OPT-2.7B model with a batch size of 32. At this batch size, the KV Cache becomes 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","","","percent","[]","is_blank","is_blank",""
"q213","Which software package was used to measure energy consumption during inference runs?","The context states that the energy consumption and the runtime of the inference phase were measured by the CodeCarbon package, but it does not explicitly mention which software package was used to measure energy consumption during inference runs.","is_blank","is_blank","[""zschache2025""]","is_blank","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context states that the energy consumption and the runtime of the inference phase were measured by the CodeCarbon package, but it does not explicitly mention which software package was used to measure energy consumption during inference runs."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context includes an analysis of 100 news articles on ChatGPT's energy use, and it is stated that 53% of these articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search. However, the context does not provide information on the percentage of articles that cite the specific estimate of '10 times more than a Google search'.","is_blank","percent","[""luccioni2025c"", ""jegham2025""]","is_blank","['53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search', 'A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%']","The context includes an analysis of 100 news articles on ChatGPT's energy use, and it is stated that 53% of these articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search. However, the context does not provide information on the percentage of articles that cite the specific estimate of '10 times more than a Google search'."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance is the Compute Time Calibration Function (CTCF).","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance is the Compute Time Calibration Function (CTCF)."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards increases the energy per second, which implies that increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","1","is_blank","[""samsi2024""]","is_blank","B. Energy per Second: LLaMA 65B, We first take a look at the amount of energy inference costs per unit time in seconds. Figures 4 and 5 show a more in-depth look of the energy inference costs of LLaMA 65B across different batch sizes and degrees of sharding. Specifically, Figure 4 shows energy costs for maximum generation length 512 and Figure 5 shows energy costs for 1024. Overall, we see an average increase in energy per second with the number of shards. While there is a slight correlation as energy per second increases with increasing batch size, increasing the number of shards always increases the wattage. Indeed, the energy per second increases with the number of shards even at the same batch size (e.g., the energy of inference at batch size 64, going from 16 shards to 32 shards). For both datasets, increasing the max generation length from 512 to 1024 does seem to increase the energy per second for each batch size within each shard configuration, but the overall effect is less clear or consistent. Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The context states that increasing the number of shards increases the energy per second, which implies that increasing the number of GPU shards increased the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context does not provide specific information on the water consumption for mining rare earth materials for a single H100 GPU that is 0.1% rare earth metal by mass. It only mentions that mining 1 kg of rare earth materials consumes about 11 kL of water. We cannot extrapolate this to a single GPU based on the given context.","is_blank","kL","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the water consumption for mining rare earth materials for a single H100 GPU that is 0.1% rare earth metal by mass. It only mentions that mining 1 kg of rare earth materials consumes about 11 kL of water. We cannot extrapolate this to a single GPU based on the given context."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses the need for reporting energy consumption at the cumulative server level, including open-source models, and the removal of the exemption for open-source models. However, it does not explicitly mention the reporting of energy consumption to authorities.","is_blank","is_blank","[""ebert2024""]","is_blank","ebert2024: Page 8, 9, 14 - Conference’17, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker","The context discusses the need for reporting energy consumption at the cumulative server level, including open-source models, and the removal of the exemption for open-source models. However, it does not explicitly mention the reporting of energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context from [ref_id=luccioni2025a] states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide.","30","percent","[""luccioni2025a""]","is_blank","['In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide']","The context from [ref_id=luccioni2025a] states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The total public health cost attributable to data center operations is projected to potentially triple from 2023 to 2028, reaching up to $20.9 billion in 2028 based on low- and high-growth scenarios. This information is provided in a Lawrence Berkeley National Laboratory (LBNL) report mentioned in the context.","20.9","USD","[""han2024""]","is_blank","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center elec-tricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption. This substantial growth in electricity demand and onsite generation is expected to offset, and in fact outweigh, the gradual pollution emission intensity reductions anticipated from the power sector. As a result, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach$11.7 billion and$20.9 billion in 2028, respectively.","The total public health cost attributable to data center operations is projected to potentially triple from 2023 to 2028, reaching up to $20.9 billion in 2028 based on low- and high-growth scenarios. This information is provided in a Lawrence Berkeley National Laboratory (LBNL) report mentioned in the context."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context does not provide specific information about the energy consumption of the o3 model or how it compares to the GPT-4.1 nano model.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the energy consumption of the o3 model or how it compares to the GPT-4.1 nano model."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context provides a detailed comparison of the deployment costs of Mélange to single-GPU-type baselines across different datasets and SLOs. It includes a figure (Figure 11) that displays costs normalized against the cost of Mélange, and the detailed GPU allocations and cost savings are included in App. C. The A10G-only and L4-only baselines are only included for the Arena dataset because the PubMed and Mixed datasets contain large requests that exceed A10G and L4’s GPU memory capacity. L4 and A10G are included in Mélange’s allocation but are limited to serving requests smaller than 12,000 tokens.","15-77","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context provides a detailed comparison of the deployment costs of Mélange to single-GPU-type baselines across different datasets and SLOs. It includes a figure (Figure 11) that displays costs normalized against the cost of Mélange, and the detailed GPU allocations and cost savings are included in App. C. The A10G-only and L4-only baselines are only included for the Arena dataset because the PubMed and Mixed datasets contain large requests that exceed A10G and L4’s GPU memory capacity. L4 and A10G are included in Mélange’s allocation but are limited to serving requests smaller than 12,000 tokens."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context does not provide the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide information on the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide information on the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The public health costs of AI are not stated to be evenly distributed across communities in the U.S. in the provided context. Instead, it is mentioned that the public health impact of AI significantly differs from environmental costs and is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.","is_blank","is_blank","[""han2024""]","is_blank","The public health impact of AI significantly differs from these environmental costs or metrics... The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The public health costs of AI are not stated to be evenly distributed across communities in the U.S. in the provided context. Instead, it is mentioned that the public health impact of AI significantly differs from environmental costs and is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context explicitly states that GPU theoretical performance per watt doubles every 3-4 years as of 2019.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architec-tural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context explicitly states that GPU theoretical performance per watt doubles every 3-4 years as of 2019."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context does not provide specific information about the open-source tool used for 4-bit quantization and local deployment in the financial sentiment case study. It only mentions Ollama is used for local AI model deployment.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the open-source tool used for 4-bit quantization and local deployment in the financial sentiment case study. It only mentions Ollama is used for local AI model deployment."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context does not provide information about the specific storage service used for sharding and streaming datasets for spot VMs.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information about the specific storage service used for sharding and streaming datasets for spot VMs."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context does not provide specific information about the relationship between runtime and energy consumption being nearly linear.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the relationship between runtime and energy consumption being nearly linear."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context does not provide information on which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The price per hour for an NVIDIA H100 is provided in the table from Chen et al. (2025).","2.70","USD per hour","[""chen2024""]","is_blank","[ref_id=chen2024] Table 1: H100, H20, and TPU v6e specifications. H100 H20 TPU v6e [7] BF16 TFLOPs 989 148 918 Memory capacity 80 GB 96 GB 32 GB Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s Power rating 700 W 400 W unlisted Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s Network bandwidth 400 Gbps 400 Gbps 200 Gbps Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr *: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","The price per hour for an NVIDIA H100 is provided in the table from Chen et al. (2025)."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context does not provide specific information about the estimated average GPU lifetime before retirement in AI data centers in 2024.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the estimated average GPU lifetime before retirement in AI data centers in 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for the 65B LLaMA model, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. However, it does not explicitly state the minimum number of V100_32GB_GPUs required for LLaMA-13B without compression or quantization.","is_blank","V100_32GB_GPUs","[""is_blank""]","is_blank","is_blank","The context states that for the 65B LLaMA model, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences. However, it does not explicitly state the minimum number of V100_32GB_GPUs required for LLaMA-13B without compression or quantization."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the context, Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e. This is over 4x the 'five cars' estimate, which is about 284 metric tons CO2e.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number,","According to the context, Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e. This is over 4x the 'five cars' estimate, which is about 284 metric tons CO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context from Strubell et al. (2019) states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context from Strubell et al. (2019) states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours)."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the U.S. national average water consumption for electricity generation, which is 3.1 L/kWh. However, it does not explicitly state the value in L/kWh for the estimated U.S. national average water consumption for electricity generation.","3.1","L/kWh","[""is_blank""]","is_blank","is_blank","The context provides the U.S. national average water consumption for electricity generation, which is 3.1 L/kWh. However, it does not explicitly state the value in L/kWh for the estimated U.S. national average water consumption for electricity generation."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that Google's PUE was 1.10 for the 12 months ending in Q1 2021. However, it does not explicitly state that this is for their hyperscale data centers.","is_blank","PUE","[""dodge2022""]","is_blank","[ref_id=dodge2022] Some companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].","The context states that Google's PUE was 1.10 for the 12 months ending in Q1 2021. However, it does not explicitly state that this is for their hyperscale data centers."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the context, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This is mentioned in the context as a goal that Amazon, including AWS, achieved in 2023.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the context, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy. This is mentioned in the context as a goal that Amazon, including AWS, achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context states that in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU. An experiment was conducted where a BERT-base model was trained on a single NVIDIA TITAN X GPU, and the GPU accounted for almost 3/4 of the electricity consumption.","74","percent","[""dodge2022""]","is_blank","In ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [ 5, 45]. To confirm this, a BERT-base model was trained on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption of different components. The GPU accounted for almost 3/4 of the electricity consumption.","The context states that in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU. An experiment was conducted where a BERT-base model was trained on a single NVIDIA TITAN X GPU, and the GPU accounted for almost 3/4 of the electricity consumption."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","","","H100 GPUs","[]","is_blank","is_blank",""
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, which is over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W.","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, which is over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context from Strubell et al. (2019) provides the CO2e emissions for an average human life in one year, which is 11,023 lbs.","11023","lbs","[""strubell2019""]","is_blank","Consumption CO 2e (lbs) Human life, avg, 1 year 11,023","The context from Strubell et al. (2019) provides the CO2e emissions for an average human life in one year, which is 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context does not provide a direct comparison of inference throughput speedup for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs. However, it mentions a 1.25x increase in inference latency for LLaMA-13B on A100 GPUs compared to V100 GPUs. Since latency and throughput are inversely related, we can infer that there is approximately a 1.25x decrease in inference throughput for LLaMA-13B on A100 GPUs compared to V100 GPUs.","1.25","multiplier","[""samsi2024""]","is_blank","[ref_id=samsi2024] As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context does not provide a direct comparison of inference throughput speedup for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs. However, it mentions a 1.25x increase in inference latency for LLaMA-13B on A100 GPUs compared to V100 GPUs. Since latency and throughput are inversely related, we can infer that there is approximately a 1.25x decrease in inference throughput for LLaMA-13B on A100 GPUs compared to V100 GPUs."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides the energy consumption of a single short query to GPT-4o as 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","[ref_id=jegham2025] A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)","The context provides the energy consumption of a single short query to GPT-4o as 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","According to the context, the Max-Performance instance (g6e.xlarge) is about 280% more expensive than InferSave's top choice for an online inference workload with a 400 TPS SLO requirement. However, the specific cost difference is not provided.","is_blank","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","According to the context, the Max-Performance instance (g6e.xlarge) is about 280% more expensive than InferSave's top choice for an online inference workload with a 400 TPS SLO requirement. However, the specific cost difference is not provided."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context states that for models generating a single token per inference, a V100 or A30 GPU is more efficient in inference in terms of energy consumption. However, the specific energy consumption values are not provided.","is_blank","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. (Page 4, ref_id=zschache2025)","The context states that for models generating a single token per inference, a V100 or A30 GPU is more efficient in inference in terms of energy consumption. However, the specific energy consumption values are not provided."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context mentions that Green AI involves making efficiency an evaluation criterion for research alongside accuracy and related measures, and reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","The context mentions that Green AI involves making efficiency an evaluation criterion for research alongside accuracy and related measures, and reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","","","metric tons","[]","is_blank","is_blank",""
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context does not provide a direct comparison of the average system power per processor between the TPU v2 and the V100 GPU. However, it mentions that GPUs, including the V100, can run closer to their TDP due to Turbo Mode and Dynamic Voltage Frequency Scaling, which are not found in TPU v2/v3. The TDP for V100 is 300 Watts and for TPU v2 is 280 Watts. Therefore, we can infer that the average system power per processor for V100 is likely higher than TPU v2.","is_blank","Watts","[""patterson2021""]","is_blank","GPUs, including the V100, can run closer to their TDP due to Turbo Mode and Dynamic Voltage Frequency Scaling, which are not found in TPU v2/v3. The TDP for V100 is 300 Watts and for TPU v2 is 280 Watts.","The context does not provide a direct comparison of the average system power per processor between the TPU v2 and the V100 GPU. However, it mentions that GPUs, including the V100, can run closer to their TDP due to Turbo Mode and Dynamic Voltage Frequency Scaling, which are not found in TPU v2/v3. The TDP for V100 is 300 Watts and for TPU v2 is 280 Watts. Therefore, we can infer that the average system power per processor for V100 is likely higher than TPU v2."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context provides information on the water usage of GPT-3's training phase, but it does not specify the exact amount of water that can be directly evaporated during the training phase in Microsoft's U.S. data centers.","is_blank","liters","[""is_blank""]","is_blank","The context includes the following relevant snippets, but they do not provide enough information to answer the question: 'Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13]' and 'By taking the GPT-3 model with 175 billion parameters as an example [11], we show that training GPT-3 in Microsoft’s U.S. data centers can consume a total of 5.4 million liters of water, including 700,000 liters of scope-1 on-site water consumption.' However, these snippets do not specify the amount of water directly evaporated during the training phase in Microsoft's U.S. data centers.","The context provides information on the water usage of GPT-3's training phase, but it does not specify the exact amount of water that can be directly evaporated during the training phase in Microsoft's U.S. data centers."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that Facebook's recommendation model sizes have increased by 20x between 2019 and 2021. However, it does not provide a specific numerical value for the size of the models.","is_blank","multiplier","[""wu2021a""]","is_blank","Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021. ---","The context states that Facebook's recommendation model sizes have increased by 20x between 2019 and 2021. However, it does not provide a specific numerical value for the size of the models."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context discusses a study that uses Data Envelopment Analysis (DEA) to assess how effectively each model converts environmental inputs into performance. The study ranks AI models based on their eco-efficiency scores, but it does not explicitly state which model ranked highest in a recent eco-efficiency analysis using DEA.","is_blank","is_blank","[""is_blank""]","is_blank","The context does not provide specific information about the highest-ranked model in a recent eco-efficiency analysis using DEA.","The context discusses a study that uses Data Envelopment Analysis (DEA) to assess how effectively each model converts environmental inputs into performance. The study ranks AI models based on their eco-efficiency scores, but it does not explicitly state which model ranked highest in a recent eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","","","is_blank","[]","is_blank","is_blank",""
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context does not provide information on the per-GPU speedup for intra-zone scaling with T4 GPUs for CV models.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide information on the per-GPU speedup for intra-zone scaling with T4 GPUs for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context mentions that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","[ref_id=li2025a] The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context mentions that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, making it less compute-intensive than diffusion models. It also mentions that diffusion models consume nearly the maximum power of the GPU when batch size is not small due to their high compute-intensity.","1","is_blank","[""chung2025""]","is_blank","Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context states that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, making it less compute-intensive than diffusion models. It also mentions that diffusion models consume nearly the maximum power of the GPU when batch size is not small due to their high compute-intensity."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context does not provide specific information about the percentage of Amazon's people managers globally who identified as women in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage of Amazon's people managers globally who identified as women in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides a detailed breakdown of the amortized cost of four specific models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) when excluding equity. The range of computing hardware costs is given as 61-76%.","61-76%","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2--6%. However, if we exclude equity the fraction for R&D staff drops to 19--33%, and the fractions of computing hardware costs and energy rise to 61--76% and 2--7% respectively.","The context provides a detailed breakdown of the amortized cost of four specific models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) when excluding equity. The range of computing hardware costs is given as 61-76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.' This implies that accuracy and F1 scores did not always improve after optimization.","0","is_blank","[""khan2025""]","is_blank","Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling. This underscores the importance of carefully balancing sustainability and performance when applying optimization techniques, as the ideal solution will depend on the specific use case and application requirements. TABLE III COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLM S BEFORE AND AFTER OPTIMIZATION . CARBON EMISSIONS ARE CALCULATED PER INFERENCE TASK .","The context states that 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.' This implies that accuracy and F1 scores did not always improve after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., which is used to convert power to estimated CO2 emissions.","0.954","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0 .954pt (2)","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., which is used to convert power to estimated CO2 emissions."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context provides a range of electricity consumption by the global AI in 2027, which is then used to estimate the potential water usage for global AI in 2027. However, the exact water usage is not provided in the context.","is_blank","TWh","[""li2025b""]","is_blank","['A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]']","The context provides a range of electricity consumption by the global AI in 2027, which is then used to estimate the potential water usage for global AI in 2027. However, the exact water usage is not provided in the context."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context does not provide specific information on the number of packages delivered by Amazon via EVs in Europe in 2023 in millions.","is_blank","packages","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of packages delivered by Amazon via EVs in Europe in 2023 in millions."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context does not provide the total number of tokens processed during the entire online inference workload evaluation.","is_blank","tokens","[""is_blank""]","is_blank","is_blank","The context does not provide the total number of tokens processed during the entire online inference workload evaluation."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context states that the AI Act does not mandate the disclosure of greenhouse gas emissions generated by AI applications, such as in oil and gas exploration. It also mentions a recent investigation revealing Microsoft's aggressive pitch of its AI models to ExxonMobile for fossil fuel exploration optimization.","0","is_blank","[""ebert2024""]","is_blank","5) The AI Act fails to address the greenhouse gas (GHG) emis-sions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37]. For example, a recent investiga- tion has revealed Microsoft’s aggressive pitch of its AI models to ExxonMobile to optimize fossil fuel exploration [35].","The context states that the AI Act does not mandate the disclosure of greenhouse gas emissions generated by AI applications, such as in oil and gas exploration. It also mentions a recent investigation revealing Microsoft's aggressive pitch of its AI models to ExxonMobile for fossil fuel exploration optimization."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context provides information about the Flexible Start optimization analysis in the 2022 Dodge et al. paper, which found significant emissions reductions for shorter jobs like the DenseNet experiments. However, the exact percentage reduction for a short job in the West US region is not explicitly stated in the context.","is_blank","percent","[""dodge2022""]","is_blank","['is_blank']","The context provides information about the Flexible Start optimization analysis in the 2022 Dodge et al. paper, which found significant emissions reductions for shorter jobs like the DenseNet experiments. However, the exact percentage reduction for a short job in the West US region is not explicitly stated in the context."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context does not provide specific information about the factor by which the energy required for image generation exceeds that of text classification in a 2024 study.","is_blank","times","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the factor by which the energy required for image generation exceeds that of text classification in a 2024 study."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context mentions that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the energy costs of LLM deployment.","1","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context mentions that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, justifying the energy costs of LLM deployment."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context does not provide specific information about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024.","is_blank","projects","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the number of Amazon Renewable Energy Projects announced in the United States as of January 2024."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide specific information about the percentage of power usage that AWS covered with renewable energy in 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage of power usage that AWS covered with renewable energy in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context does not provide specific information about the measurement level for AI energy consumption reporting that balances accuracy and feasibility.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the measurement level for AI energy consumption reporting that balances accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","","","percent","[]","is_blank","is_blank",""
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving a Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantital size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving a Llama2-70b model at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that there was a 28.5% operational energy footprint reduction over a two-year time period due to iterative hardware-software optimization in the context of AI model training and inference at Facebook.","28.5","percent","[""wu2021a""]","is_blank","[ref_id=wu2021a] The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context states that there was a 28.5% operational energy footprint reduction over a two-year time period due to iterative hardware-software optimization in the context of AI model training and inference at Facebook."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not provide specific information about the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides information about the amortized hardware CapEx + energy cost of GPT-4, which is $40M. It also mentions that the acquisition cost is one to two orders of magnitude higher than the amortized cost. Since one order of magnitude higher would be a factor of 10, and two orders of magnitude higher would be a factor of 100, the acquisition cost would be between $400M and $4000M.","is_blank","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context provides information about the amortized hardware CapEx + energy cost of GPT-4, which is $40M. It also mentions that the acquisition cost is one to two orders of magnitude higher than the amortized cost. Since one order of magnitude higher would be a factor of 10, and two orders of magnitude higher would be a factor of 100, the acquisition cost would be between $400M and $4000M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass using AI in climate-positive applications and improving the environmental sustainability of AI approaches.","1","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially propo sed by van Wynsberghe as a ﬁeld of practice that both aims to use AI in climate-positive applications, as well as i mproving upon the (environmental) sustainability of AI approaches themselves [203].","The umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass using AI in climate-positive applications and improving the environmental sustainability of AI approaches."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides the projected maximum batch size of Mixtral for different GPUs, including the NVIDIA A100-40GB GPU. It mentions that the maximum batch size supported for fine-tuning Mixtral on a NVIDIA A100-40GB GPU is 28.","28","samples","[""xia2024""]","is_blank","0 20 40 60 80 100 1200
5
10
15
20
25
30
35
40
A100-40GB
A100-80GB
A40
H100
bsz=28
bsz=35
Projected GPU capacity
Ground Truth Projection
Max batch size
GPU DRAM capacity
Fig. 13. Projected maximum batch size of Mixtral for different GPUs.","The context provides the projected maximum batch size of Mixtral for different GPUs, including the NVIDIA A100-40GB GPU. It mentions that the maximum batch size supported for fine-tuning Mixtral on a NVIDIA A100-40GB GPU is 28."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","","","is_blank","[]","is_blank","is_blank",""
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The 2024 environmental report by Google does not provide specific data on the percentage increase in GHG emissions since 2019.","is_blank","percent","[""luccioni2025a"", ""jegham2025""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf"", ""https://sustainability.google/reports/google-2024-environmental-report/""]","is_blank","The 2024 environmental report by Google does not provide specific data on the percentage increase in GHG emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to a recent Lawrence Berkeley National Laboratory (LBNL) report, the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7-12.0% in 2028, depending on the growth trajectory of AI adoption. This represents the projection for the percentage of U.S. national electricity consumption that data centers are anticipated to account for in 2030.","6.7-12.0","percent","[""han2024""]","is_blank","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center elec-tricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption.","According to a recent Lawrence Berkeley National Laboratory (LBNL) report, the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7-12.0% in 2028, depending on the growth trajectory of AI adoption. This represents the projection for the percentage of U.S. national electricity consumption that data centers are anticipated to account for in 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides a figure (Fig. 7) that shows the optimization results for the training of a 6B Parameter Transformer on 256 A100s. The figure shows that the 'Pause and Resume' optimization resulted in a 25% reduction in CO2 emissions after 24 hours.","25","percent","[""dodge2022""]","is_blank","Fig. 7, [ref_id=dodge2022]","The context provides a figure (Fig. 7) that shows the optimization results for the training of a 6B Parameter Transformer on 256 A100s. The figure shows that the 'Pause and Resume' optimization resulted in a 25% reduction in CO2 emissions after 24 hours."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. However, it does not provide the exact percentage reduction.","is_blank","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. However, it does not provide the exact percentage reduction."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context includes a reference to a study by Alexandra Sasha Luccioni et al. that quantified the carbon footprint of training the BERT large language model, which reached 626,155 pounds of CO2e emissions. This is the carbon footprint of training the model once, and does not include the carbon impact of model development and inference, as well as other tasks such as data processing, data transfer, and data storage.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","[ref_id=luccioni2025b] which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context includes a reference to a study by Alexandra Sasha Luccioni et al. that quantified the carbon footprint of training the BERT large language model, which reached 626,155 pounds of CO2e emissions. This is the carbon footprint of training the model once, and does not include the carbon impact of model development and inference, as well as other tasks such as data processing, data transfer, and data storage."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context provides the estimated training energy of the full GPT-3 model as 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","3.3.1 Training GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context provides the estimated training energy of the full GPT-3 model as 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context discusses the optimization of the Mixture of Experts (MoE) layer for improving the performance of LLM fine-tuning. It mentions that the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average.","1","is_blank","[""xia2024""]","is_blank","The MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average.","The context discusses the optimization of the Mixture of Experts (MoE) layer for improving the performance of LLM fine-tuning. It mentions that the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context does not provide information on the maximum batch size supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide information on the maximum batch size supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that intercontinental training for high granularity tasks like CV only slows down performance by 7% compared to local training, even when distributing VMs over four continents.","7","is_blank","[""erben2023""]","is_blank","When comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (CV 7%, NLP 35%) as with C-4 (CV 9%, NLP 36%) to A-4. Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4), due to the decreasing granularity of 0.4 (Figure 9b). The small granularity removes the additional gain of four more GPUs since the task is no longer suitable for distributed training. However, as the CV task is still at a granularity of 3.33 on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.","The context states that intercontinental training for high granularity tasks like CV only slows down performance by 7% compared to local training, even when distributing VMs over four continents."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","","","hectares","[]","is_blank","is_blank",""
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides the emissions per 1,000 queries for the BERT-based model bert-base-multilingual-uncased-sentiment for the task of text classification, which is 0.32g CO2eq.","0.32","g CO2eq","[""luccioni2024""]","is_blank","The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context provides the emissions per 1,000 queries for the BERT-based model bert-base-multilingual-uncased-sentiment for the task of text classification, which is 0.32g CO2eq."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context provides information about carbon emissions from training a BERT model across different regions, with a specific mention of the range of emissions for the most and least efficient regions (7k grams vs. 26k grams). However, the context does not specify the unit of measurement for these values. Since the question asks for the range in thousands of grams, we need to convert the values accordingly.","is_blank","grams","[""dodge2022""]","is_blank","Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for 36 hours) in 16 different regions (one region per line) at different times throughout the year. Each line is relatively flat, indicating the emissions in a single region during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context provides information about carbon emissions from training a BERT model across different regions, with a specific mention of the range of emissions for the most and least efficient regions (7k grams vs. 26k grams). However, the context does not specify the unit of measurement for these values. Since the question asks for the range in thousands of grams, we need to convert the values accordingly."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This suggests that the practice of directly releasing environmental information for notable models peaked in 2022.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. (luccioni2025c)","According to the context, the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. This suggests that the practice of directly releasing environmental information for notable models peaked in 2022."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context does not provide information on the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens.","is_blank","days","[""is_blank""]","is_blank","is_blank","The context does not provide information on the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information about Google's DeepMind AlphaFold servers' water consumption in 2023.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about Google's DeepMind AlphaFold servers' water consumption in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context mentions that adding compute resources to accelerate the MoE layers can reduce cost, but it does not explicitly state that it increases cost.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions that adding compute resources to accelerate the MoE layers can reduce cost, but it does not explicitly state that it increases cost."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context does not provide the total energy consumption for training the FLM-101B model.","is_blank","kWh","[""is_blank""]","is_blank","is_blank","The context does not provide the total energy consumption for training the FLM-101B model."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to a recent Lawrence Berkeley National Laboratory (LBNL) report, the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7-12.0% in 2028. Based on low- and high-growth scenarios, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively. This highlights the growing health externalities of U.S. data centers at a national scale.","20.9","USD","[""han2024""]","is_blank","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center elec-tricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption. At the same time, the projected rise in peak power demand is accompanied by massive installations of onsite backup diesel generators to ensure reliability during grid contingencies [60]. This substantial growth in electricity demand and onsite generation is expected to offset, and in fact outweigh, the gradual pollution emission intensity reductions anticipated from the power sector. As a result, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach$11.7 billion and$20.9 billion in 2028, respectively.","According to a recent Lawrence Berkeley National Laboratory (LBNL) report, the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7-12.0% in 2028. Based on low- and high-growth scenarios, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively. This highlights the growing health externalities of U.S. data centers at a national scale."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context does not provide information on the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context does not provide information on the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context does not provide information on the batch size (in samples) of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide information on the batch size (in samples) of the longest-running MoE layer for a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context states that GPU-level power consumption monitoring is not recommended for overall energy measurements as it substantially underrepresents the actual energy consumption, and it is advised to use the cumulative server level or data center level for more accurate measurements.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. Despite GPU power consumption being a significant factor and its usage correlating with the total power usage, it substantially underrepresents the actual energy consumption since it measures just a single component.","The context states that GPU-level power consumption monitoring is not recommended for overall energy measurements as it substantially underrepresents the actual energy consumption, and it is advised to use the cumulative server level or data center level for more accurate measurements."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context does not provide specific information about what percentage of the BLOOM model's overall emissions were accounted for by training.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about what percentage of the BLOOM model's overall emissions were accounted for by training."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that for any meaningful inferences with the 65B LLaMA model, at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, it does not explicitly mention the number of V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization.","is_blank","V100_32GB_GPU","[""is_blank""]","is_blank","is_blank","The context states that for any meaningful inferences with the 65B LLaMA model, at least 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required. However, it does not explicitly mention the number of V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context does not provide information on the amount of water consumed per user request for GPT-3 in a data center in Arizona.","is_blank","requests","[""is_blank""]","is_blank","is_blank","The context does not provide information on the amount of water consumed per user request for GPT-3 in a data center in Arizona."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context provides the CO2 emissions for an average US home energy use, which is 8.3 metric tons per year. The context also mentions that the largest experiment in their set, a 6 billion parameter transformer, emits more CO2 than the average US home energy use even when it is only partially trained.","MoreThan8.3","metric tons","[""dodge2022""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context provides the CO2 emissions for an average US home energy use, which is 8.3 metric tons per year. The context also mentions that the largest experiment in their set, a 6 billion parameter transformer, emits more CO2 than the average US home energy use even when it is only partially trained."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context does not provide specific information about the GSM8k benchmark score for the JetMoE-8B model.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the GSM8k benchmark score for the JetMoE-8B model."
