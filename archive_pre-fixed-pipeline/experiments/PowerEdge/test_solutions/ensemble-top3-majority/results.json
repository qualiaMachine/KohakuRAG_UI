[
  {
    "id": "q001",
    "question": "What was the average increase in U.S. data center electricity consumption between 2010 and 2014?",
    "gt_value": "4",
    "gt_unit": "percent",
    "gt_ref": "['wu2021b']",
    "pred_value": "4",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "4",
      "qwen32b-bench": "4",
      "qwen14b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q002",
    "question": "In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",
    "gt_value": "13900",
    "gt_unit": "cars",
    "gt_ref": "['amazon2023']",
    "pred_value": "13900",
    "pred_unit": "cars",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "13900",
      "qwen32b-bench": "13900",
      "qwen14b-bench": "13900"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q004",
    "question": "How many data centers did AWS begin using recycled water for cooling in 2023?",
    "gt_value": "24",
    "gt_unit": "data centers",
    "gt_ref": "['amazon2023']",
    "pred_value": "24",
    "pred_unit": "data centers",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "24",
      "qwen32b-bench": "24",
      "qwen14b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q005",
    "question": "Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",
    "gt_value": "463",
    "gt_unit": "kg/GPU",
    "gt_ref": "['morrison2025']",
    "pred_value": "463",
    "pred_unit": "kg/GPU",
    "pred_ref": "[\"luccioni2023\", \"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "463",
      "qwen32b-bench": "463",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\", \"luccioni2023\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q006",
    "question": "By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?",
    "gt_value": "400",
    "gt_unit": "ratio",
    "gt_ref": "['cottier2025', 'li2025a']",
    "pred_value": "400",
    "pred_unit": "ratio",
    "pred_ref": "[\"cottier2024\", \"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "400",
      "qwen32b-bench": "400",
      "qwen14b-bench": "400"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\", \"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\", \"li2025a\"]",
      "qwen14b-bench": "[\"cottier2024\", \"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 0.3333333333333333,
    "na_correct": true
  },
  {
    "id": "q007",
    "question": "What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?",
    "gt_value": "1.2",
    "gt_unit": "tCO2e",
    "gt_ref": "['patterson2021']",
    "pred_value": "1.2",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.2",
      "qwen32b-bench": "1.2",
      "qwen14b-bench": "1.2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q008",
    "question": "When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",
    "gt_value": "43.94",
    "gt_unit": "score",
    "gt_ref": "['li2025a']",
    "pred_value": "43.94",
    "pred_unit": "score",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "43.94",
      "qwen32b-bench": "43.94",
      "qwen14b-bench": "43.94"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q010",
    "question": "By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?",
    "gt_value": "6750",
    "gt_unit": "fold",
    "gt_ref": "['wu2021b']",
    "pred_value": "6750",
    "pred_unit": "fold",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "6750",
      "qwen32b-bench": "6750",
      "qwen14b-bench": "6750"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q011",
    "question": "How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",
    "gt_value": "14.8",
    "gt_unit": "days",
    "gt_ref": "['patterson2021']",
    "pred_value": "14.8",
    "pred_unit": "days",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "14.8",
      "qwen32b-bench": "14.8",
      "qwen14b-bench": "14.8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q012",
    "question": "What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?",
    "gt_value": "0.036",
    "gt_unit": "kWh",
    "gt_ref": "['morrison2025']",
    "pred_value": "0.036",
    "pred_unit": "kWh",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.036",
      "qwen32b-bench": "0.036",
      "qwen14b-bench": "0.036"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q013",
    "question": "What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",
    "gt_value": "13000",
    "gt_unit": "tons",
    "gt_ref": "['han2024']",
    "pred_value": "13000",
    "pred_unit": "tons",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "13000",
      "qwen32b-bench": "13000",
      "qwen14b-bench": "13000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q014",
    "question": "A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?",
    "gt_value": "72",
    "gt_unit": "percent",
    "gt_ref": "['li2025a']",
    "pred_value": "72",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "72",
      "qwen32b-bench": "72",
      "qwen14b-bench": "72"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q015",
    "question": "Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?",
    "gt_value": "1300",
    "gt_unit": "deaths",
    "gt_ref": "['han2024']",
    "pred_value": "is_blank",
    "pred_unit": "deaths",
    "pred_ref": "[\"han2024\", \"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q016",
    "question": "Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?",
    "gt_value": "60",
    "gt_unit": "days",
    "gt_ref": "['dodge2022']",
    "pred_value": "60",
    "pred_unit": "days",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "60",
      "qwen32b-bench": "60",
      "qwen14b-bench": "60"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q017",
    "question": "For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",
    "gt_value": "Command-R Plus",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "is_blank",
    "pred_unit": "is_blank",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q018",
    "question": "In what year was the One Hundred Year Study on Artificial Intelligence launched?",
    "gt_value": "2014",
    "gt_unit": "year",
    "gt_ref": "['stone2022']",
    "pred_value": "2014",
    "pred_unit": "year",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2014",
      "qwen32b-bench": "2014",
      "qwen14b-bench": "2014"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q019",
    "question": "According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",
    "gt_value": "22",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "22",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "22",
      "qwen32b-bench": "22",
      "qwen14b-bench": "22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q020",
    "question": "What is the energy consumption (in MWh) for pre-training the BLOOM model?",
    "gt_value": "520",
    "gt_unit": "MWh",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "51686",
    "pred_unit": "MWh",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "51686",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q021",
    "question": "What percentage of the Switch Transformer's 1500 billion parameters are activated per token?",
    "gt_value": "0.1",
    "gt_unit": "percent",
    "gt_ref": "['patterson2021']",
    "pred_value": "0.1",
    "pred_unit": "percent",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.1",
      "qwen32b-bench": "0.1",
      "qwen14b-bench": "0.1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q022",
    "question": "The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?",
    "gt_value": "8",
    "gt_unit": "experts",
    "gt_ref": "['shen2024']",
    "pred_value": "8",
    "pred_unit": "experts",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8",
      "qwen32b-bench": "8",
      "qwen14b-bench": "8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q023",
    "question": "What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",
    "gt_value": "1",
    "gt_unit": "second",
    "gt_ref": "['xia2024']",
    "pred_value": "is_blank",
    "pred_unit": "second",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q024",
    "question": "According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",
    "gt_value": "28.22",
    "gt_unit": "zettaFLOPs",
    "gt_ref": "['li2025a']",
    "pred_value": "28.22",
    "pred_unit": "zettaFLOPs",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "28.22",
      "qwen32b-bench": "28.22",
      "qwen14b-bench": "28.22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q025",
    "question": "Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?",
    "gt_value": "Intel Core i7-1165G7",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "11th Gen Intel(R) Core(TM) i7-1165G7",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11th Gen Intel(R) Core(TM) i7-1165G7",
      "qwen32b-bench": "11th Gen Intel(R) Core(TM) i7-1165G7 processor",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q026",
    "question": "How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?",
    "gt_value": "88",
    "gt_unit": "models",
    "gt_ref": "['luccioni2024']",
    "pred_value": "is_blank",
    "pred_unit": "models",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q027",
    "question": "By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?",
    "gt_value": "3",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "3",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3",
      "qwen32b-bench": "3",
      "qwen14b-bench": "3"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q028",
    "question": "Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?",
    "gt_value": "[1.2,4]",
    "gt_unit": "multiplier",
    "gt_ref": "['cottier2024']",
    "pred_value": "1.2 to 4",
    "pred_unit": "multiplier",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.2 to 4",
      "qwen32b-bench": "1.2x to 4x",
      "qwen14b-bench": "1.2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q029",
    "question": "What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?",
    "gt_value": "103.5",
    "gt_unit": "MWh",
    "gt_ref": "['dodge2022']",
    "pred_value": "103.5",
    "pred_unit": "MWh",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "103.5",
      "qwen32b-bench": "103.5",
      "qwen14b-bench": "103593"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q030",
    "question": "The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?",
    "gt_value": "Jevons' Paradox",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "Jevons' Paradox",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Jevons' Paradox",
      "qwen32b-bench": "Jevons' Paradox",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q031",
    "question": "By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",
    "gt_value": "[4.2,6.6]",
    "gt_unit": "billion cubic meters",
    "gt_ref": "['li2025b']",
    "pred_value": "4.2 \u2013 6.6",
    "pred_unit": "billion cubic meters",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "4.2 - 6.6",
      "qwen32b-bench": "4.2 \u2013 6.6",
      "qwen14b-bench": "4.2 \u2013 6.6"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q032",
    "question": "True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['schwartz2019']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q033",
    "question": "Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",
    "gt_value": "21.54",
    "gt_unit": "days",
    "gt_ref": "['li2025a']",
    "pred_value": "21.54",
    "pred_unit": "days",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "21.54",
      "qwen32b-bench": "21.54",
      "qwen14b-bench": "21.54"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q034",
    "question": "True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['wu2021a']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q035",
    "question": "How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?",
    "gt_value": "1287",
    "gt_unit": "MWh",
    "gt_ref": "['jegham2025']",
    "pred_value": "1287",
    "pred_unit": "MWh",
    "pred_ref": "[\"jegham2025\", \"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1287",
      "qwen32b-bench": "1287",
      "qwen14b-bench": "1287"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\", \"jegham2025\"]",
      "qwen32b-bench": "[\"li2025b\", \"jegham2025\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q036",
    "question": "What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?",
    "gt_value": "AI Energy Score",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "AI Energy Score",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "AI Energy Score",
      "qwen32b-bench": "AI Energy Score",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q037",
    "question": "For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",
    "gt_value": "1000",
    "gt_unit": "microseconds",
    "gt_ref": "['xia2024']",
    "pred_value": "is_blank",
    "pred_unit": "microseconds",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q038",
    "question": "In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",
    "gt_value": "2",
    "gt_unit": "experts",
    "gt_ref": "['shen2024']",
    "pred_value": "2",
    "pred_unit": "experts",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2",
      "qwen32b-bench": "2",
      "qwen14b-bench": "2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q039",
    "question": "True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['schwartz2019']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q040",
    "question": "What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?",
    "gt_value": "6.4",
    "gt_unit": "percent",
    "gt_ref": "['wu2021b']",
    "pred_value": "6.4",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "6.4",
      "qwen32b-bench": "6.4",
      "qwen14b-bench": "6.4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q041",
    "question": "In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",
    "gt_value": "22",
    "gt_unit": "data centers",
    "gt_ref": "['amazon2023']",
    "pred_value": "22",
    "pred_unit": "data centers",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "22",
      "qwen32b-bench": "22",
      "qwen14b-bench": "22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q042",
    "question": "What is the approximate age of the field of Artificial Intelligence in 2025?",
    "gt_value": "69",
    "gt_unit": "years",
    "gt_ref": "['stone2022']",
    "pred_value": "69",
    "pred_unit": "years",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "69",
      "qwen32b-bench": "69",
      "qwen14b-bench": "69"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q043",
    "question": "The well-known \"five cars\" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",
    "gt_value": "Neural architecture search (NAS)",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "Neural Architecture Search (NAS)",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Neural Architecture Search (NAS)",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "Neural Architecture Search (NAS)"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q044",
    "question": "For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",
    "gt_value": "44",
    "gt_unit": "percent",
    "gt_ref": "['chung2025']",
    "pred_value": "44",
    "pred_unit": "percent",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "44",
      "qwen32b-bench": "44",
      "qwen14b-bench": "44"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q045",
    "question": "What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?",
    "gt_value": "8",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "0",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "20"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q046",
    "question": "As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",
    "gt_value": "1.3",
    "gt_unit": "GW",
    "gt_ref": "['amazon2023']",
    "pred_value": "1.3",
    "pred_unit": "GW",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.3",
      "qwen32b-bench": "1.3",
      "qwen14b-bench": "1.3"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q047",
    "question": "The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?",
    "gt_value": "2300",
    "gt_unit": "flights",
    "gt_ref": "['jegham2025']",
    "pred_value": "272",
    "pred_unit": "flights",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "272",
      "qwen32b-bench": "272",
      "qwen14b-bench": "272"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q048",
    "question": "What percentage of AI inference workloads in Asia were powered by coal in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "percent",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "percent",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q049",
    "question": "What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "PUE",
    "gt_ref": "is_blank",
    "pred_value": "1.58",
    "pred_unit": "PUE",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.58",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": false
  },
  {
    "id": "q050",
    "question": "During inference, how many of JetMoE-8B's parameters are activated for each input token?",
    "gt_value": "2000000000",
    "gt_unit": "parameters",
    "gt_ref": "['shen2024']",
    "pred_value": "2B",
    "pred_unit": "parameters",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2B",
      "qwen32b-bench": "2B",
      "qwen14b-bench": "2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q051",
    "question": "What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?",
    "gt_value": "14",
    "gt_unit": "tCO2e",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "14",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "14",
      "qwen32b-bench": "14",
      "qwen14b-bench": "14"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q052",
    "question": "How many Amazon electric delivery vans were added in total across 2022 and 2023?",
    "gt_value": "21600",
    "gt_unit": "electric delivery vans",
    "gt_ref": "['amazon2023']",
    "pred_value": "9200",
    "pred_unit": "electric delivery vans",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "9200",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "9200"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q053",
    "question": "True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['morrison2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q055",
    "question": "How much energy (in Wh) does the o3 model consume for a long prompt?",
    "gt_value": "39.223",
    "gt_unit": "Wh",
    "gt_ref": "['jegham2025']",
    "pred_value": "12.222",
    "pred_unit": "Wh",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "12.222",
      "qwen32b-bench": "12.222",
      "qwen14b-bench": "12.222"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q056",
    "question": "When was the field of Artificial Intelligence officially christened?",
    "gt_value": "1956",
    "gt_unit": "year",
    "gt_ref": "['stone2022']",
    "pred_value": "1956",
    "pred_unit": "year",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1956",
      "qwen32b-bench": "1956",
      "qwen14b-bench": "1956"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q057",
    "question": "What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?",
    "gt_value": "is_blank",
    "gt_unit": "WUE",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "WUE",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q058",
    "question": "True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['wu2021b']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q059",
    "question": "How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?",
    "gt_value": "[3,4]",
    "gt_unit": "joules per token",
    "gt_ref": "['samsi2024']",
    "pred_value": "3-4",
    "pred_unit": "joules per token",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3-4",
      "qwen32b-bench": "3-4",
      "qwen14b-bench": "3-4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q060",
    "question": "By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?",
    "gt_value": "15",
    "gt_unit": "percent",
    "gt_ref": "['wu2021a']",
    "pred_value": "15",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "15",
      "qwen32b-bench": "15",
      "qwen14b-bench": "15"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q061",
    "question": "True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q063",
    "question": "True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['patterson2021']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q064",
    "question": "What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?",
    "gt_value": "25000",
    "gt_unit": "USD",
    "gt_ref": "['schwartz2019']",
    "pred_value": "25000",
    "pred_unit": "USD",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "25000",
      "qwen32b-bench": "25000",
      "qwen14b-bench": "25000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q065",
    "question": "What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?",
    "gt_value": "53",
    "gt_unit": "percent",
    "gt_ref": "['xia2024']",
    "pred_value": "53",
    "pred_unit": "percent",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "53",
      "qwen32b-bench": "53",
      "qwen14b-bench": "53"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q066",
    "question": " A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",
    "gt_value": "83",
    "gt_unit": "MWh",
    "gt_ref": "['luccioni2024']",
    "pred_value": "83",
    "pred_unit": "MWh",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "83",
      "qwen32b-bench": "83",
      "qwen14b-bench": ""
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q067",
    "question": "What was the average global data center PUE in 2023?",
    "gt_value": "1.58",
    "gt_unit": "PUE",
    "gt_ref": "['ebert2024']",
    "pred_value": "1.58",
    "pred_unit": "PUE",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.58",
      "qwen32b-bench": "1.58",
      "qwen14b-bench": "1.58"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q068",
    "question": "How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "wind turbines",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "wind turbines",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q069",
    "question": "In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",
    "gt_value": "49",
    "gt_unit": "percent",
    "gt_ref": "['cottier2024']",
    "pred_value": "49",
    "pred_unit": "percent",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "49",
      "qwen32b-bench": "49",
      "qwen14b-bench": "49"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q070",
    "question": "How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?",
    "gt_value": "17",
    "gt_unit": "people",
    "gt_ref": "['stone2022']",
    "pred_value": "17",
    "pred_unit": "people",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "17",
      "qwen32b-bench": "17",
      "qwen14b-bench": "17"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q071",
    "question": "What percentage of a client device's total carbon footprint is accounted for by its manufacturing?",
    "gt_value": "74",
    "gt_unit": "percent",
    "gt_ref": "['wu2021a']",
    "pred_value": "74",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "74",
      "qwen32b-bench": "74",
      "qwen14b-bench": "74"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q072",
    "question": "True or False: A model with more parameters will always consume more energy during inference.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['chung2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q073",
    "question": "True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['stone2022']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q074",
    "question": "How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?",
    "gt_value": "is_blank",
    "gt_unit": "tCO2e",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q076",
    "question": "What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?",
    "gt_value": "11390",
    "gt_unit": "tCO2e",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "11390",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11390",
      "qwen32b-bench": "11390",
      "qwen14b-bench": "11390"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q077",
    "question": "By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",
    "gt_value": "2.9",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "2.9",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2.9",
      "qwen32b-bench": "2.9",
      "qwen14b-bench": "2.9"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q079",
    "question": "How many miles is the Earth from the Sun?",
    "gt_value": "is_blank",
    "gt_unit": "miles",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "miles",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q080",
    "question": "True or False: The AlphaGo program defeated the human Go champion.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['stone2022']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q081",
    "question": "What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?",
    "gt_value": "Continuous batching",
    "gt_unit": "is_blank",
    "gt_ref": "['fernandez2025']",
    "pred_value": "Continuous batching",
    "pred_unit": "is_blank",
    "pred_ref": "[\"fernandez2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Continuous batching",
      "qwen32b-bench": "Continuous batching",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"fernandez2025\"]",
      "qwen32b-bench": "[\"fernandez2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q082",
    "question": "How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",
    "gt_value": "60",
    "gt_unit": "H100 GPU hours",
    "gt_ref": "['shen2024']",
    "pred_value": "60",
    "pred_unit": "H100 GPU hours",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "60",
      "qwen32b-bench": "60",
      "qwen14b-bench": "60"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q083",
    "question": "In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",
    "gt_value": "26.7",
    "gt_unit": "percent",
    "gt_ref": "['kim2025']",
    "pred_value": "26.7",
    "pred_unit": "percent",
    "pred_ref": "[\"kim2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "26.7",
      "qwen32b-bench": "26.7",
      "qwen14b-bench": "26.7"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"kim2025\"]",
      "qwen32b-bench": "[\"kim2025\"]",
      "qwen14b-bench": "[\"kim2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q084",
    "question": "The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",
    "gt_value": "1594",
    "gt_unit": "g CO2eq",
    "gt_ref": "['luccioni2024']",
    "pred_value": "1594",
    "pred_unit": "g CO2eq",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1594",
      "qwen32b-bench": "1594",
      "qwen14b-bench": "1594"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q085",
    "question": "What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?",
    "gt_value": "[0.06,3,426]",
    "gt_unit": "Wh",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "0.06 to 3426",
    "pred_unit": "Wh",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.06 to 3426",
      "qwen32b-bench": "0.06 Wh to over 3,426 Wh",
      "qwen14b-bench": "0.06 to over 3426"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q086",
    "question": "True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q087",
    "question": "What was the gross carbon intensity of energy according to the U.S. average mix in 2021?",
    "gt_value": "0.429",
    "gt_unit": "kg of CO2e/KWh",
    "gt_ref": "['patterson2021']",
    "pred_value": "0.429",
    "pred_unit": "kg of CO2e/KWh",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.429",
      "qwen32b-bench": "0.429",
      "qwen14b-bench": "0.429"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q088",
    "question": "What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?",
    "gt_value": "Hivemind",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "Hivemind",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Hivemind",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q089",
    "question": "What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?",
    "gt_value": "Social transparency",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "Social transparency",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Social transparency",
      "qwen32b-bench": "Social transparency",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q090",
    "question": "In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",
    "gt_value": "Linear Embedding",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "traditional linear model using pre-trained sentence embeddings",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "traditional linear model using pre-trained sentence embeddings",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q092",
    "question": "What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",
    "gt_value": "Lamina",
    "gt_unit": "is_blank",
    "gt_ref": "['chen2024']",
    "pred_value": "Lamina",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Lamina",
      "qwen32b-bench": "Lamina",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chen2024\"]",
      "qwen32b-bench": "[\"chen2024\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q093",
    "question": "How many parameters does the largest T5 model have?",
    "gt_value": "11000000000",
    "gt_unit": "parameters",
    "gt_ref": "['patterson2021']",
    "pred_value": "is_blank",
    "pred_unit": "parameters",
    "pred_ref": "[\"is_blank\", \"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q094",
    "question": "What is the total number of parameters in the JetMoE-8B model?",
    "gt_value": "8000000000",
    "gt_unit": "parameters",
    "gt_ref": "['shen2024']",
    "pred_value": "8B",
    "pred_unit": "parameters",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8B",
      "qwen32b-bench": "8B",
      "qwen14b-bench": "8000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q095",
    "question": "By what percentage did Google's data center water consumption increase from 2021 to 2022?",
    "gt_value": "20",
    "gt_unit": "percent",
    "gt_ref": "['li2025b']",
    "pred_value": "20",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "20",
      "qwen32b-bench": "20",
      "qwen14b-bench": "20"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q096",
    "question": "What is the name of the emissions metric defined as 'CO\u2082 emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?",
    "gt_value": "Carbon Intensity",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "Carbon Intensity",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Carbon Intensity",
      "qwen32b-bench": "Carbon Intensity",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q097",
    "question": "In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",
    "gt_value": "52.88",
    "gt_unit": "percent",
    "gt_ref": "['li2025a']",
    "pred_value": "52.88",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "52.88",
      "qwen32b-bench": "52.88",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q098",
    "question": "What were the estimated amortized training costs for OpenAI's GPT-4?",
    "gt_value": "40000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "40000000",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "40000000",
      "qwen32b-bench": "40000000",
      "qwen14b-bench": "40"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q099",
    "question": "Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",
    "gt_value": "810",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "810",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "810",
      "qwen32b-bench": "810",
      "qwen14b-bench": "810"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q100",
    "question": "What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?",
    "gt_value": "0.59",
    "gt_unit": "multiplier",
    "gt_ref": "['erben2023']",
    "pred_value": "0.59",
    "pred_unit": "multiplier",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.59",
      "qwen32b-bench": "0.59",
      "qwen14b-bench": "0.59"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"erben2023\"]",
      "qwen14b-bench": "[\"erben2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q101",
    "question": "How many liters of water were returned to communities from Amazon's replenishment projects in 2023?",
    "gt_value": "3500000000",
    "gt_unit": "liters",
    "gt_ref": "['amazon2023']",
    "pred_value": "3500000000",
    "pred_unit": "liters",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3500000000",
      "qwen32b-bench": "3.5 billion",
      "qwen14b-bench": "3500000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q103",
    "question": "True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['rubei2025']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"rubei2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"rubei2025\"]",
      "qwen32b-bench": "[\"rubei2025\"]",
      "qwen14b-bench": "[\"rubei2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q104",
    "question": "As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",
    "gt_value": "3700000",
    "gt_unit": "GPUs",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "3.7",
    "pred_unit": "GPUs",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3.7",
      "qwen32b-bench": "3.7 million",
      "qwen14b-bench": "3700000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q107",
    "question": "What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",
    "gt_value": "44",
    "gt_unit": "percent",
    "gt_ref": "['cottier2024']",
    "pred_value": "44",
    "pred_unit": "percent",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "44",
      "qwen32b-bench": "44",
      "qwen14b-bench": "44"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q108",
    "question": "What is the Power Usage Effectiveness (PUE) for Facebook's data centers?",
    "gt_value": "1.1",
    "gt_unit": "PUE",
    "gt_ref": "['wu2021a']",
    "pred_value": "1.10",
    "pred_unit": "PUE",
    "pred_ref": "[\"wu2021a\", \"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.10",
      "qwen32b-bench": "1.10",
      "qwen14b-bench": "1.10"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\", \"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q109",
    "question": "What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",
    "gt_value": "ETAIROS",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "ETAIROS",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "ETAIROS",
      "qwen32b-bench": "ETAIROS",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q110",
    "question": "What were the estimated amortized training costs for Google's Gemini Ultra?",
    "gt_value": "30000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "30000000",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "30000000",
      "qwen32b-bench": "30000000",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q111",
    "question": "True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q112",
    "question": "What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?",
    "gt_value": "9",
    "gt_unit": "\u00b5g/m\u00b3",
    "gt_ref": "['han2024']",
    "pred_value": "9",
    "pred_unit": "\u00b5g/m\u00b3",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "9",
      "qwen32b-bench": "9",
      "qwen14b-bench": "9"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q113",
    "question": "A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?",
    "gt_value": "115",
    "gt_unit": "books",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "115",
    "pred_unit": "books",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "115",
      "qwen32b-bench": "115",
      "qwen14b-bench": "115"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q114",
    "question": "According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",
    "gt_value": "200",
    "gt_unit": "multiplier",
    "gt_ref": "['han2024']",
    "pred_value": "200",
    "pred_unit": "multiplier",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "200",
      "qwen32b-bench": "200",
      "qwen14b-bench": "200"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q115",
    "question": "What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?",
    "gt_value": "702.06",
    "gt_unit": "Wh",
    "gt_ref": "['zschache2025']",
    "pred_value": "702.06",
    "pred_unit": "Wh",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "702.06",
      "qwen32b-bench": "702.06",
      "qwen14b-bench": "702.06"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"zschache2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q116",
    "question": "According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",
    "gt_value": "6100000000",
    "gt_unit": "parameters",
    "gt_ref": "['dodge2022']",
    "pred_value": "is_blank",
    "pred_unit": "parameters",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q117",
    "question": "What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",
    "gt_value": "Jevons paradox",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "Jevons' Paradox",
    "pred_unit": "is_blank",
    "pred_ref": "[\"jegham2025\", \"luccioni2025a\", \"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Jevons' Paradox",
      "qwen32b-bench": "Jevons' Paradox",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\", \"luccioni2025a\", \"jegham2025\"]",
      "qwen32b-bench": "[\"morrison2025\", \"luccioni2025a\", \"jegham2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q118",
    "question": "How many Meena training runs would use the same total energy as a single full training run of GPT-3?",
    "gt_value": "5",
    "gt_unit": "multiplier",
    "gt_ref": "['patterson2021']",
    "pred_value": "5.3",
    "pred_unit": "multiplier",
    "pred_ref": "[\"li2025b\", \"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "5.3",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "12.43"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\", \"patterson2021\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"li2025b\", \"dodge2022\"]"
    },
    "value_correct": false,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q119",
    "question": "According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",
    "gt_value": "2.907",
    "gt_unit": "kWh",
    "gt_ref": "['luccioni2024']",
    "pred_value": "2.907",
    "pred_unit": "kWh",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2.907",
      "qwen32b-bench": "2.907",
      "qwen14b-bench": "2.907"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q120",
    "question": "How many pounds of CO2e are estimated for an average American life in one year?",
    "gt_value": "36156",
    "gt_unit": "lbs",
    "gt_ref": "['strubell2019']",
    "pred_value": "36156",
    "pred_unit": "lbs",
    "pred_ref": "[\"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "36156",
      "qwen32b-bench": "36,156",
      "qwen14b-bench": "36156"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"strubell2019\"]",
      "qwen32b-bench": "[\"strubell2019\"]",
      "qwen14b-bench": "[\"strubell2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q121",
    "question": "According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",
    "gt_value": "Mason County",
    "gt_unit": "is_blank",
    "gt_ref": "['han2024']",
    "pred_value": "Marion",
    "pred_unit": "is_blank",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Marion",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q122",
    "question": "By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?",
    "gt_value": "1.33",
    "gt_unit": "multiplier",
    "gt_ref": "['khan2025']",
    "pred_value": "0.75",
    "pred_unit": "multiplier",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.75",
      "qwen32b-bench": "0.75",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q123",
    "question": "What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?",
    "gt_value": "59257",
    "gt_unit": "kWh",
    "gt_ref": "['luccioni2024']",
    "pred_value": "59257",
    "pred_unit": "kWh",
    "pred_ref": "[\"ebert2024\", \"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "59257",
      "qwen32b-bench": "59257",
      "qwen14b-bench": "59257"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"ebert2024\", \"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q125",
    "question": "What is the total number of parameters in the final FLM-101B model?",
    "gt_value": "1.01E+11",
    "gt_unit": "parameters",
    "gt_ref": "['li2025a']",
    "pred_value": "101",
    "pred_unit": "parameters",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "101",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q126",
    "question": "Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",
    "gt_value": "1035000000",
    "gt_unit": "inferences",
    "gt_ref": "['dodge2022', 'luccioni2024']",
    "pred_value": "592570000",
    "pred_unit": "inferences",
    "pred_ref": "[\"dodge2022\", \"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "592570000",
      "qwen32b-bench": "1,035,930,000",
      "qwen14b-bench": ""
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\", \"luccioni2024\"]",
      "qwen32b-bench": "[\"dodge2022\", \"luccioni2024\"]",
      "qwen14b-bench": "[]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q127",
    "question": "In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",
    "gt_value": "754.66",
    "gt_unit": "kWh",
    "gt_ref": "['luccioni2024']",
    "pred_value": "754.66",
    "pred_unit": "kWh",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "754.66",
      "qwen32b-bench": "754.66",
      "qwen14b-bench": "754.66"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q128",
    "question": "For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",
    "gt_value": "592570000",
    "gt_unit": "inferences",
    "gt_ref": "['luccioni2024']",
    "pred_value": "592570000",
    "pred_unit": "inferences",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "592570000",
      "qwen32b-bench": "592570000",
      "qwen14b-bench": "592570000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q129",
    "question": "What dataset name is used for the German nuclear waste site objection texts classified in the experiments?",
    "gt_value": "FKTG",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "FKTG-dataset",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "FKTG-dataset",
      "qwen32b-bench": "FKTG-dataset",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q130",
    "question": "How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?",
    "gt_value": "is_blank",
    "gt_unit": "liters",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "liters",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q131",
    "question": "What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?",
    "gt_value": "is_blank",
    "gt_unit": "percent",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "percent",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q132",
    "question": "The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?",
    "gt_value": "3",
    "gt_unit": "passengers",
    "gt_ref": "['patterson2021']",
    "pred_value": "3",
    "pred_unit": "passengers",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2.67",
      "qwen32b-bench": "3",
      "qwen14b-bench": "3"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q133",
    "question": "According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",
    "gt_value": "84",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "84",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "84",
      "qwen32b-bench": "84",
      "qwen14b-bench": "84"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q134",
    "question": "What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?",
    "gt_value": "1",
    "gt_unit": "A100_80GB_GPU",
    "gt_ref": "['samsi2024']",
    "pred_value": "1",
    "pred_unit": "A100_80GB_GPU",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q136",
    "question": "What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?",
    "gt_value": "[21,78]",
    "gt_unit": "metric tons",
    "gt_ref": "['dodge2022']",
    "pred_value": "21 to 78",
    "pred_unit": "metric tons",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "21 to 78",
      "qwen32b-bench": "21 to 78",
      "qwen14b-bench": "21 to 78"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q137",
    "question": "What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "tCO2e",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q138",
    "question": "In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",
    "gt_value": "24",
    "gt_unit": "percent",
    "gt_ref": "['griggs2024']",
    "pred_value": "24",
    "pred_unit": "percent",
    "pred_ref": "[\"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "24",
      "qwen32b-bench": "24",
      "qwen14b-bench": "24"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"griggs2024\"]",
      "qwen32b-bench": "[\"griggs2024\"]",
      "qwen14b-bench": "[\"griggs2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q140",
    "question": "According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",
    "gt_value": "4.63",
    "gt_unit": "USD per hour",
    "gt_ref": "['chen2024']",
    "pred_value": "4.63",
    "pred_unit": "USD per hour",
    "pred_ref": "[\"chen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "4.63",
      "qwen32b-bench": "4.63",
      "qwen14b-bench": "4.63"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chen2024\"]",
      "qwen32b-bench": "[\"chen2024\"]",
      "qwen14b-bench": "[\"chen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q141",
    "question": "True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q142",
    "question": "In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",
    "gt_value": "43",
    "gt_unit": "percent",
    "gt_ref": "['han2024']",
    "pred_value": "44",
    "pred_unit": "percent",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "44",
      "qwen32b-bench": "44",
      "qwen14b-bench": "44"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q143",
    "question": "What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?",
    "gt_value": "1",
    "gt_unit": "A100_80GB_GPU",
    "gt_ref": "['samsi2024']",
    "pred_value": "1",
    "pred_unit": "A100_80GB_GPU",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": ""
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q144",
    "question": "True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"khan2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q145",
    "question": "How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?",
    "gt_value": "95",
    "gt_unit": "answers",
    "gt_ref": "['luccioni2025b']",
    "pred_value": "95",
    "pred_unit": "answers",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "95",
      "qwen32b-bench": "95",
      "qwen14b-bench": "95"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q147",
    "question": "Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",
    "gt_value": "3.33",
    "gt_unit": "USD per hour",
    "gt_ref": "['shen2024']",
    "pred_value": "3.33",
    "pred_unit": "USD per hour",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3.33",
      "qwen32b-bench": "3.33",
      "qwen14b-bench": "3.33"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q148",
    "question": "When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",
    "gt_value": "122",
    "gt_unit": "percent",
    "gt_ref": "['han2024']",
    "pred_value": "120",
    "pred_unit": "percent",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "120",
      "qwen32b-bench": "120",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q149",
    "question": "How many tokens were used to pre-train the JetMoE-8B model?",
    "gt_value": "1.25E+12",
    "gt_unit": "tokens",
    "gt_ref": "['shen2024']",
    "pred_value": "1250000000000",
    "pred_unit": "tokens",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1250000000000",
      "qwen32b-bench": "1.25T",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q150",
    "question": "As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",
    "gt_value": "36",
    "gt_unit": "projects",
    "gt_ref": "['amazon2023']",
    "pred_value": "36",
    "pred_unit": "projects",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "36",
      "qwen32b-bench": "36",
      "qwen14b-bench": "36"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q151",
    "question": "In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",
    "gt_value": "53.2",
    "gt_unit": "percent",
    "gt_ref": "['amazon2023']",
    "pred_value": "44.1",
    "pred_unit": "percent",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "52.9",
      "qwen32b-bench": "44.1",
      "qwen14b-bench": "44.1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q152",
    "question": "What percentage of Apple's total water footprint is accounted for by its supply chain?",
    "gt_value": "99",
    "gt_unit": "percent",
    "gt_ref": "['li2025b']",
    "pred_value": "99",
    "pred_unit": "percent",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "99",
      "qwen32b-bench": "99",
      "qwen14b-bench": "99"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q154",
    "question": "What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",
    "gt_value": "1.5",
    "gt_unit": "seconds",
    "gt_ref": "['xia2024']",
    "pred_value": "is_blank",
    "pred_unit": "seconds",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q155",
    "question": "Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?",
    "gt_value": "Granularity",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "granularity metric",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "granularity metric",
      "qwen32b-bench": "granularity metric",
      "qwen14b-bench": "granularity"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"erben2023\"]",
      "qwen14b-bench": "[\"erben2023\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q156",
    "question": "According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",
    "gt_value": "6.4",
    "gt_unit": "times",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "640",
    "pred_unit": "times",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "640",
      "qwen32b-bench": "640",
      "qwen14b-bench": "640"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q157",
    "question": "What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",
    "gt_value": "Water withdrawal",
    "gt_unit": "is_blank",
    "gt_ref": "['li2025b']",
    "pred_value": "Water withdrawal",
    "pred_unit": "is_blank",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Water withdrawal",
      "qwen32b-bench": "Water withdrawal",
      "qwen14b-bench": "Water withdrawal"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q159",
    "question": "How often does the Standing Committee of the One Hundred Year Study form a Study Panel?",
    "gt_value": "5",
    "gt_unit": "years",
    "gt_ref": "['stone2022']",
    "pred_value": "5",
    "pred_unit": "years",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "5",
      "qwen32b-bench": "5",
      "qwen14b-bench": "5"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q160",
    "question": "What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",
    "gt_value": "25",
    "gt_unit": "devices",
    "gt_ref": "['wu2021b']",
    "pred_value": "25",
    "pred_unit": "devices",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "25",
      "qwen32b-bench": "25",
      "qwen14b-bench": "25"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q161",
    "question": "Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?",
    "gt_value": "[0.8,3500]",
    "gt_unit": "MWh",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "0.8 to 3500",
    "pred_unit": "MWh",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.8 to 3500",
      "qwen32b-bench": "0.8 to 3,500",
      "qwen14b-bench": "0.8 to 3500"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q162",
    "question": "True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['stone2022']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"stone2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"stone2022\"]",
      "qwen32b-bench": "[\"stone2022\"]",
      "qwen14b-bench": "[\"stone2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q163",
    "question": "One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?",
    "gt_value": "[10,50]",
    "gt_unit": "queries",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "10-50",
    "pred_unit": "queries",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "10-50",
      "qwen32b-bench": "10\u201350",
      "qwen14b-bench": "10"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q165",
    "question": "After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",
    "gt_value": "6.681",
    "gt_unit": "score",
    "gt_ref": "['shen2024']",
    "pred_value": "6.681",
    "pred_unit": "score",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "6.681",
      "qwen32b-bench": "6.681",
      "qwen14b-bench": "6.681"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q167",
    "question": "How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?",
    "gt_value": "[10,50]",
    "gt_unit": "responses",
    "gt_ref": "['li2025b']",
    "pred_value": "10 - 50",
    "pred_unit": "responses",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "10 - 50",
      "qwen32b-bench": "between 10 and 50",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q168",
    "question": "The 2024 Griggs et al. paper reports that M\u00e9lange can reduce deployment costs by up to what percentage in conversational chat settings?",
    "gt_value": "77",
    "gt_unit": "percent",
    "gt_ref": "['griggs2024']",
    "pred_value": "77",
    "pred_unit": "percent",
    "pred_ref": "[\"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "77",
      "qwen32b-bench": "77",
      "qwen14b-bench": "77"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"griggs2024\"]",
      "qwen32b-bench": "[\"griggs2024\"]",
      "qwen14b-bench": "[\"griggs2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q169",
    "question": "What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?",
    "gt_value": "4",
    "gt_unit": "A100_80GB_GPUs",
    "gt_ref": "['samsi2024']",
    "pred_value": "4",
    "pred_unit": "A100_80GB_GPUs",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "4",
      "qwen32b-bench": "4",
      "qwen14b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q171",
    "question": "Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?",
    "gt_value": "10000",
    "gt_unit": "round trips",
    "gt_ref": "['han2024']",
    "pred_value": "more than 10,000",
    "pred_unit": "round trips",
    "pred_ref": "[\"han2024\", \"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "more than 10,000",
      "qwen32b-bench": "more than 10,000",
      "qwen14b-bench": "10000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\", \"luccioni2025c\"]",
      "qwen32b-bench": "[\"han2024\", \"luccioni2025c\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q172",
    "question": "What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?",
    "gt_value": "[80,90]",
    "gt_unit": "percent",
    "gt_ref": "['patterson2021']",
    "pred_value": "80-90",
    "pred_unit": "percent",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "80-90",
      "qwen32b-bench": "80\u201390",
      "qwen14b-bench": "85"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q173",
    "question": "Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",
    "gt_value": "178.97",
    "gt_unit": "kg CO2eq",
    "gt_ref": "['luccioni2024']",
    "pred_value": "178.97",
    "pred_unit": "kg CO2eq",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "178.97",
      "qwen32b-bench": "178.97",
      "qwen14b-bench": "178.97"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q174",
    "question": "True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['chung2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q175",
    "question": "True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['jegham2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q176",
    "question": "What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",
    "gt_value": "4",
    "gt_unit": "queries/sec",
    "gt_ref": "['xia2024']",
    "pred_value": "0.5",
    "pred_unit": "queries/sec",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.5",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q177",
    "question": "True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q178",
    "question": "In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",
    "gt_value": "7.52",
    "gt_unit": "USD per hour",
    "gt_ref": "['griggs2024']",
    "pred_value": "7.5164",
    "pred_unit": "USD per hour",
    "pred_ref": "[\"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "7.5164",
      "qwen32b-bench": "7.5164",
      "qwen14b-bench": "7.5164"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"griggs2024\"]",
      "qwen32b-bench": "[\"griggs2024\"]",
      "qwen14b-bench": "[\"griggs2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q179",
    "question": "How many liters of water were used for cooling during OpenAI's GPT-4 training run?",
    "gt_value": "is_blank",
    "gt_unit": "liters of water",
    "gt_ref": "is_blank",
    "pred_value": "5000000",
    "pred_unit": "liters of water",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "5000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": false
  },
  {
    "id": "q180",
    "question": "Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",
    "gt_value": "3.61",
    "gt_unit": "USD per hour",
    "gt_ref": "['griggs2024']",
    "pred_value": "7.22",
    "pred_unit": "USD per hour",
    "pred_ref": "[\"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "7.22",
      "qwen32b-bench": "1.85",
      "qwen14b-bench": "7.22"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"griggs2024\"]",
      "qwen32b-bench": "[\"griggs2024\"]",
      "qwen14b-bench": "[\"griggs2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q181",
    "question": "To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",
    "gt_value": "1000",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "1000",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1000",
      "qwen32b-bench": "1000",
      "qwen14b-bench": "1000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q182",
    "question": "Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",
    "gt_value": "730000",
    "gt_unit": "miles",
    "gt_ref": "['strubell2019', 'luccioni2024']",
    "pred_value": "is_blank",
    "pred_unit": "miles",
    "pred_ref": "[\"is_blank\", \"luccioni2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2023\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q183",
    "question": "The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.",
    "gt_value": "60610",
    "gt_unit": "MWh",
    "gt_ref": "['luccioni2024']",
    "pred_value": "60609.6",
    "pred_unit": "MWh",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "60609.6",
      "qwen32b-bench": "60609.6",
      "qwen14b-bench": "606.096"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q184",
    "question": "How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?",
    "gt_value": "30000",
    "gt_unit": "H100 GPU hours",
    "gt_ref": "['shen2024']",
    "pred_value": "30000",
    "pred_unit": "H100 GPU hours",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "30000",
      "qwen32b-bench": "30,000",
      "qwen14b-bench": "30000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q185",
    "question": "Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",
    "gt_value": "1000000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "1000000000",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1000000000",
      "qwen32b-bench": "1000000000",
      "qwen14b-bench": "1000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q186",
    "question": "What was the total number of floating point operations to train GPT-3, as published by OpenAI?",
    "gt_value": "3.14E+23",
    "gt_unit": "FLOPS",
    "gt_ref": "['patterson2021']",
    "pred_value": "3.14E+23",
    "pred_unit": "FLOPS",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3.14E+23",
      "qwen32b-bench": "3.14E+23",
      "qwen14b-bench": "3.14E+23"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q187",
    "question": "What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?",
    "gt_value": "8",
    "gt_unit": "V100_32GB_GPUs",
    "gt_ref": "['samsi2024']",
    "pred_value": "8",
    "pred_unit": "V100_32GB_GPUs",
    "pred_ref": "[\"rubei2025\", \"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8",
      "qwen32b-bench": "8",
      "qwen14b-bench": "8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\", \"rubei2025\"]",
      "qwen32b-bench": "[\"samsi2024\", \"rubei2025\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q188",
    "question": "Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",
    "gt_value": "17.9",
    "gt_unit": "zettaFLOPs",
    "gt_ref": "['li2025a']",
    "pred_value": "28.22",
    "pred_unit": "zettaFLOPs",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "28.22",
      "qwen32b-bench": "17.5",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q189",
    "question": "What is the top-1 accuracy on ImageNet associated with AlexNet 2012?",
    "gt_value": "56.4",
    "gt_unit": "percent",
    "gt_ref": "['schwartz2019']",
    "pred_value": "57.1",
    "pred_unit": "percent",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "57.1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q190",
    "question": "How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",
    "gt_value": "192",
    "gt_unit": "GPUs",
    "gt_ref": "['li2025a']",
    "pred_value": "192",
    "pred_unit": "GPUs",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "192",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "192"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q191",
    "question": "What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?",
    "gt_value": "17.31",
    "gt_unit": "lifetimes",
    "gt_ref": "['strubell2019']",
    "pred_value": "5",
    "pred_unit": "lifetimes",
    "pred_ref": "[\"dodge2022\", \"luccioni2023\", \"patterson2021\", \"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "5",
      "qwen32b-bench": "17.3",
      "qwen14b-bench": "626155"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\", \"luccioni2023\", \"patterson2021\", \"strubell2019\"]",
      "qwen32b-bench": "[\"strubell2019\", \"luccioni2023\"]",
      "qwen14b-bench": "[\"strubell2019\", \"luccioni2023\"]"
    },
    "value_correct": false,
    "ref_score": 0.25,
    "na_correct": true
  },
  {
    "id": "q192",
    "question": "How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?",
    "gt_value": "25000",
    "gt_unit": "hours",
    "gt_ref": "['schwartz2019']",
    "pred_value": "25000",
    "pred_unit": "hours",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "25000",
      "qwen32b-bench": "25000",
      "qwen14b-bench": "25000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q193",
    "question": "How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?",
    "gt_value": "47400",
    "gt_unit": "metric tons",
    "gt_ref": "['amazon2023']",
    "pred_value": "47400",
    "pred_unit": "metric tons",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "47400",
      "qwen32b-bench": "47400",
      "qwen14b-bench": "47400"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q194",
    "question": "What framework was used to deploy large language models across multiple GPUs and nodes?",
    "gt_value": "vLLM",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "vllm",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "vllm",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q195",
    "question": "By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?",
    "gt_value": "1.952263374",
    "gt_unit": "multiplier",
    "gt_ref": "['zschache2025']",
    "pred_value": "1.95",
    "pred_unit": "multiplier",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.95",
      "qwen32b-bench": "1.95",
      "qwen14b-bench": "1.95"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"zschache2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q196",
    "question": "How many gallons of water were consumed per ChatGPT user session in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "gallons of water",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "gallons of water",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q197",
    "question": "700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?",
    "gt_value": "35000",
    "gt_unit": "homes",
    "gt_ref": "['jegham2025']",
    "pred_value": "35000",
    "pred_unit": "homes",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "35000",
      "qwen32b-bench": "35,000",
      "qwen14b-bench": "35000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q198",
    "question": "According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",
    "gt_value": "34",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "34",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "34",
      "qwen32b-bench": "34",
      "qwen14b-bench": "34"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q199",
    "question": "True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"zschache2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q201",
    "question": "What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?",
    "gt_value": "1.11",
    "gt_unit": "PUE",
    "gt_ref": "['patterson2021']",
    "pred_value": "1.11",
    "pred_unit": "PUE",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.11",
      "qwen32b-bench": "1.11",
      "qwen14b-bench": "1.11"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q204",
    "question": "What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",
    "gt_value": "7.72E+11",
    "gt_unit": "queries",
    "gt_ref": "['jegham2025']",
    "pred_value": "772",
    "pred_unit": "queries",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "772",
      "qwen32b-bench": "772",
      "qwen14b-bench": "772000000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q205",
    "question": "What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?",
    "gt_value": "53",
    "gt_unit": "score",
    "gt_ref": "['shen2024']",
    "pred_value": "53.0",
    "pred_unit": "score",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "53.0",
      "qwen32b-bench": "53.0",
      "qwen14b-bench": "53.0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q206",
    "question": "How many AI training runs were conducted globally on renewable-only power in 2022?",
    "gt_value": "is_blank",
    "gt_unit": "training runs",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "training runs",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q208",
    "question": "True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q209",
    "question": "What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?",
    "gt_value": "1.59",
    "gt_unit": "PUE",
    "gt_ref": "['patterson2021']",
    "pred_value": "1.58",
    "pred_unit": "PUE",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.58",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "1.58"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q210",
    "question": "In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",
    "gt_value": "5.312",
    "gt_unit": "GB",
    "gt_ref": "['kim2025']",
    "pred_value": "5.312",
    "pred_unit": "GB",
    "pred_ref": "[\"kim2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "5.312",
      "qwen32b-bench": "5.312",
      "qwen14b-bench": "5.312"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"kim2025\"]",
      "qwen32b-bench": "[\"kim2025\"]",
      "qwen14b-bench": "[\"kim2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q212",
    "question": "For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",
    "gt_value": "[29,49]",
    "gt_unit": "percent",
    "gt_ref": "['cottier2024']",
    "pred_value": "29-49",
    "pred_unit": "percent",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "29 to 49",
      "qwen32b-bench": "29-49",
      "qwen14b-bench": "29-49"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q213",
    "question": "Which software package was used to measure energy consumption during inference runs?",
    "gt_value": "CodeCarbon",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "CodeCarbon",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "CodeCarbon",
      "qwen32b-bench": "CodeCarbon",
      "qwen14b-bench": "CodeCarbon package"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"zschache2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q214",
    "question": "According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",
    "gt_value": "53",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "53",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "53",
      "qwen32b-bench": "53",
      "qwen14b-bench": "53"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q216",
    "question": "What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?",
    "gt_value": "Compute Time Calibration Function",
    "gt_unit": "is_blank",
    "gt_ref": "['kim2025']",
    "pred_value": "Compute Time Calibration Function (CTCF)",
    "pred_unit": "is_blank",
    "pred_ref": "[\"kim2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Compute Time Calibration Function (CTCF)",
      "qwen32b-bench": "Compute Time Calibration Function (CTCF)",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"kim2025\"]",
      "qwen32b-bench": "[\"kim2025\"]",
      "qwen14b-bench": "[\"kim2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q217",
    "question": "True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['samsi2024']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q218",
    "question": "What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",
    "gt_value": "11",
    "gt_unit": "kL",
    "gt_ref": "['morrison2025']",
    "pred_value": "0.035",
    "pred_unit": "kL",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.035",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "110"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q219",
    "question": "True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q220",
    "question": "One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",
    "gt_value": "30",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "30",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025a\", \"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "30",
      "qwen32b-bench": "30",
      "qwen14b-bench": "30"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\", \"wu2021b\"]",
      "qwen32b-bench": "[\"luccioni2025a\", \"wu2021b\"]",
      "qwen14b-bench": "[\"luccioni2025a\", \"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q222",
    "question": "What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",
    "gt_value": "5600000000",
    "gt_unit": "USD",
    "gt_ref": "['han2024']",
    "pred_value": "6700000000",
    "pred_unit": "USD",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "6700000000",
      "qwen32b-bench": "6700",
      "qwen14b-bench": "6700000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q223",
    "question": "By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?",
    "gt_value": "70",
    "gt_unit": "multiplier",
    "gt_ref": "['jegham2025']",
    "pred_value": "14.78",
    "pred_unit": "multiplier",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "14.78",
      "qwen32b-bench": "14.78",
      "qwen14b-bench": "14.77"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q224",
    "question": "In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, M\u00e9lange achieved cost reductions in what percentage range compared to single-GPU baselines?",
    "gt_value": "[15,77]",
    "gt_unit": "percent",
    "gt_ref": "['griggs2024']",
    "pred_value": "15-77",
    "pred_unit": "percent",
    "pred_ref": "[\"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "15-77",
      "qwen32b-bench": "15-77",
      "qwen14b-bench": "15-77"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"griggs2024\"]",
      "qwen32b-bench": "[\"griggs2024\"]",
      "qwen14b-bench": "[\"griggs2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q225",
    "question": "What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?",
    "gt_value": "26",
    "gt_unit": "tCO2e",
    "gt_ref": "['li2025a']",
    "pred_value": "26",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "26",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q226",
    "question": "What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",
    "gt_value": "2",
    "gt_unit": "seconds",
    "gt_ref": "['xia2024']",
    "pred_value": "1980.198",
    "pred_unit": "seconds",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1980.198",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q227",
    "question": "True or False: The public health costs of AI are evenly distributed across communities in the U.S.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['han2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q228",
    "question": "True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['wu2021b']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q229",
    "question": "Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?",
    "gt_value": "Ollama",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "Ollama",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Ollama",
      "qwen32b-bench": "Ollama",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q232",
    "question": "What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?",
    "gt_value": "Backblaze B2",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "Backblaze (B2)",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Backblaze (B2)",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q233",
    "question": "In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q234",
    "question": "Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?",
    "gt_value": "Edward J. Markey",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "Senator Edward J. Markey",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Senator Edward J. Markey",
      "qwen32b-bench": "Senator Edward J. Markey",
      "qwen14b-bench": "Edward J. Markey"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q235",
    "question": "According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",
    "gt_value": "11.06",
    "gt_unit": "USD per hour",
    "gt_ref": "['chen2024']",
    "pred_value": "11.06",
    "pred_unit": "USD per hour",
    "pred_ref": "[\"chen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11.06",
      "qwen32b-bench": "11.06",
      "qwen14b-bench": "11.06"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chen2024\"]",
      "qwen32b-bench": "[\"chen2024\"]",
      "qwen14b-bench": "[\"chen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q236",
    "question": "What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?",
    "gt_value": "is_blank",
    "gt_unit": "years",
    "gt_ref": "is_blank",
    "pred_value": "6",
    "pred_unit": "years",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "6"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": false
  },
  {
    "id": "q237",
    "question": "What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?",
    "gt_value": "2",
    "gt_unit": "V100_32GB_GPUs",
    "gt_ref": "['samsi2024']",
    "pred_value": "2",
    "pred_unit": "V100_32GB_GPUs",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2",
      "qwen32b-bench": "2",
      "qwen14b-bench": "8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q238",
    "question": "What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?",
    "gt_value": "1247.61",
    "gt_unit": "tCO2e",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "1247.61",
    "pred_unit": "tCO2e",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1247.61",
      "qwen32b-bench": "1247.61",
      "qwen14b-bench": "1247.61"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q239",
    "question": "How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?",
    "gt_value": "336",
    "gt_unit": "hours",
    "gt_ref": "['strubell2019']",
    "pred_value": "336",
    "pred_unit": "hours",
    "pred_ref": "[\"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "336",
      "qwen32b-bench": "336",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"strubell2019\"]",
      "qwen32b-bench": "[\"strubell2019\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q240",
    "question": "What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",
    "gt_value": "3.1",
    "gt_unit": "L/kWh",
    "gt_ref": "['li2025b']",
    "pred_value": "3.1",
    "pred_unit": "L/kWh",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3.1",
      "qwen32b-bench": "3.1",
      "qwen14b-bench": "3.1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q241",
    "question": "What was the reported PUE of Google's hyperscale data centers in 2021?",
    "gt_value": "1.1",
    "gt_unit": "PUE",
    "gt_ref": "['wu2021b']",
    "pred_value": "1.10",
    "pred_unit": "PUE",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.10",
      "qwen32b-bench": "1.10",
      "qwen14b-bench": "1.10"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q242",
    "question": "According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",
    "gt_value": "96",
    "gt_unit": "percent",
    "gt_ref": "['amazon2023']",
    "pred_value": "96",
    "pred_unit": "percent",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "96",
      "qwen32b-bench": "96",
      "qwen14b-bench": "96"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q243",
    "question": "What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?",
    "gt_value": "3460",
    "gt_unit": "USD",
    "gt_ref": "['xia2024']",
    "pred_value": "3460",
    "pred_unit": "USD",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "3460",
      "qwen32b-bench": "3460",
      "qwen14b-bench": "3460"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q244",
    "question": "In a typical datacenter, GPUs account for what percentage of the total provisioned power?",
    "gt_value": "[50,70]",
    "gt_unit": "percent",
    "gt_ref": "['chung2025']",
    "pred_value": "74",
    "pred_unit": "percent",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "74",
      "qwen32b-bench": "74",
      "qwen14b-bench": "74"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q245",
    "question": "The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?",
    "gt_value": "96",
    "gt_unit": "H100 GPUs",
    "gt_ref": "['shen2024']",
    "pred_value": "96",
    "pred_unit": "H100 GPUs",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "96",
      "qwen32b-bench": "96",
      "qwen14b-bench": "96"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q247",
    "question": "During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",
    "gt_value": "600",
    "gt_unit": "Watts",
    "gt_ref": "['morrison2025']",
    "pred_value": "600",
    "pred_unit": "Watts",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "600",
      "qwen32b-bench": "600",
      "qwen14b-bench": "600"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q248",
    "question": "How many pounds of CO2e are estimated for an average human life in one year (globally)?",
    "gt_value": "11023",
    "gt_unit": "lbs",
    "gt_ref": "['strubell2019']",
    "pred_value": "11023",
    "pred_unit": "lbs",
    "pred_ref": "[\"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "11023",
      "qwen32b-bench": "11,023",
      "qwen14b-bench": "11023"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"strubell2019\"]",
      "qwen32b-bench": "[\"strubell2019\"]",
      "qwen14b-bench": "[\"strubell2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q249",
    "question": "What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?",
    "gt_value": "1.25",
    "gt_unit": "multiplier",
    "gt_ref": "['samsi2024']",
    "pred_value": "1.25",
    "pred_unit": "multiplier",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1.25",
      "qwen32b-bench": "1.25",
      "qwen14b-bench": "1.25"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q250",
    "question": "What is the energy consumption (in Wh) of a single short query to GPT-4o?",
    "gt_value": "0.43",
    "gt_unit": "Wh",
    "gt_ref": "['jegham2025']",
    "pred_value": "0.42",
    "pred_unit": "Wh",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.42",
      "qwen32b-bench": "0.42",
      "qwen14b-bench": "0.42"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q251",
    "question": "In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",
    "gt_value": "280",
    "gt_unit": "percent",
    "gt_ref": "['kim2025']",
    "pred_value": "280",
    "pred_unit": "percent",
    "pred_ref": "[\"kim2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "280",
      "qwen32b-bench": "280",
      "qwen14b-bench": "280"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"kim2025\"]",
      "qwen32b-bench": "[\"kim2025\"]",
      "qwen14b-bench": "[\"kim2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q252",
    "question": "Which GPU architecture was most energy-efficient for models generating only a single classification token?",
    "gt_value": "V100",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "V100 or A30",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "V100 or A30",
      "qwen32b-bench": "A30 GPU",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q254",
    "question": "True or False: Green AI involves providing the financial cost of finding, training, and running models.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['schwartz2019']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"schwartz2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "0",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"schwartz2019\"]",
      "qwen32b-bench": "[\"schwartz2019\"]",
      "qwen14b-bench": "[\"schwartz2019\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q255",
    "question": "As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",
    "gt_value": "62",
    "gt_unit": "metric tons",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "62000000",
    "pred_unit": "metric tons",
    "pred_ref": "[\"luccioni2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "62000000",
      "qwen32b-bench": "62",
      "qwen14b-bench": "62000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025a\"]",
      "qwen32b-bench": "[\"luccioni2025a\"]",
      "qwen14b-bench": "[\"luccioni2025a\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q256",
    "question": "(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?",
    "gt_value": "104",
    "gt_unit": "Watts",
    "gt_ref": "['patterson2021']",
    "pred_value": "104",
    "pred_unit": "Watts",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "114",
      "qwen32b-bench": "104",
      "qwen14b-bench": "104"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q257",
    "question": "How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?",
    "gt_value": "700000",
    "gt_unit": "liters",
    "gt_ref": "['li2025b']",
    "pred_value": "700000",
    "pred_unit": "liters",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "700000",
      "qwen32b-bench": "700000",
      "qwen14b-bench": "700000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q258",
    "question": "How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?",
    "gt_value": "20",
    "gt_unit": "multiplier",
    "gt_ref": "['wu2021a']",
    "pred_value": "20",
    "pred_unit": "multiplier",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "20",
      "qwen32b-bench": "20",
      "qwen14b-bench": "20"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q259",
    "question": "Which model ranked highest in a recent eco-efficiency analysis using DEA?",
    "gt_value": "Claude-3.7 Sonnet",
    "gt_unit": "is_blank",
    "gt_ref": "['jegham2025']",
    "pred_value": "o3-mini",
    "pred_unit": "is_blank",
    "pred_ref": "[\"jegham2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "o3-mini",
      "qwen32b-bench": "o3-mini",
      "qwen14b-bench": "o3-mini"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"jegham2025\"]",
      "qwen32b-bench": "[\"jegham2025\"]",
      "qwen14b-bench": "[\"jegham2025\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q260",
    "question": "True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['wu2021b']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"wu2021b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021b\"]",
      "qwen32b-bench": "[\"wu2021b\"]",
      "qwen14b-bench": "[\"wu2021b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q261",
    "question": "True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q264",
    "question": "What is the context window size, in tokens, for the FLM-101B model?",
    "gt_value": "2048",
    "gt_unit": "tokens",
    "gt_ref": "['li2025a']",
    "pred_value": "2048",
    "pred_unit": "tokens",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2048",
      "qwen32b-bench": "2,048",
      "qwen14b-bench": "2048"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"li2025a\"]",
      "qwen14b-bench": "[\"li2025a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q265",
    "question": "True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['chung2025']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q266",
    "question": "In 2023, what percentage of Amazon's People Managers globally identified as women?",
    "gt_value": "31.6",
    "gt_unit": "percent",
    "gt_ref": "['amazon2023']",
    "pred_value": "is_blank",
    "pred_unit": "percent",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q267",
    "question": "When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",
    "gt_value": "[61,76]",
    "gt_unit": "percent",
    "gt_ref": "['cottier2024']",
    "pred_value": "61\u201376",
    "pred_unit": "percent",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "61-76",
      "qwen32b-bench": "61\u201376",
      "qwen14b-bench": "61\u201376"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q268",
    "question": "True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['khan2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"khan2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"khan2025\"]",
      "qwen32b-bench": "[\"khan2025\"]",
      "qwen14b-bench": "[\"khan2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q269",
    "question": "What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",
    "gt_value": "0.954",
    "gt_unit": "lbs/kWh",
    "gt_ref": "['strubell2019']",
    "pred_value": "0.932",
    "pred_unit": "lbs/kWh",
    "pred_ref": "[\"patterson2021\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.932",
      "qwen32b-bench": "0.932",
      "qwen14b-bench": "1438"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"patterson2021\"]",
      "qwen32b-bench": "[\"patterson2021\"]",
      "qwen14b-bench": "[\"patterson2021\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q270",
    "question": "According to one study, what is the projected range of electricity consumption by the global AI in 2027?",
    "gt_value": "[85,134]",
    "gt_unit": "TWh",
    "gt_ref": "['li2025b']",
    "pred_value": "85 \u2013 134",
    "pred_unit": "TWh",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "85 \u2013 134",
      "qwen32b-bench": "85 \u2013 134",
      "qwen14b-bench": "85 \u2013 134"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q271",
    "question": "How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",
    "gt_value": "150000000",
    "gt_unit": "packages",
    "gt_ref": "['amazon2023']",
    "pred_value": "150",
    "pred_unit": "packages",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "150",
      "qwen32b-bench": "150",
      "qwen14b-bench": "150"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q273",
    "question": "What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?",
    "gt_value": "1920000",
    "gt_unit": "tokens",
    "gt_ref": "['kim2025']",
    "pred_value": "1920000",
    "pred_unit": "tokens",
    "pred_ref": "[\"kim2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1920000",
      "qwen32b-bench": "1872000",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"kim2025\"]",
      "qwen32b-bench": "[\"kim2025\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q274",
    "question": "True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q275",
    "question": "According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",
    "gt_value": "80",
    "gt_unit": "percent",
    "gt_ref": "['dodge2022']",
    "pred_value": "80",
    "pred_unit": "percent",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "80",
      "qwen32b-bench": "80",
      "qwen14b-bench": "80"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q276",
    "question": "Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",
    "gt_value": "1450",
    "gt_unit": "times",
    "gt_ref": "['luccioni2024']",
    "pred_value": "1453.5",
    "pred_unit": "times",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1453.5",
      "qwen32b-bench": "1453.5",
      "qwen14b-bench": "60"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q277",
    "question": "True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['zschache2025']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"zschache2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"zschache2025\"]",
      "qwen32b-bench": "[\"zschache2025\"]",
      "qwen14b-bench": "[\"zschache2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q279",
    "question": "As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",
    "gt_value": "244",
    "gt_unit": "projects",
    "gt_ref": "['amazon2023']",
    "pred_value": "244",
    "pred_unit": "projects",
    "pred_ref": "[\"amazon2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "244",
      "qwen32b-bench": "244",
      "qwen14b-bench": "244"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"amazon2023\"]",
      "qwen32b-bench": "[\"amazon2023\"]",
      "qwen14b-bench": "[\"amazon2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q281",
    "question": "What percent of power usage did Amazon's AWS cover with renewable energy in 2018?",
    "gt_value": "50",
    "gt_unit": "percent",
    "gt_ref": "['schwartz2019']",
    "pred_value": "17",
    "pred_unit": "percent",
    "pred_ref": "[\"strubell2019\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "17",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"strubell2019\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q283",
    "question": "At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?",
    "gt_value": "Cumulative server level",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "Cumulative server level",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Cumulative server level",
      "qwen32b-bench": "Cumulative server level",
      "qwen14b-bench": "cumulative server level"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q284",
    "question": "In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",
    "gt_value": "74",
    "gt_unit": "percent",
    "gt_ref": "['dodge2022']",
    "pred_value": "74",
    "pred_unit": "percent",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "74",
      "qwen32b-bench": "74",
      "qwen14b-bench": "74"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q285",
    "question": "Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?",
    "gt_value": "2",
    "gt_unit": "NVIDIA A100-80GB GPUs",
    "gt_ref": "['griggs2024']",
    "pred_value": "2",
    "pred_unit": "NVIDIA A100-80GB GPUs",
    "pred_ref": "[\"griggs2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2",
      "qwen32b-bench": "2",
      "qwen14b-bench": "2"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"griggs2024\"]",
      "qwen32b-bench": "[\"griggs2024\"]",
      "qwen14b-bench": "[\"griggs2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q286",
    "question": "What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?",
    "gt_value": "28.5",
    "gt_unit": "percent",
    "gt_ref": "['wu2021a']",
    "pred_value": "28.5",
    "pred_unit": "percent",
    "pred_ref": "[\"wu2021a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "28.5",
      "qwen32b-bench": "28.5",
      "qwen14b-bench": "28.5"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"wu2021a\"]",
      "qwen32b-bench": "[\"wu2021a\"]",
      "qwen14b-bench": "[\"wu2021a\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q287",
    "question": "How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "kilometers of fiberoptic cable",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "kilometers of fiberoptic cable",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q288",
    "question": "What is the estimated upfront hardware acquisition cost to train GPT-4?",
    "gt_value": "800000000",
    "gt_unit": "USD",
    "gt_ref": "['cottier2024']",
    "pred_value": "800",
    "pred_unit": "USD",
    "pred_ref": "[\"cottier2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "800",
      "qwen32b-bench": "800",
      "qwen14b-bench": "800000000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"cottier2024\"]",
      "qwen32b-bench": "[\"cottier2024\"]",
      "qwen14b-bench": "[\"cottier2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q289",
    "question": "True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "luccioni2025b",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q290",
    "question": "What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU",
    "gt_value": "5",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "10",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "10",
      "qwen14b-bench": "28"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q291",
    "question": "When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",
    "gt_value": "Swapping",
    "gt_unit": "is_blank",
    "gt_ref": "['chung2025']",
    "pred_value": "Swapping",
    "pred_unit": "is_blank",
    "pred_ref": "[\"chung2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "Swapping",
      "qwen32b-bench": "Swapping",
      "qwen14b-bench": "Swapping"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"chung2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"chung2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q292",
    "question": "In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",
    "gt_value": "48",
    "gt_unit": "percent",
    "gt_ref": "['luccioni2025a']",
    "pred_value": "is_blank",
    "pred_unit": "percent",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q293",
    "question": "According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",
    "gt_value": "11.7",
    "gt_unit": "percent",
    "gt_ref": "['han2024']",
    "pred_value": "9.1 to 11.7",
    "pred_unit": "percent",
    "pred_ref": "[\"fernandez2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "9.1 to 11.7",
      "qwen32b-bench": "11.7",
      "qwen14b-bench": "10"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"fernandez2025\"]",
      "qwen32b-bench": "[\"chung2025\"]",
      "qwen14b-bench": "[\"fernandez2025\", \"chung2025\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  },
  {
    "id": "q294",
    "question": "When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",
    "gt_value": "25",
    "gt_unit": "percent",
    "gt_ref": "['dodge2022']",
    "pred_value": "25",
    "pred_unit": "percent",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "25",
      "qwen32b-bench": "25",
      "qwen14b-bench": "25"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q295",
    "question": "By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?",
    "gt_value": "70",
    "gt_unit": "percent",
    "gt_ref": "['shen2024']",
    "pred_value": "70",
    "pred_unit": "percent",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "70",
      "qwen32b-bench": "70",
      "qwen14b-bench": "70"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"shen2024\"]",
      "qwen32b-bench": "[\"shen2024\"]",
      "qwen14b-bench": "[\"shen2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q298",
    "question": "What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?",
    "gt_value": "626155",
    "gt_unit": "lbs CO2e",
    "gt_ref": "luccioni2025b",
    "pred_value": "626155",
    "pred_unit": "lbs CO2e",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "626155",
      "qwen32b-bench": "626155",
      "qwen14b-bench": "626155"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q299",
    "question": "What was the estimated training energy of the full GPT-3 model, in MWh?",
    "gt_value": "1287",
    "gt_unit": "MWh",
    "gt_ref": "['li2025b']",
    "pred_value": "1287",
    "pred_unit": "MWh",
    "pred_ref": "[\"jegham2025\", \"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1287",
      "qwen32b-bench": "1287",
      "qwen14b-bench": "1287"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\", \"jegham2025\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 0.5,
    "na_correct": true
  },
  {
    "id": "q300",
    "question": "True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['xia2024']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q301",
    "question": "What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?",
    "gt_value": "2",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "2",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "4"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q302",
    "question": "True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",
    "gt_value": "1",
    "gt_unit": "is_blank",
    "gt_ref": "['erben2023']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"erben2023\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "1"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"erben2023\"]",
      "qwen32b-bench": "[\"erben2023\"]",
      "qwen14b-bench": "[\"erben2023\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q303",
    "question": "How many hectares of land were occupied by new AI data centers globally in 2022?",
    "gt_value": "is_blank",
    "gt_unit": "hectares",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "hectares",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q305",
    "question": "A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",
    "gt_value": "0.32",
    "gt_unit": "g CO2eq",
    "gt_ref": "['luccioni2024']",
    "pred_value": "0.32",
    "pred_unit": "g CO2eq",
    "pred_ref": "[\"luccioni2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0.32",
      "qwen32b-bench": "0.32",
      "qwen14b-bench": "0.32"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2024\"]",
      "qwen32b-bench": "[\"luccioni2024\"]",
      "qwen14b-bench": "[\"luccioni2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q307",
    "question": "In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",
    "gt_value": "[7000,26000]",
    "gt_unit": "grams",
    "gt_ref": "['dodge2022']",
    "pred_value": "19000",
    "pred_unit": "grams",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "19000",
      "qwen32b-bench": "19000",
      "qwen14b-bench": "19000"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q308",
    "question": "In what year did the practice of directly releasing environmental information for notable models peak before declining?",
    "gt_value": "2022",
    "gt_unit": "year",
    "gt_ref": "['luccioni2025c']",
    "pred_value": "2022",
    "pred_unit": "year",
    "pred_ref": "[\"luccioni2025c\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "2022",
      "qwen32b-bench": "2022",
      "qwen14b-bench": "2022"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025c\"]",
      "qwen32b-bench": "[\"luccioni2025c\"]",
      "qwen14b-bench": "[\"luccioni2025c\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q309",
    "question": "What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",
    "gt_value": "5",
    "gt_unit": "days",
    "gt_ref": "['morrison2025']",
    "pred_value": "5",
    "pred_unit": "days",
    "pred_ref": "[\"morrison2025\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "5",
      "qwen32b-bench": "5",
      "qwen14b-bench": "5"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"morrison2025\"]",
      "qwen32b-bench": "[\"morrison2025\"]",
      "qwen14b-bench": "[\"morrison2025\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q310",
    "question": "How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?",
    "gt_value": "is_blank",
    "gt_unit": "liters of freshwater",
    "gt_ref": "is_blank",
    "pred_value": "is_blank",
    "pred_unit": "liters of freshwater",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q311",
    "question": "True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['xia2024']",
    "pred_value": "1",
    "pred_unit": "is_blank",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"xia2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q312",
    "question": "According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",
    "gt_value": "40000",
    "gt_unit": "kWh",
    "gt_ref": "['li2025a']",
    "pred_value": "52.76",
    "pred_unit": "kWh",
    "pred_ref": "[\"li2025a\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "52.76",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025a\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q313",
    "question": "According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",
    "gt_value": "20000000000",
    "gt_unit": "USD",
    "gt_ref": "['han2024']",
    "pred_value": "20900000000",
    "pred_unit": "USD",
    "pred_ref": "[\"han2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "20900000000",
      "qwen32b-bench": "20.9",
      "qwen14b-bench": "20"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"han2024\"]",
      "qwen32b-bench": "[\"han2024\"]",
      "qwen14b-bench": "[\"han2024\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q314",
    "question": "What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?",
    "gt_value": "32.7",
    "gt_unit": "USD",
    "gt_ref": "['xia2024']",
    "pred_value": "32.7",
    "pred_unit": "USD",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "32.7",
      "qwen32b-bench": "32.7",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"xia2024\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q315",
    "question": "For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",
    "gt_value": "32",
    "gt_unit": "samples",
    "gt_ref": "['xia2024']",
    "pred_value": "8",
    "pred_unit": "samples",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q317",
    "question": "What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",
    "gt_value": "4",
    "gt_unit": "seconds",
    "gt_ref": "['xia2024']",
    "pred_value": "1980.198",
    "pred_unit": "seconds",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1980.198",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"xia2024\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q318",
    "question": "True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "0",
      "qwen32b-bench": "0",
      "qwen14b-bench": "0"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"ebert2024\"]",
      "qwen32b-bench": "[\"ebert2024\"]",
      "qwen14b-bench": "[\"ebert2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q319",
    "question": "In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",
    "gt_value": "50",
    "gt_unit": "percent",
    "gt_ref": "luccioni2025b",
    "pred_value": "50",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "50",
      "qwen32b-bench": "50",
      "qwen14b-bench": "50"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"luccioni2025b\"]",
      "qwen32b-bench": "[\"luccioni2025b\"]",
      "qwen14b-bench": "[\"luccioni2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q320",
    "question": "What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?",
    "gt_value": "1",
    "gt_unit": "V100_32GB_GPU",
    "gt_ref": "['samsi2024']",
    "pred_value": "1",
    "pred_unit": "V100_32GB_GPU",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "1",
      "qwen32b-bench": "1",
      "qwen14b-bench": "8"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"samsi2024\"]",
      "qwen32b-bench": "[\"samsi2024\"]",
      "qwen14b-bench": "[\"samsi2024\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q321",
    "question": "When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",
    "gt_value": "16.7",
    "gt_unit": "requests",
    "gt_ref": "['li2025b']",
    "pred_value": "16.7",
    "pred_unit": "requests",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "16.7",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "10"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"li2025b\"]",
      "qwen32b-bench": "[\"li2025b\"]",
      "qwen14b-bench": "[\"li2025b\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q322",
    "question": "What is the estimated CO2 emission in metric tons for one year of average US home energy use?",
    "gt_value": "8.3",
    "gt_unit": "metric tons",
    "gt_ref": "['dodge2022']",
    "pred_value": "8.3",
    "pred_unit": "metric tons",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "8.3",
      "qwen32b-bench": "8.3",
      "qwen14b-bench": "8.3"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"dodge2022\"]",
      "qwen32b-bench": "[\"dodge2022\"]",
      "qwen14b-bench": "[\"dodge2022\"]"
    },
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true
  },
  {
    "id": "q323",
    "question": "On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",
    "gt_value": "27.8",
    "gt_unit": "score",
    "gt_ref": "['shen2024']",
    "pred_value": "is_blank",
    "pred_unit": "score",
    "pred_ref": "[\"is_blank\"]",
    "pred_explanation": "Ensemble (majority) of 3 runs: qwen72b-bench, qwen32b-bench, qwen14b-bench",
    "raw_response": "",
    "individual_answers": {
      "qwen72b-bench": "is_blank",
      "qwen32b-bench": "is_blank",
      "qwen14b-bench": "is_blank"
    },
    "individual_refs": {
      "qwen72b-bench": "[\"is_blank\"]",
      "qwen32b-bench": "[\"is_blank\"]",
      "qwen14b-bench": "[\"is_blank\"]"
    },
    "value_correct": false,
    "ref_score": 0.0,
    "na_correct": true
  }
]