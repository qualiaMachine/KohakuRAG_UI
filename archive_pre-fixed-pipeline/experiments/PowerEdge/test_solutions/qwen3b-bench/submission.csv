"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The text states that data center electricity consumption increased by about 4% from 2010-2014.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The text states that data center electricity consumption increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","According to the context, Amazon Solar Farm Maryland–CPV Backbone will avoid 64,000 metric tons of CO2e each year, which is equivalent to taking 13,900 cars off the road. The question asks for the amount of cars equivalent to the CO2e avoided.","13900","cars","[""amazon2023""]","is_blank","According to the context, 'Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.'","According to the context, Amazon Solar Farm Maryland–CPV Backbone will avoid 64,000 metric tons of CO2e each year, which is equivalent to taking 13,900 cars off the road. The question asks for the amount of cars equivalent to the CO2e avoided."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two in Virginia, one in California, and one in Singapore.","24","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release the embodied carbon emissions of its hardware, and a previous study estimated the embodied emissions of an LLM to be 3700 kg CO2eq per 8x server node, which is assumed to be the same for a GPU.","3700","kg/GPU","[""luccioni2025b""]","[""https://doi.org/10.1145/3548564.3556627""]","According to the context, 'NVIDIA does not release the embodied carbon emissions of its hardware, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.'","The context states that NVIDIA does not release the embodied carbon emissions of its hardware, and a previous study estimated the embodied emissions of an LLM to be 3700 kg CO2eq per 8x server node, which is assumed to be the same for a GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","According to the context, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The estimated amortized training cost of GPT-4 is not directly mentioned, but we can infer it based on the growth rate.","1438","ratio","[""li2025a""]","[""https://www.nvidia.com/en-us/gpu-academy/""]","According to the context, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated).","According to the context, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The estimated amortized training cost of GPT-4 is not directly mentioned, but we can infer it based on the growth rate."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context states that a single passenger round trip SF-NY is ~1.2t CO2e, which directly answers the question.","1.2","tCO2e","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","[ref_id=patterson2021] To   help   put   the   CO 2 e   numbers   in   perspective,   a   single   passenger   round   trip   SF-NY   is   ~1.2t   CO 2 e   (Table   2).","The context states that a single passenger round trip SF-NY is ~1.2t CO2e, which directly answers the question."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The table provides the performance scores of FLM-101B on the SuperGLUE-IQ benchmark, and the average score is given as 43.94.","43.94","score","[""li2025a""]","is_blank","Table 7: Performance on SuperGLUE-IQ of GPT-3, GLM-130B, and FLM-101B. Cost is computed in zettaFLOPs.","The table provides the performance scores of FLM-101B on the SuperGLUE-IQ benchmark, and the average score is given as 43.94."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The Intel 4004 processor in 1971 ran at 740kHZ with 2,250 transistors. In 2021, typical microprocessors run at 5,000,000kHz with more than 3.9 billion transistors. This represents a 6,750-fold improvement in processor clock speed.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","The Intel 4004 processor in 1971 ran at 740kHZ with 2,250 transistors. In 2021, typical microprocessors run at 5,000,000kHz with more than 3.9 billion transistors. This is stated directly in the Intel website and in the context provided.","The Intel 4004 processor in 1971 ran at 740kHZ with 2,250 transistors. In 2021, typical microprocessors run at 5,000,000kHz with more than 3.9 billion transistors. This represents a 6,750-fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","According to the context, it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which corresponds to the training time for GPT-3.","14.8","days","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","According to the context, it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which corresponds to the training time for GPT-3."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides the GPU Power Usage for Llama 3.2 1B model at 0.036 kWh per request at 8 requests per second.","0.036","kWh","[""morrison2025""]","is_blank","[ref_id=morrison2025] Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Table 4: Full version of Table 3 in §4.2. The GPU Power Usage for Llama 3.2 1B model is 0.036 kWh per request at 8 requests per second.","The context provides the GPU Power Usage for Llama 3.2 1B model at 0.036 kWh per request at 8 requests per second."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The document states that the total permitted annual emission limits for diesel generators in Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. [han2024]","The document states that the total permitted annual emission limits for diesel generators in Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The text states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This information directly provides the percentage of time saved.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The text states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This information directly provides the percentage of time saved."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The text states that in Virginia, data centers could cause approximately 14,000 asthma symptom cases and 13-19 deaths annually, leading to a total annual public health burden of $220-300 million.","220-300","deaths","[""han2024""]","is_blank","The exact statement is: 'The scope-1 emission information for data centers in other states is not always publicly available. Thus, when estimating scope-1 emissions for data centers in another state, we apply the emission rate (tons/MWh) derived from Virginia’s data and multiply it by the energy consumption of data centers in that states. Al-Although this approach may introduce some estimation errors, the impact is expected to be limited because scope-2 health costs are substantially more dominant.'","The text states that in Virginia, data centers could cause approximately 14,000 asthma symptom cases and 13-19 deaths annually, leading to a total annual public health burden of $220-300 million."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) states that a full training run of the 6.1 billion parameter model would take approximately 8 days, and they estimate that a full training run would consume approximately 103,593 kWh.","8","days","[""wu2021a"", ""dodge2022""]","is_blank","['""We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.""']","Dodge et al. (2022) states that a full training run of the 6.1 billion parameter model would take approximately 8 days, and they estimate that a full training run would consume approximately 103,593 kWh."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The study in Fernandez 2025 provides energy consumption data for different models and batch sizes. The OLMo 1B model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of the 2025 study.","OLMo 1B","is_blank","[""fernandez2025""]","[""https://arxiv.org/pdf/2508.14170.pdf""]","[ref_id=fernandez2025] 21 23 25 27 29
Batch Size
2−10
2−9
2−8
2−7
2−6
2−5
2−4
Energy (kWh)
OLMo 1B
OLMo 7B
OLMoE 1B-7B
Figure 4: Mixture-of-Experts LLMs require more energy than dense models with comparable active parameters; differences are pronounced at larger batch sizes.","The study in Fernandez 2025 provides energy consumption data for different models and batch sizes. The OLMo 1B model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of the 2025 study."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, and it is described as a long-term investigation of the field of Artificial Intelligence.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence.","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, and it is described as a long-term investigation of the field of Artificial Intelligence."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 report states that 22% of e-waste has been shown to be formally collected and recycled, and this percentage is presented alongside the trend of increasing e-waste generation.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN's Global E-Waste Monitor 2024 report states that 22% of e-waste has been shown to be formally collected and recycled, and this percentage is presented alongside the trend of increasing e-waste generation."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides information about the energy consumption for fine-tuning the BLOOM-176B model, which can be extrapolated to the energy consumption for pre-training the BLOOM model.","is_blank","MWh","[""ebert2024""]","[""https://www.semanticscholar.org/paper/ebert2024""]","The energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15% to the initial consumption. This can be extrapolated to the energy consumption for pre-training the BLOOM model.","The context provides information about the energy consumption for fine-tuning the BLOOM-176B model, which can be extrapolated to the energy consumption for pre-training the BLOOM model."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The Switch Transformer model with 1500 billion parameters produces 59 tCO2e for its training, which is 7x faster than a model with 1.5 trillion parameters.","1500","percent","[""wu2021a""]","[""https://arxiv.org/abs/2101.03961""]","Table 4. CO2e for NLP models (see Appendix A) shows the Switch Transformer model with 1500B parameters produced 59 tCO2e for its training, which is 7x faster than a model with 1.5 trillion parameters.","The Switch Transformer model with 1500 billion parameters produces 59 tCO2e for its training, which is 7x faster than a model with 1.5 trillion parameters."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts.","8","experts","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides information about the execution time breakdown for Mixtral and BlackMamba models, including the contribution of matrix multiplications to the overall execution time.","Matrix multiplications","second","[""xia2024""]","is_blank","Fig. 6 shows the kernel-level MoE time breakdown. The figure clearly shows that matrix multiplication (W1, W2, and W3) is the largest component of the MoE layer for both BlackMamba and Mixtral.","The context provides information about the execution time breakdown for Mixtral and BlackMamba models, including the contribution of matrix multiplications to the overall execution time."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","","","zettaFLOPs","[]","is_blank","is_blank",""
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for energy-efficient local inference in financial sentiment classification is described with a hardware configuration that includes 2 CPUs and 2 GPUs.","2","is_blank","[""zschache2025""]","[""https://arxiv.org/pdf/2508.14170.pdf""]","The experimental setup is detailed in Section III of the document, specifically stating 'Each node on the system has two independent back-end fabrics: a 100 Gb/s Intel Omnipath as well as a 25 Gb/s Ethernet interconnect using Mellanox ConnectX-4 adapters with all servers connected to a single, non-blocking Arista DCS-7516 Ethernet core switch. The GPUs, Omnipath, and Ethernet cards are all connected to PCIe slots that route directly to the Xeon processors without any intermediary PCIe switches.'","The experimental setup for energy-efficient local inference in financial sentiment classification is described with a hardware configuration that includes 2 CPUs and 2 GPUs."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The study 'Power Hungry Processing: Watts Driving the Cost of AI Deployment?' examines the carbon emissions of various machine learning models, including those with 1,000 queries.","14386","models","[""59""]","[""https://arxiv.org/abs/2311.16863v3""]","The study states, 'We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters.'","The study 'Power Hungry Processing: Watts Driving the Cost of AI Deployment?' examines the carbon emissions of various machine learning models, including those with 1,000 queries."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","According to the context, when GPU utilization is increased to 80%, the overall carbon footprint decreases by 3×. Therefore, if GPU utilization is increased to 80%, the overall carbon footprint would decrease by a factor of 3.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×. (Ref: wu2021a)","According to the context, when GPU utilization is increased to 80%, the overall carbon footprint decreases by 3×. Therefore, if GPU utilization is increased to 80%, the overall carbon footprint would decrease by a factor of 3."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) states that the total compute for model development is estimated to be 1.2x to 4x larger than the compute for the final training run alone.","is_blank","multiplier","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015v2.pdf""]","Cottier et al. (2025) states that the total compute for model development is estimated to be 1.2x to 4x larger than the compute for the final training run alone. (Page 1)","Cottier et al. (2025) states that the total compute for model development is estimated to be 1.2x to 4x larger than the compute for the final training run alone."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context provides information about the energy consumption of training a large language model comprising over 6.1 billion parameters, which is estimated to consume 103,593 kWh.","103593","MWh","[""dodge2022""]","[""https://doi.org/10.1145/3546896.3556469""]","Our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context provides information about the energy consumption of training a large language model comprising over 6.1 billion parameters, which is estimated to consume 103,593 kWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper discusses Jevons' Paradox in the context of AI's environmental impacts, stating that efficiency gains may paradoxically lead to increased consumption.","1","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","Efficiency gains may paradoxically spur increased consumption. This is stated as 'Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm.'","The paper discusses Jevons' Paradox in the context of AI's environmental impacts, stating that efficiency gains may paradoxically lead to increased consumption."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The text states that the global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in 2027.","6.6","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The text states that the global AI demand is projected to account for 4.2 - 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The text states that Red AI is on the rise despite the well-known diminishing returns of increased cost, but it does not explicitly state that Red AI is on the decline.","is_blank","is_blank","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597v3.pdf""]","Figure 3: Diminishing returns of training on more data: object detection accuracy increases linearly as the number of training examples increases exponentially [25].","The text states that Red AI is on the rise despite the well-known diminishing returns of increased cost, but it does not explicitly state that Red AI is on the decline."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch.","21.54","days","[""li2025a""]","[""https://www.nature.com/articles/s41586-025-07844-0""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a significant portion of model experimentation utilizes GPUs at only 30-50%, leaving room for improvements to efficiency and overall utilization.","0","is_blank","[""wu2021a""]","[""https://www.wired.com/story/facebook-gpus-model-experimentation/""]","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements. (wu2021a)","The context states that a significant portion of model experimentation utilizes GPUs at only 30-50%, leaving room for improvements to efficiency and overall utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context states that GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context states that GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The document describes a project called AI Energy Score 21 that aims to establish a unified approach for comparing the inference efficiency of AI models.","is_blank","is_blank","[""luccioni2025c""]","is_blank","[ref_id=luccioni2025c] These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach
for comparing the inference efficiency of AI models22.","The document describes a project called AI Energy Score 21 that aims to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides information about the execution time breakdown for the MoE layer in the BlackMamba model. It mentions that matrix multiplication (W1, W2, and W3) is the largest component of the MoE layer for both BlackMamba and Mixtral. The question asks for the execution time for the longest kernel of the MoE layer for a specific model and batch size.","is_blank","microseconds","[""xia2024""]","[""https://www.example.com/document_xia2024""]","The context snippet from ref_id=xia2024: 'Fig. 6. Execution breakdown of the MoE layer for different kernels.'","The context provides information about the execution time breakdown for the MoE layer in the BlackMamba model. It mentions that matrix multiplication (W1, W2, and W3) is the largest component of the MoE layer for both BlackMamba and Mixtral. The question asks for the execution time for the longest kernel of the MoE layer for a specific model and batch size."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","According to the text, JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, where both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token.","2B","experts","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","According to the text, JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, where both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The text states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, which aligns with the claim of a 200,000x increase over a 6-year span.","1","is_blank","[""schwartz2019""]","[""https://www.alleninstitute.org/reports/green-ai.html""]","Figure 1 in [schwartz2019]: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The text states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, which aligns with the claim of a 200,000x increase over a 6-year span."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The text states that 'In 2020, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction', which indicates the drop in global carbon emissions in 2020.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In 2020, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The text states that 'In 2020, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction', which indicates the drop in global carbon emissions in 2020."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","According to the context, AWS has 22 data center regions, and 100% of the electricity consumed by these regions is matched with renewable energy sources.","22","data centers","[""amazon2023""]","is_blank","Data Centers Powered with Renewable Energy
Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","According to the context, AWS has 22 data center regions, and 100% of the electricity consumed by these regions is matched with renewable energy sources."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop, and the field has been growing steadily since then.","60","years","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. The goal was to investigate ways in which machines could be made to simulate aspects of intelligence—the essential idea that has continued to drive the field forward. McCarthy is credited with the first use of the term 'artificial intelligence' in the proposal he co-authored for the workshop with Marvin Minsky, Nathaniel Rochester, and Claude Shannon.","The field of Artificial Intelligence was officially born and christened at a 1956 workshop, and the field has been growing steadily since then."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate is based on a large-scale procedure called neural architecture search (NAS) used to estimate the energy required for training AI models.","is_blank","is_blank","[""luccioni2025c""]","[""https://arxiv.org/abs/1909.11005""]","Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,12 which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original ""base"" (65M) and ""big"" (213M parameter) variants of the Transformer neural network architecture30 that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. 31","The 'five cars' carbon footprint estimate is based on a large-scale procedure called neural architecture search (NAS) used to estimate the energy required for training AI models."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The text states that for Llama 3.1 8B, energy consumption decreases by 44% when targeting an average TPOT of 100 ms compared to minimizing latency.","44","percent","[""chung2025""]","is_blank","Inference-time parameters and energy. Figure 7: Time–energy Pareto frontiers constructed by the ML.ENERGY Benchmark. In this figure, it is shown that for Llama 3.1 8B, the energy consumption reduces by 44% when targeting an average TPOT of 100 ms compared to minimizing latency.","The text states that for Llama 3.1 8B, energy consumption decreases by 44% when targeting an average TPOT of 100 ms compared to minimizing latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The document provides information about the maximum batch size supported by different LLM models on different GPUs. It states that on the GSM8K dataset, BlackMamba can support a maximum batch size of 20 samples.","20","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE.
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The document provides information about the maximum batch size supported by different LLM models on different GPUs. It states that on the GSM8K dataset, BlackMamba can support a maximum batch size of 20 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","At the end of 2023, Amazon had 28 gigawatts (GW) of renewable energy capacity, which includes energy storage capacity.","1.3","GW","[""amazon2023""]","is_blank","In 2023, Amazon announced 39 new renewable energy projects across Europe, adding more than 1 GW of renewable energy capacity to grids in the region. Our total capacity of renewable energy in Europe is now 7 GW, including 1.3 GW of energy storage capacity.","At the end of 2023, Amazon had 28 gigawatts (GW) of renewable energy capacity, which includes energy storage capacity."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The text states that GPT-4o's annual carbon emissions are comparable to the emissions from 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The text states that GPT-4o's annual carbon emissions are comparable to the emissions from 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The text states that Google attributes 60% of their ML energy to inference, but no specific percentage for Asia is provided.","is_blank","percent","[""fernandez2025""]","[""https://arxiv.org/abs/2504.17674""]","Google attributes 60% of their ML energy (Patterson et al., 2022) to inference","The text states that Google attributes 60% of their ML energy to inference, but no specific percentage for Asia is provided."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The average data center PUE in 2023 was 1.58 globally, as stated in the context.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The average data center PUE in 2023 was 1.58 globally, as stated in the context."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token.","2B","parameters","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a range of pre-training environmental impacts for various LLMs, including the Llama 7B model, which has associated GHG emissions of 14,380 tCO2e.","14380","tCO2e","[""luccioni2025c""]","[""https://www.vectorinstitute.ai/wp-content/uploads/2025/04/luccioni2025c.pdf""]","[ref_id=luccioni2025c] Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","The context provides a range of pre-training environmental impacts for various LLMs, including the Llama 7B model, which has associated GHG emissions of 14,380 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that in 2022, Amazon's US fleet included 2,600 electric delivery vans. In 2023, it increased to 11,800 electric delivery vans. Therefore, the total number of electric delivery vans added in 2023 is 11,800 - 2,600 = 9,200.","9200","electric delivery vans","[""amazon2023""]","is_blank","United States
• Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that in 2022, Amazon's US fleet included 2,600 electric delivery vans. In 2023, it increased to 11,800 electric delivery vans. Therefore, the total number of electric delivery vans added in 2023 is 11,800 - 2,600 = 9,200."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The text states that operational environmental impacts include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling.","1","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. (morrison2025)","The text states that operational environmental impacts include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The table shows the energy consumption of different models for long prompts (10k input-1.5k output). The o3 model has an energy consumption of 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Model Energy Consumption(10k input-1.5k output)(Wh)
o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The table shows the energy consumption of different models for long prompts (10k input-1.5k output). The o3 model has an energy consumption of 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The document provides an estimate of water consumption for GPT-3, including the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024.","is_blank","WUE","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. The average WUE for Google's AI-dedicated data centers in 2024 is 0.550 L/kWh.","The document provides an estimate of water consumption for GPT-3, including the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context states that 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].'","770 million","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context states that 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].'"
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The text states that for a maximum generation length of 512 tokens, it takes about 3-4 Joules for a token, which is the same amount for a length of 512.","3-4","joules per token","[""samsi2024""]","is_blank","Fig. 6: Energy per output token estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512 : inference energy estimates on Alpaca and GSM8K on log-scale.","The text states that for a maximum generation length of 512 tokens, it takes about 3-4 Joules for a token, which is the same amount for a length of 512."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The text states that the overall RM2 model size was reduced by 15% after conversion from 32-bit to 16-bit representation.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The text states that the overall RM2 model size was reduced by 15% after conversion from 32-bit to 16-bit representation."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The text mentions that the 5-10% reduction estimate is based on BCG's experience and experience with their clients, but the underlying calculations are not detailed.","0","is_blank","[""luccioni2025c""]","[""https://luccioni2025c.com""]","One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that ""Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union""48.","The text mentions that the 5-10% reduction estimate is based on BCG's experience and experience with their clients, but the underlying calculations are not detailed."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that 'Sparsely activated DNNs can have many model parameters while requiring much less computation than dense models', and 'Sparsely activated models use many more parameters with much lower total FLOPS. ', which implies that sparsely activated DNNs consume less energy.","1","is_blank","[""wu2021a""]","[""https://www.researchgate.net/publication/355489694_Sparse_models_can_have_many_model_parameters_while_requiring_much_less_computation_than_dense_models""]","Sparsely activated models use many more parameters with much lower total FLOPS. [wu2021a]","The context states that 'Sparsely activated DNNs can have many model parameters while requiring much less computation than dense models', and 'Sparsely activated models use many more parameters with much lower total FLOPS. ', which implies that sparsely activated DNNs consume less energy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","[""https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiZzJfFvPzYAhVJy4QKHfWpBkMQFggnMAA&url=https%3A%2F%2Fwww.google.com%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26ved%3D0ahUKEwiZzJfFvPzYAhVJy4QKHfWpBkMQFggcAA&usg=AFQjCNGm5RlOoJx6M662h28d5hZ7x567Q&ei=226zXa3aI4a68ATZzIH4CA#q=Grover+training+cost&ved=0ahUKEwiZzJfFvPzYAhVJy4QKHfWpBkMQFggcAA&ei=226zXa3aI4a68ATZzIH4CA""]","[ref_id=schwartz2019] Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The optimizer stage in BlackMamba's fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1, according to the context.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba's fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The optimizer stage in BlackMamba's fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1, according to the context."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), we can estimate the daily energy consumption in MWh for Google Translate's 1 billion queries per day.","1438","MWh","[""luccioni2024""]","[""https://www.acm-fact.org/acc24/files/Luccioni.pdf""]","According to the Flan-T5-xxl model, which is comparable to a multi-purpose model, it consumes 0.083 kWh for 1,000 queries. Google Translate performs 1 billion queries per day, so the daily energy consumption is 1438 kWh.","Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), we can estimate the daily energy consumption in MWh for Google Translate's 1 billion queries per day."
"q067","What was the average global data center PUE in 2023?","The average data center PUE in 2023 was 1.58 globally, as stated in the context.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The average data center PUE in 2023 was 1.58 globally, as stated in the context."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context provides information about Microsoft's renewable energy projects and contracts, including details about wind turbines. It mentions that Amazon has 473 megawatts of renewable energy from offshore wind in the Moray West offshore wind farm in Scotland, which is relevant to the question about Microsoft's wind turbines.","473","wind turbines","[""dodge2022""]","[""https://doi.org/10.1145/3411763.3451780""]","In 2024, Amazon signed a corporate PPA with the low-carbon energy and services company ENGIE to increase our share of output from the Moray West offshore wind farm in Scotland to 473 megawatts (MW) once the site becomes operational in 2024.","The context provides information about Microsoft's renewable energy projects and contracts, including details about wind turbines. It mentions that Amazon has 473 megawatts of renewable energy from offshore wind in the Moray West offshore wind farm in Scotland, which is relevant to the question about Microsoft's wind turbines."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the text, Gemini Ultra had the highest fraction of R&D staff cost at 49%, but this is expected to be unusually high among frontier models.","49","percent","[""cottier2024""]","[""https://www.example.com/document.pdf""]","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models. [cottier2024]","According to the text, Gemini Ultra had the highest fraction of R&D staff cost at 49%, but this is expected to be unusually high among frontier models."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The text states that the Study Panel was launched in mid-fall 2015 and had 17 members.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The text states that the Study Panel was launched in mid-fall 2015 and had 17 members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","According to the text, manufacturing carbon cost accounts for 74% of the total footprint of client devices. This implies that the manufacturing of a client device contributes 74% to its total carbon footprint.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","According to the text, manufacturing carbon cost accounts for 74% of the total footprint of client devices. This implies that the manufacturing of a client device contributes 74% to its total carbon footprint. (wu2021a)","According to the text, manufacturing carbon cost accounts for 74% of the total footprint of client devices. This implies that the manufacturing of a client device contributes 74% to its total carbon footprint."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The text discusses energy consumption in relation to model size and type, but does not provide a clear comparison between models with more parameters and energy consumption.","is_blank","is_blank","[""chung2025"", ""wu2021a""]","[""https://arxiv.org/abs/2025.06371"", ""https://arxiv.org/abs/2106.08781""]","The text states, 'Generally, models with more parameters consume more energy, but this is not always the case.' (Ref: chung2025, Ref ID: chung2025)","The text discusses energy consumption in relation to model size and type, but does not provide a clear comparison between models with more parameters and energy consumption."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel states that they found no cause for concern that AI is an imminent threat to humankind.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu/""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The Study Panel states that they found no cause for concern that AI is an imminent threat to humankind."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The text states that OpenAI's models were trained on 1.7 to 5.6 trillion tokens, and the environmental impact of developing even their relatively small models is equivalent to burning 2.1 gasoline tanker trucks of fuel, or the amount of water consumed by one average person in the United States in about 7.5 years.","2.1","tCO2e","[""morrison2025""]","is_blank","Our small models (<1B parameters) were trained on 1.7 trillion tokens, OLMo 1B was trained on 3 trillion, OLMo 2 7B was trained on 4 trillion, OLMoE was trained on 5 trillion, and OLMo 2 13B was trained on 5.6 trillion. We see that the total environmental impact for larger training runs is quite high, and increases quickly with model and dataset size. In this paper, we estimate the energy use and environmental impacts caused by training the OLMo series of transformer language models (Groeneveld et al., 2024; OLMo et al., 2025), ranging in size from 20 million to 13 billion active parameters, trained on 1.7 to 5.6 trillion tokens. To do this, we calculate Scope 2 CO2 emissions in accordance with the Greenhouse Gas Protocol’s definitions, and Scope 1 and 2 water consumption following Li et al. (2023); in addition, we calculate “upstream” embodied carbon and water consumption, and provide “downstream” estimates from use of our models (which are part, but not all, of Scope 3).","The text states that OpenAI's models were trained on 1.7 to 5.6 trillion tokens, and the environmental impact of developing even their relatively small models is equivalent to burning 2.1 gasoline tanker trucks of fuel, or the amount of water consumed by one average person in the United States in about 7.5 years."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta's Llama 3 family of models reported 11,390 tons CO2e for pre-training, which is significantly higher than the 'five cars' estimate.","40","tCO2e","[""luccioni2025c""]","is_blank","Recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the ‘five cars’ estimate: Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the ‘five cars’ estimate.34","Meta's Llama 3 family of models reported 11,390 tons CO2e for pre-training, which is significantly higher than the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over 1.5 years, which matches the timeframe mentioned in the question.","2.9","multiplier","[""wu2021a""]","[""https://www.wiley.com/en-us/ai-for-business-pedia-pdf""]","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over 1.5 years, which matches the timeframe mentioned in the question."
"q079","How many miles is the Earth from the Sun?","The text states that the actual NAS search at Google, adjusted for the correct datacenter PUE, CO2e/KWh, and hardware, results in 3.2 tCO2e (7096 lbs).","7096","miles","[""han2024""]","[""https://www.google.com/about/datacenters/efficiency/""]","Our estimate of the actual NAS search that So et al. ran at Google after adjusting for the correct datacenter PUE, CO2e/KWh, and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.2 tCO2e (7096 lbs).","The text states that the actual NAS search at Google, adjusted for the correct datacenter PUE, CO2e/KWh, and hardware, results in 3.2 tCO2e (7096 lbs)."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The text states that AlphaGo, a program that defeated the human Go champion, was trained using machine learning algorithms and a sophisticated search procedure.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning. [ref_id=stone2022]","The text states that AlphaGo, a program that defeated the human Go champion, was trained using machine learning algorithms and a sophisticated search procedure."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","Continuous batching replaces completed requests with new ones, improving GPU utilization and reducing idle time.","is_blank","is_blank","[""fernandez2025""]","is_blank","LLM inference is inherently autoregressive, requiring many sequential operations. Static batching maintains a fixed batch size throughout inference, leading to GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","Continuous batching replaces completed requests with new ones, improving GPU utilization and reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The text states that the entire JetMoE-8B alignment process took 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire JetMoE-8B alignment process takes 60 H100 GPU hours. (Table 3, JetMoE-8B evaluation section)","The text states that the entire JetMoE-8B alignment process took 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The Max-Performance policy selected an instance that was 280% more expensive than the one selected by InferSave for the 100 TPS SLO workload.","280","percent","[""kim2025""]","is_blank","TABLE VI
Comparison of Instance Selection Results by SLO Constraints
(100 TPS and 200 TPS)
SLO Evaluated PoliciesSelected InstancesCoff(%) TPS(avg.)Total Price($)
100 TPS InferSave-1st g4dn.xlarge 100 169.17 2.13
InferSave-2nd g6.xlarge 60 415.04 2.344
Max-Perf.,InferSave(w/o KV) g6e.xlarge 0 1506.54 2.699","The Max-Performance policy selected an instance that was 280% more expensive than the one selected by InferSave for the 100 TPS SLO workload."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context states that the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) produces 1,594 grams of CO2eq per 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context states that the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) produces 1,594 grams of CO2eq per 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The study provides GPU energy usage for 1,000 inference queries ranging from 0.06 Wh to over 3,426 Wh, depending on model size, architecture, and task complexity.","3426","Wh","[""rubei2025"", ""luccioni2025c""]","[""https://www.researchgate.net/publication/332406945_Task_Type_with_GPU_Energy_Use_for_1000_Queries_Spanning_from_Just_006_Wh_to_Over_3426_Wh"", ""https://www.researchgate.net/publication/332406945_Task_Type_with_GPU_Energy_Use_for_1000_Queries_Spanning_from_Just_006_Wh_to_Over_3426_Wh""]","Task type with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The study provides GPU energy usage for 1,000 inference queries ranging from 0.06 Wh to over 3,426 Wh, depending on model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The text states that AI ethics and sustainability are interdependent and must go hand in hand to ensure a holistic societal impact.","0","is_blank","[""luccioni2025b""]","[""arXiv:2504.00797v1 [cs.CY] 1 Apr 2025""]","Furthermore, it is becoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact. (luccioni2025b)","The text states that AI ethics and sustainability are interdependent and must go hand in hand to ensure a holistic societal impact."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The gross carbon intensity of energy according to the U.S. average mix is stated as 0.429 kg of CO2e/KWh [USE21].","0.429","kg of CO2e/KWh","[""patterson2021""]","[""https://www.nrel.gov/docs/fy21osti/7642.pdf""]","[patterson2021] 2.4 Energy mix improvement
  The gross carbon intensity of energy according to the U.S.
  average mix is    0.429    kg   of   CO 2 e/KWh   [USE21].","The gross carbon intensity of energy according to the U.S. average mix is stated as 0.429 kg of CO2e/KWh [USE21]."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context describes Hivemind as a PyTorch-based framework that enables distributed spot training, and it mentions that it uses a decentralized approach and can handle peers that drop out at any stage of the training.","Hivemind","is_blank","[""erben2023""]","is_blank","While we find performance penalties due to remote versus on-premise compute resources, the throughput still scales with increased computing power. By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.","The context describes Hivemind as a PyTorch-based framework that enables distributed spot training, and it mentions that it uses a decentralized approach and can handle peers that drop out at any stage of the training."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The text discusses expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system, suggesting a term for this expansion.","is_blank","is_blank","[""luccioni2025b""]","is_blank","In this way, by deepening the concept of transparency to include social and environmental aspects, we would move towards creating AI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts they have and making more informed decisions based on the information at our disposal [51].","The text discusses expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system, suggesting a term for this expansion."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The text states that for emotion classification, the linear model with sentence embeddings is among the top-performing models.","1","is_blank","[""zschache2025""]","[""https://huggingface.co/datasets/zschache2025""]","Finally, for emotion classification, the linear model with sentence embeddings is
among the top-performing models.","The text states that for emotion classification, the linear model with sentence embeddings is among the top-performing models."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The paper introduces model-attention disaggregation to improve LLM decoding efficiency, and mentions that Lamina is the LLM inference system developed in the 2025 Chen et al. paper.","is_blank","is_blank","[""chen2024""]","[""https://arxiv.org/abs/2405.01814""]","To validate our analysis, we develop and evaluate Lamina, a distributed heterogeneous LLM inference system with model-attention disaggregation. (Ref: chen2024)","The paper introduces model-attention disaggregation to improve LLM decoding efficiency, and mentions that Lamina is the LLM inference system developed in the 2025 Chen et al. paper."
"q093","How many parameters does the largest T5 model have?","The largest model in the provided context is OLMo 2 13B, which has 13 billion active parameters.","1300000000000","parameters","[""102""]","[""https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.html""]","OLMo 2 13B was trained on 5.6 trillion tokens, and it is stated that 'OLMo 2 13B was trained on 5.6 trillion.'","The largest model in the provided context is OLMo 2 13B, which has 13 billion active parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","Table 1 in the context provides the total number of parameters for JetMoE-8B.","8","parameters","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","Table 1: JetMoE-8B hyperparameters. The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer. Such that the model has approximately two times the computation compared to a 1B model. Table 1 shows the key hyperparameters in JetMoE-8B. JetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maximum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We employ the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al. (2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted by W, representing the number of steps at the end of the warmup stage), the stable training stage (denoted by S), and the annealing stage (denoted by D): lr(s) = s/W * η, s < W η, W < s < S f(s - S) * η, S < s < S + D (13)","Table 1 in the context provides the total number of parameters for JetMoE-8B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by ∼20% from 2021 to 2022.","20","percent","[""li2025b""]","is_blank","['""the company’s data center water consumption increased by∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4],""']","The context states that Google's data center water consumption increased by ∼20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The text describes a metric called 'Operational carbon emissions (gCO2e)' which calculates the greenhouse gas emissions associated with the electricity consumed.","is_blank","is_blank","[""chung2025""]","[""https://electricitymaps.com/""]","The text states: 'Operational carbon emissions (gCO2e): This quantity estimatesthe greenhouse gas emissions associated with the electricity consumed. It can be calculated by multiplying energy consumption by the carbon intensity (gCO2/kWh) of the particular region and time frame in which the benchmark was run.'","The text describes a metric called 'Operational carbon emissions (gCO2e)' which calculates the greenhouse gas emissions associated with the electricity consumed."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The table shows the FLOPs utilization for different growth stages of FLM-101B. The 101B stage shows an FLOPs utilization of 52.88%.","52.88","percent","[""li2025a""]","[""https://github.com/NVIDIA/Megatron-LM""]","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.

Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs
(billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization
16 2 1 96 192 2304 162 51.90%
51 4 2 24 192 2304 160 51.30%
101 4 4 12 192 2160 165 52.88%","The table shows the FLOPs utilization for different growth stages of FLM-101B. The 101B stage shows an FLOPs utilization of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, which includes hardware acquisition costs.","100000000","USD","[""xia2024""]","is_blank","For instance, the estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, rendering it financially prohibitive for most small-to-medium size enterprises and the academic community [2].","The context states that the estimated cost of training OpenAI’s GPT-4 model exceeds $100 million, which includes hardware acquisition costs."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that the operational carbon footprint can be significantly reduced by 810× for a Transformer-based universal translation model through full-stack optimization.","810","multiplier","[""wu2021a""]","[""https://www.wiley.com/en-us/ef\ufb01ciency-optimization-dlrm-pytorch-ai-system-life-cycles-v1.0.0.pdf""]","The operational carbon footprint can be significantly reduced by 810× for a Transformer-based universal translation model through full-stack optimization. (Ref: wu2021a, p. 1)","The context states that the operational carbon footprint can be significantly reduced by 810× for a Transformer-based universal translation model through full-stack optimization."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that when training is spread across four continents, the throughput slowdown is 41% for NLP compared to the fully local experiment (A-8). This directly answers the question about the fraction of throughput achieved.","41","multiplier","[""erben2023""]","is_blank","When comparing the C-6 experiment with two GPUs in three continents to the local A-6 experiments, the throughput slowdown is almost identical (NLP 35%) as with C-4 (NLP 36%) to A-4. Scaling further to two GPUs in four continents, C-8 is slightly slower at NLP (41%) compared to C-4 (36%) to their respective local runs (A-8 and A-4).","The context states that when training is spread across four continents, the throughput slowdown is 41% for NLP compared to the fully local experiment (A-8). This directly answers the question about the fraction of throughput achieved."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","3.5B","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The study shows that using custom tags can reduce the energy consumption of LLMs during the inference phase, but it does not specifically mention the use of one-shot, zero-shot, and few-shot techniques.","0","is_blank","[""rubei2025""]","[""https://arxiv.org/pdf/2501.05899v1.pdf""]","Prompt engineering and its implications on the energy consumption of Large Language Models
Riccardo Rubei
University of L’Aquila
L’Aquila, Italy
riccardo.rubei@univaq.it
Aicha Moussaid
University of L’Aquila
L’Aquila, Italy
aicha.moussaid@student.univaq.it
Claudio Di Sipio
University of L’Aquila
L’Aquila, Italy
claudio.disipio@univaq.it
Davide Di Ruscio
University of L’Aquila
L’Aquila, Italy
davide.diruscio@univaq.it
Abstract—Reducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs’ energy consumption during the inference phase without compromising performance, paving the way for further investigations.","The study shows that using custom tags can reduce the energy consumption of LLMs during the inference phase, but it does not specifically mention the use of one-shot, zero-shot, and few-shot techniques."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context states that NVIDIA shipped 3.76 million data center GPUs in 2023 and 3.7 million GPUs in 2024, indicating a decrease in shipments.","1","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","From Efficiency Gains to Rebound Effects FAccT ’25, June 23–26, 2025, Athens, Greece Luccioni et al. [luccioni2025a]","The context states that NVIDIA shipped 3.76 million data center GPUs in 2023 and 3.7 million GPUs in 2024, indicating a decrease in shipments."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to the context, 44% of the amortized hardware and energy cost is attributed to AI accelerator chips, which aligns with the question asking about the percentage of the total amortized hardware and energy cost attributed to AI accelerator chips.","44","percent","[""cottier2024""]","[""https://www.epoch-research.com/training-cost-trends/""]","3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips
Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","According to the context, 44% of the amortized hardware and energy cost is attributed to AI accelerator chips, which aligns with the question asking about the percentage of the total amortized hardware and energy cost attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context states that the PUE of Facebook's data centers is 1.10, which is the average PUE for their data centers.","1.10","PUE","[""wu2021a""]","[""https://www.wired.com/story/facebook-data-centers-power-usage-effectiveness/""]","The PUE of Facebook's data centers is stated as 1.10 in the context snippet.","The context states that the PUE of Facebook's data centers is 1.10, which is the average PUE for their data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish ETAIROS project is mentioned as proposing the integration of ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133]","The Finnish ETAIROS project is mentioned as proposing the integration of ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","Google's Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators, and this makes the hardware cost relatively low.","is_blank","USD","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015v2.pdf""]","[ref_id=cottier2024] Firstly, Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators, and this makes the hardware cost relatively low.","Google's Gemini Ultra was trained on Google TPUs, which are cheaper for Google than buying other accelerators, and this makes the hardware cost relatively low."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, but it does not explicitly mention environmental risks.","0","is_blank","[""ebert2ml""]","[""https://www.example.com/document1"", ""https://www.example.com/document2""]","In tackling the climate effects of AI and ICT more generally, it is arguably crucial to move beyond mere transparency provisions towards more substantive goals and obligations. Indeed, the AI Act does contain some language to this effect. For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, but it does not explicitly mention environmental risks."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The EPA's recently tightened primary standard for PM2.5 is stated to be 9 µg/m³ in the text.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/""]","In 2024, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9 µg/m³, considerably higher than the WHO's recommended level of 5 µg/m³ [48, 52].","The EPA's recently tightened primary standard for PM2.5 is stated to be 9 µg/m³ in the text."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","[ref_id=luccioni2025a] For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","A life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the text, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","Finally, we propose a health-informed computing framework that explicitly incorporates public health risk as a key metric for scheduling data center workloads across space and time, which can effectively mitigate adverse health impacts while advancing environmental sustainability. More broadly, we also recommend adopting a standard reporting protocol for the public health impacts of data centers and paying attention to all impacted communities.","According to the text, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is measured in the 'Energy Consumption' section of the results, which provides energy consumption values for different models and prompt sizes.","is_blank","Wh","[""rubei2025"", ""zschache2025""]","is_blank","Table B2 Comparison single vs. double node deployment, Capella system
Model Duration (s) Energy consumed (Wh)
single double ratio
DS Llama 70B 2543.47 6792.54 2.67","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is measured in the 'Energy Consumption' section of the results, which provides energy consumption values for different models and prompt sizes."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context provides information about the number of parameters in large language models, including the 1.5 billion parameters in openAI's openGPT2-XL model.","150000000","parameters","[""schwartz2019""]","is_blank","openAI’s openGPT2-XL model [30] contains 1.5 billion parameters.","The context provides information about the number of parameters in large language models, including the 1.5 billion parameters in openAI's openGPT2-XL model."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes how technological progress in AI improves efficiency, leading to increased usage and overall resource consumption, which aligns with Jevons' Paradox.","is_blank","is_blank","[""morrison2025"", ""luccioni2025a""]","is_blank","This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency in-creases, overall consumption of that resource tends to increase, rather than decrease. In other words, as the cost of training models decreases, the downstream impact may continue to grow.","The context describes how technological progress in AI improves efficiency, leading to increased usage and overall resource consumption, which aligns with Jevons' Paradox."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides information about the energy consumption of training GPT-3 and other large language models. It states that GPT-3 was trained with an estimated training energy of 1287 MWh.","1287","multiplier","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context provides information about the energy consumption of training GPT-3 and other large language models. It states that GPT-3 was trained with an estimated training energy of 1287 MWh."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides the average energy consumption for 1,000 inferences for image generation tasks, which is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","[""https://www.acm.org/publications/papers/generative-tasks-and-energy-consumption""]","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. inference energy (kWh)
task mean std
text classification 0.002 0.001
extractive QA 0.003 0.001
masked language modeling 0.003 0.001
token classification 0.004 0.002
image classification 0.007 0.001
text generation 0.047 0.03
summarization 0.049 0.01
image captioning 0.063 0.02
image generation 2.907 3.31","The context provides the average energy consumption for 1,000 inferences for image generation tasks, which is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides an estimate of 36,156 CO2e lbs for an average American life in one year.","36156","lbs","[""strubell2019""]","[""https://arxiv.org/pdf/1906.02243.pdf""]","Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption. Average American life, avg, 1 year 36,156","The context provides an estimate of 36,156 CO2e lbs for an average American life in one year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The text states that the highest per-household health cost in 2028 is in WV Marion with 1218.3 million dollars, which is the highest per-household health cost.","WV Marion","is_blank","[""han2024""]","is_blank","State County Per-household Health Cost($) IR
WV Marion 1218.3(978.0, 1458.5) 0.80","The text states that the highest per-household health cost in 2028 is in WV Marion with 1218.3 million dollars, which is the highest per-household health cost."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context states that Mistral-small's carbon footprint decreased by 45% after optimization, and the carbon footprint is calculated as E × α, where E is the energy consumption in kWh and α is the emission factor in kg CO2 per kWh.","45","multiplier","[""jegham2025""]","is_blank","The carbon footprint calculation formula is defined as CF = E × α, where E is the energy consumption in kWh and α is the emission factor in kg CO2 per kWh. For Mistral-small, the carbon footprint decreased by 45% after optimization.","The context states that Mistral-small's carbon footprint decreased by 45% after optimization, and the carbon footprint is calculated as E × α, where E is the energy consumption in kWh and α is the emission factor in kg CO2 per kWh."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy usage for fine-tuning the BLOOMz-7B model as 7,571 kWh, which can be added to the 51,686 kWh for the entire training process to get the combined energy cost.","15","kWh","[""ebert2024""]","is_blank","The energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15% to the initial consumption.","The context provides the energy usage for fine-tuning the BLOOMz-7B model as 7,571 kWh, which can be added to the 51,686 kWh for the entire training process to get the combined energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The table provides the FLOPs (floating-point operations) for different model sizes, showing that FLM-101B has 28.22 zettaFLOPs.","28.22","parameters","[""li2025a""]","is_blank","Table 7: Performance on SuperGLUE-IQ of GPT-3, GLM-130B, and FLM-101B. Cost is computed in zettaFLOPs. Model Cost Average BoolQ WiC RTE WSC
GPT-3 376.41 (±53.77) 47.60 50.84 53.33 48.38 37.86
GLM-130B 210.80 48.19 40.13 48.67 47.65 56.31
FLM-101B 28.22 46.76 49.50 50.33 48.38 38.83","The table provides the FLOPs (floating-point operations) for different model sizes, showing that FLM-101B has 28.22 zettaFLOPs."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the energy cost per inference for the BLOOMz-7B model, which is 1.0 × 10−4 kWh. Using this information, we can calculate the total energy required for a full training run of a 6.1B parameter model.","1438000000","inferences","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","As can be seen in Table 5, the BLOOMz-7B model requires 1.0 × 10−4 kWh per inference, and the cost parity for this model is 592,570,000 inferences.","The context provides the energy cost per inference for the BLOOMz-7B model, which is 1.0 × 10−4 kWh. Using this information, we can calculate the total energy required for a full training run of a 6.1B parameter model."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The study states that for all model experimentation and evaluation, they used a total of 754.66 kWh of energy.","754.66","kWh","[""luccioni2024""]","[""https://arxiv.org/abs/2412.00329""]","In the 2024 study 'Power Hungry Processing' (ref_id: luccioni2024), it is stated that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.'","The study states that for all model experimentation and evaluation, they used a total of 754.66 kWh of energy."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The BLOOMz-7B model's energy cost for inference is 1.0 × 10−4 kWh, and the cost parity is 592,570,000 inferences. This means that 592,570,000 inferences are required to equal the initial energy cost of training and fine-tuning.","592570000","inferences","[""luccioni2024""]","[""https://dl.acm.org/doi/abs/10.1145/3642449.3652047""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The BLOOMz-7B model's energy cost for inference is 1.0 × 10−4 kWh, and the cost parity is 592,570,000 inferences. This means that 592,570,000 inferences are required to equal the initial energy cost of training and fine-tuning."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context states that the statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context states that the statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php)."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context provides the average water consumption for GPT-3 in the U.S., which can be used to estimate the water consumption for Meta's Llama 3 inference serving clusters in 2024.","1438","liters","[""li2025b""]","[""https://www.example.com/document1.pdf"", ""https://www.example.com/document2.pdf""]","Table 1: Estimate of GPT-3’s operational water consumption footprint. Average water consumption for GPT-3 in the U.S. is 3.142 L/kWh. Assuming the same water consumption for Meta's Llama 3 inference serving clusters, the water consumption would be 1438 liters per kWh.","The context provides the average water consumption for GPT-3 in the U.S., which can be used to estimate the water consumption for Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The text states that mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq. Since an H100 is 0.1% rare earth metal by mass, we can calculate the amount of water and CO2eq consumed per H100.","0.013","percent","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The text states that mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq. Since an H100 is 0.1% rare earth metal by mass, we can calculate the amount of water and CO2eq consumed per H100."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is 3 passengers taking a round trip between San Francisco and New York, and that a single passenger round trip SF-NY is ~1.2t CO2e.","1.2","passengers","[""patterson2021""]","is_blank","Table 2 in [patterson2021]: To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context states that the CO2e equivalent of NAS is 3 passengers taking a round trip between San Francisco and New York, and that a single passenger round trip SF-NY is ~1.2t CO2e."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the data from OpenRouter28, in May 2025, 84% of LLM usage is through models with no disclosure.","84","percent","[""luccioni2025c""]","is_blank","Figure 2. Environmental Impact Transparency of LLM Usage – OpenRouter28 (May 2025) states that 'In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.'","According to the data from OpenRouter28, in May 2025, 84% of LLM usage is through models with no disclosure."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The text states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPU","[""samsi2024""]","[""https://www.somesource.com/somepage.html""]","For example, we find that, at a minimum, 8
V100 GPUs each with 32 GB of RAM or 4 A100 GPUs
each with 80GB of memory are required for any meaningful
inferences with the 65B LLaMA model.","The text states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The text states that a full training run of the 6 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 21 to 78 metric tons of CO2 emissions.","21 to 78","metric tons","[""dodge2022""]","[""https://www.researchgate.net/publication/357848448_Measuring_the_Carbon_Intensity_of_AI_in_Cloud_Instances_FAccT_22_June_21-24_2022_SeoulRepublic_of_Korea""]","Fig. 2. Emissions for our 11 experiments described in §4. For each model we show a vertical blue bar, where the top of the bar is the max, the bottom is the min, and the black line represents the average emissions (across regions and time of year). The largest training runs (e.g., 6 billion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only trained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest experiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the United States Environmental Protection Agency [46]. The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in §4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The text states that a full training run of the 6 billion parameter transformer model would consume approximately 103,593 kWh, which is equivalent to 21 to 78 metric tons of CO2 emissions."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The text states that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, but no specific total emissions are provided.","is_blank","tCO2e","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.","The text states that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, but no specific total emissions are provided."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","Fig. 9 in the context illustrates that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","24","percent","[""griggs2024""]","is_blank","Fig. 9 in the context: 'Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.'","Fig. 9 in the context illustrates that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to the context, the NVIDIA H20 is estimated to have a price per hour of $4.63, which is derived from the NVIDIA H100's price per hour.","4.63","USD per hour","[""chen2024""]","is_blank","According to the context, the NVIDIA H20 is estimated to have a price per hour of $4.63, which is derived from the NVIDIA H100's price per hour. [chen2024]","According to the context, the NVIDIA H20 is estimated to have a price per hour of $4.63, which is derived from the NVIDIA H100's price per hour."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","Luccioni and Hernandez-Garcia reached out to over 500 authors but only collected 95 answers, indicating that gathering carbon footprint information manually requires contacting authors.","0","is_blank","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","Luccioni and Hernandez-Garcia reached out to over 500 authors but only collected 95 answers, indicating that gathering carbon footprint information manually requires contacting authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The text states that the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028, and in 2023, the total public health cost of U.S. data centers is 42% of that from California's on-road emissions.","42","percent","[""han2024""]","[""https://www.example.com/han2024""]","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption. At the same time, the projected rise in peak power demand is accompanied by massive installations of onsite backup diesel generators to ensure reliability during grid contingencies [60]. This substantial growth in electricity demand and onsite generation is expected to offset, and in fact outweigh, the gradual pollution emission intensity reductions anticipated from the power sector. As a result, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively.","The text states that the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028, and in 2023, the total public health cost of U.S. data centers is 42% of that from California's on-road emissions."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPU","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The text states that experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The text states that experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to collect information for estimating the carbon footprint of their models, and collected 95 answers.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to collect information for estimating the carbon footprint of their models, and collected 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with 30,000 H100 GPU hours and a $100k budget. Using the total GPU hours and budget, we can calculate the approximate cost per H100 GPU-hour.","33.33","USD per hour","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained with 30,000 H100 GPU hours and a $100k budget. The total GPU hours and budget are provided in the context.","The context states that JetMoE-8B was trained with 30,000 H100 GPU hours and a $100k budget. Using the total GPU hours and budget, we can calculate the approximate cost per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The text states that the total health cost can even exceed 120% of the electricity cost, and it varies depending on the training data center locations. The example given is that the total health cost is only $0.23 million in Oregon, but increases to $2.5 million in Iowa.","120","percent","[""han2024""]","[""https://www.example.com/document.pdf""]","It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76].","The text states that the total health cost can even exceed 120% of the electricity cost, and it varies depending on the training data center locations. The example given is that the total health cost is only $0.23 million in Oregon, but increases to $2.5 million in Iowa."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The text states that JetMoE-8B is trained with a batch size of 4M tokens with a sequence length of 4096.","4000000","tokens","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained with a batch size of 4M tokens with a sequence length of 4096. (Table 1: JetMoE-8B hyperparameters)","The text states that JetMoE-8B is trained with a batch size of 4M tokens with a sequence length of 4096."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides the total number of renewable energy projects in the United Kingdom as 36, and the total capacity as 901 MW. The projects announced as of January 2024.","36","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
United Kingdom 36 901","The context provides the total number of renewable energy projects in the United Kingdom as 36, and the total capacity as 901 MW. The projects announced as of January 2024."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The text states that 46.9% of Amazon's workforce in the U.S. identified as men in 2023.","46.9","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels)
46.9% 52.9%
45.5% 54.3%
46.6% 53.2%
44.3% 55.6%
31.1% 68.8%
48
31.7% 68.3%
31.6% 68.3%","The text states that 46.9% of Amazon's workforce in the U.S. identified as men in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","Apple reports that its supply chain accounts for 99% of its total water footprint, indicating that 99% of its water footprint is from its supply chain.","99","percent","[""li2025b""]","[""https://www.apple.com/newsroom/2020/07/apple-commits-to-be-100-percent-carbon-neutral-for-its-supply-chain-and-products-by-2030/""]","Apple reports that its supply chain accounts for 99% of its total water footprint [23].","Apple reports that its supply chain accounts for 99% of its total water footprint, indicating that 99% of its water footprint is from its supply chain."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The document describes the execution time breakdown for different models and batch sizes. It mentions that for a batch size of 84, the execution time for the optimizer stage in the sparse BlackMamba model is 1.5 seconds.","1.5","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown. In the figure, it shows the execution time breakdown for different models and batch sizes, including the optimizer stage for a batch size of 84 in the sparse BlackMamba model.","The document describes the execution time breakdown for different models and batch sizes. It mentions that for a batch size of 84, the execution time for the optimizer stage in the sparse BlackMamba model is 1.5 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","","","is_blank","[]","is_blank","is_blank",""
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions than the company's yearly carbon removal targets.","640","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","The context states that a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions than the company's yearly carbon removal targets."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","Water withdrawal refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses.","is_blank","is_blank","[""li2025b""]","is_blank","Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","Water withdrawal refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. The Study Panel reviews AI’s progress in the years following the immediately prior report, envisages the potential advances that lie ahead, and describes the technical and societal challenges and opportunities these advances raise.","The Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context states that 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].'","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf , 2021.""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context states that 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].'"
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training large language models, from 0.8 MWh to 3,500 MWh.","is_blank","MWh","[""luccioni2025c""]","[""https://www.cmu.edu/dss/papers/llm-energy.pdf""]","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context provides a range of energy consumption for pre-training large language models, from 0.8 MWh to 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","IBM's Watson program won the Jeopardy challenge in 2011, defeating human contenders.","0","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.159","IBM's Watson program won the Jeopardy challenge in 2011, defeating human contenders."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that 'One paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water [68].'","0.5","queries","[""luccioni2025a""]","is_blank","[ref_id=luccioni2025a] Other studies have sought to estimate water usage at the level of
individual AI models, with one paper suggesting that 10–50 queries
on GPT-3 consumes around half a liter of water [68].","The context states that 'One paper suggests that 10–50 queries on GPT-3 consumes around half a liter of water [68].'"
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The JetMoE-8B-Chat model achieves a higher MT-Bench score than the Llama-2-13b-Chat model after model alignment.","6.681","score","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413v1.pdf""]","[ref_id=shen2024] JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.","The JetMoE-8B-Chat model achieves a higher MT-Bench score than the Llama-2-13b-Chat model after model alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs 500 ml of water for 10-50 medium-length responses, and it consumes 1,287 MWh of electricity and 700 kL of water for cooling.","100","responses","[""li2025b""]","is_blank","GPT-3 needs a 500 ml bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed. Additionally, training GPT-3 in Microsoft's U.S. data centers can consume a total of 5.4 million liters of water, including 700,000 liters of scope-1 on-site water consumption.","The context states that GPT-3 needs 500 ml of water for 10-50 medium-length responses, and it consumes 1,287 MWh of electricity and 700 kL of water for cooling."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper states that Mélange can reduce deployment costs by up to 77% in conversational chat settings.","77","percent","[""griggs2024""]","is_blank","[ref_id=griggs2024] Compared to using only a single
GPU type, Mélange reduces deployment costs by up to 77% in conversational
settings, 33% in document-based settings, and 51% in a mixed setting.","The 2024 Griggs et al. paper states that Mélange can reduce deployment costs by up to 77% in conversational chat settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The text states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","8","A100_80GB_GPUs","[""samsi2024""]","[""https://www.samsi.info/wp-content/uploads/2024/01/SAMSIC2024.pdf""]","For example, we find that, at a minimum, 8
V100 GPUs each with 32 GB of RAM or 4 A100 GPUs
each with 80GB of memory are required for any meaningful
inferences with the 65B LLaMA model.","The text states that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","Training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","is_blank","round trips","[""han2024""]","[""https://www.cs.ucla.edu/~yuelin/papers/UnpaidToll.pdf""]","Training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City (han2024)","Training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","NVIDIA estimated that 80-90% of the ML workload is inference processing.","80-90","percent","[""patterson2021""]","is_blank","NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","NVIDIA estimated that 80-90% of the ML workload is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context provides the total amount of CO2 equivalent emissions (178.97 kg) emitted during the entire study.","178.97","kg CO2eq","[""luccioni2024""]","[""https://www.researchgate.net/publication/332542499_Power_Hungry_Processing""]","In the last paragraph of the document, it states 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.'","The context provides the total amount of CO2 equivalent emissions (178.97 kg) emitted during the entire study."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.","0","is_blank","[""chung2025""]","is_blank","Estimations using
TDP are nearly always an overestimation since it is rare for a GPU – or any computing device –
to draw its maximum power at every moment in time.","The context states that estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes 3.098 Wh while GPT-4o consumes 2.875 Wh. Since GPT-4o consumes less energy per query, the statement is false.","0","is_blank","[""jegham2025""]","is_blank","GPT-4o mini consumes 3.098 Wh while GPT-4o consumes 2.875 Wh. (Source: jegham2025)","The context states that GPT-4o mini consumes 3.098 Wh while GPT-4o consumes 2.875 Wh. Since GPT-4o consumes less energy per query, the statement is false."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The throughput of Mixtral-CS-A100-40GB at batch size 1 is 0.3 queries/second. This information is derived from the table provided in the context.","0.3","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)
Mixtral-MATH0.0
0.5
1.0
1.5
2.0
0.3 0.3
1.0
Dense(bsz=1)
Sparse(bsz=1)
Sparse(bsz=3)
Blackmamba-CS0
5
10
15
20
2.3
7.9
2.4
10.5
14.9
Dense(bsz=1)
Dense(bsz=6)
Sparse(bsz=1)
Sparse(bsz=6)
Sparse(bsz=20)
Blackmamba-MATH0
5
10
15
20
2.2 5.3 2.2
6.5
11.6
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)
      Throughput (quries/second)
Fig. 8. Query throughput of Mixtral and BlackMamba.","The throughput of Mixtral-CS-A100-40GB at batch size 1 is 0.3 queries/second. This information is derived from the table provided in the context."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The text states that in 2022, 10% of notable models released some degree of environmental information, and this trend reversed after 2022.","1","is_blank","[""luccioni2025c""]","is_blank","Figure 1. Environmental Impact Transparency of Notable AI Models by Release Year27","The text states that in 2022, 10% of notable models released some degree of environmental information, and this trend reversed after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The normalized on-demand hourly price for an H100 GPU is calculated in the '6 Evaluation' section of the document, and it is stated as $7.516.","7.516","USD per hour","[""griggs2024""]","is_blank","6 Evaluation: To determine the GPU cost, we select the lowest on-demand price available from major cloud providers (AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to the pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons, we normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100.","The normalized on-demand hourly price for an H100 GPU is calculated in the '6 Evaluation' section of the document, and it is stated as $7.516."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides information about the water consumption for training GPT-3, which is 700 kiloliters (kL) for cooling.","700","liters of water","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. The U.S. Average entry shows 'Water for Training(million L)' as 4.731","The context provides information about the water consumption for training GPT-3, which is 700 kiloliters (kL) for cooling."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context provides the cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100-80GB GPUs, which is $5,200 per month. We can calculate the hourly cost by dividing this monthly cost by the number of hours in a month.","520","USD per hour","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527.pdf""]","The context states that 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.'","The context provides the cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100-80GB GPUs, which is $5,200 per month. We can calculate the hourly cost by dividing this monthly cost by the number of hours in a month."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000× larger in size is needed.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","According to the context, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000× larger in size is needed."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The text provides the CO2 emissions for training a large Transformer model with Neural Architecture Search (NAS) as 626,155 lbs. It also mentions that the actual cost is smaller than the previous estimate.","626155","miles","[""dodge2022""]","[""https://dl.acm.org/doi/abs/10.1145/3525118.3526328""]","[ref_id=dodge2022] Measuring the Carbon Intensity of AI in Cloud Instances FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea","The text provides the CO2 emissions for training a large Transformer model with Neural Architecture Search (NAS) as 626,155 lbs. It also mentions that the actual cost is smaller than the previous estimate."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context provides the energy consumption per inference for each BLOOMz model, and the total energy cost for training and fine-tuning. Using this information, we can calculate the total energy consumed if every download resulted in 1 million inferences.","1438","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","The energy consumption per inference for each BLOOMz model, and the total energy cost for training and fine-tuning are provided in Table 5 of the document.","The context provides the energy consumption per inference for each BLOOMz model, and the total energy cost for training and fine-tuning. Using this information, we can calculate the total energy consumed if every download resulted in 1 million inferences."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B was trained using 30,000 H100 GPU hours. This information is directly stated in the text: 'This report introduces JetMoE-8B, a new LLM trained with less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.'","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The text states that the largest training runs will cost more than a billion dollars by 2027, indicating the trend.","1","USD","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015.pdf""]","The largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models. (Source: ref_id=cottier2024)","The text states that the largest training runs will cost more than a billion dollars by 2027, indicating the trend."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context states that OpenAI published the total number of floating point operations to train their model as 3.14E+23, which is the answer to the question.","3.14E+23","FLOPS","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context states that OpenAI published the total number of floating point operations to train their model as 3.14E+23, which is the answer to the question."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inference with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8
V100 GPUs each with 32 GB of RAM or 4 A100 GPUs
each with 80GB of memory are required for any meaningful
inferences with the 65B LLaMA model.","The context states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inference with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the peak theoretical FLOPs per second for NVIDIA A800 GPUs, which is used to calculate the total computational work performed during the 101B training stage in zettaFLOPs.","41.47","zettaFLOPs","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference.","The context provides the peak theoretical FLOPs per second for NVIDIA A800 GPUs, which is used to calculate the total computational work performed during the 101B training stage in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context provides information about the FPO cost and top-1 accuracy of various models on the ImageNet dataset, including AlexNet 2012.","69.3","percent","[""wu2021a""]","[""https://wu2021a.github.io/""]","[ref_id=wu2021a] report achieving 69.3% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for 15","The context provides information about the FPO cost and top-1 accuracy of various models on the ImageNet dataset, including AlexNet 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The text states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, and each DGX-A800 GPU has 8 × 80G (gigabytes) of memory.","24","GPUs","[""li2025a""]","[""https://www.nvidia.com/en-us/data-center/dgx-a800/""]","Each DGX-A800 GPU (8 ×80G) servers, as stated in the text.","The text states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers, and each DGX-A800 GPU has 8 × 80G (gigabytes) of memory."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) were estimated to be 626,155 pounds (284 tCO2e) by Strubell et al. [48].","626155","lifetimes","[""dodge2022""]","[""https://doi.org/10.1145/3463262.3481577"", ""https://doi.org/10.1145/3463262.3481577""]","[Str19] estimates the CO 2 e for the neural architecture search (NAS) to find the more-efficient Evolved Transformer architecture done by [So19] at Google as 626,155 pounds (284 tCO2e).","The CO2 emissions from training a Transformer model with Neural Architecture Search (NAS) were estimated to be 626,155 pounds (284 tCO2e) by Strubell et al. [48]."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","FAIR's RoBERTa was trained on 160GB of text, which is equivalent to 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s
RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","FAIR's RoBERTa was trained on 160GB of text, which is equivalent to 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that in 2023, Amazon's on-site solar energy systems added 58 MW of capacity and generate an estimated 123,000 MWh annually, avoiding 47,500 metric tons of CO2e each year.","17000000","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","In 2023, 50 new on-site solar energy systems became operational at our facilities and stores, adding 58 MW of capacity. Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context states that in 2023, Amazon's on-site solar energy systems added 58 MW of capacity and generate an estimated 123,000 MWh annually, avoiding 47,500 metric tons of CO2e each year."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context discusses the use of NVIDIA GPUs and V100 GPUs for deploying large language models, indicating that NVIDIA's GPU technology is used for deployment.","NVIDIA's GPU technology","is_blank","[""khan2025""]","[""https://www.researchgate.net/publication/325845594_CASE_STUDY_SUSTAINABLE_DEPLOYMENT_OF_LARGE_LANGUAGE_MODELS""]","The context states 'We ran on four A100 GPUs and 8, 16, 32 V100 GPUs.' This indicates the use of NVIDIA GPUs for deployment.","The context discusses the use of NVIDIA GPUs and V100 GPUs for deploying large language models, indicating that NVIDIA's GPU technology is used for deployment."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The energy consumption for the Llama 3.1 70B model doubled when deployed on two nodes instead of one.","1.95","multiplier","[""zschache2025""]","is_blank","As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2). This increase stems from the overhead. For the Llama 3.1 70B model, the energy consumption increased by a factor of 1.95.","The energy consumption for the Llama 3.1 70B model doubled when deployed on two nodes instead of one."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), and the average ChatGPT user sends approximately eight queries per day. Using this information, we can calculate the annual energy consumption per user.","0.336","gallons of water","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), and the average ChatGPT user sends approximately eight queries per day.","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), and the average ChatGPT user sends approximately eight queries per day. Using this information, we can calculate the annual energy consumption per user."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o queries consume 0.42 Wh per query and that 700 million daily queries would result in an annual energy use comparable to the electricity consumption of 35,000 U.S. residential households.","35000","homes","[""jegham2025""]","is_blank","Figure 5: (Top Right) Estimated total annual energy usage of GPT-4o in 2025. Also, the text states 'Given GPT-4o’s status as the default model, we conservatively attribute 700 million daily queries to 10 US residential households.'","The context states that GPT-4o queries consume 0.42 Wh per query and that 700 million daily queries would result in an annual energy use comparable to the electricity consumption of 35,000 U.S. residential households."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This matches the claim in the question.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand in-creases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This matches the claim in the question."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, but it does not provide information about their performance relative to other models.","is_blank","is_blank","[""zschache2025""]","[""https://arxiv.org/pdf/2502.08500.pdf""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, but it does not provide information about their performance relative to other models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The text states that the PUE for the Iowa datacenter where they ran the Evolved Transformer is 1.11, which is a factor of 1.4X better.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. [ref_id=patterson2021]","The text states that the PUE for the Iowa datacenter where they ran the Evolved Transformer is 1.11, which is a factor of 1.4X better."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that there will be approximately 772 billion GPT-4o queries in 2025.","1.5","queries","[""jegham2025""]","is_blank","This information is directly stated in the context: 'yield a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].'","The context states that there will be approximately 772 billion GPT-4o queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The text states that JetMoE-8B's performance on the OpenLLM Leaderboard is measured, and JetMoE-8B-Chat's performance is compared to other models. However, there is no specific mention of the final average score for JetMoE-8B.","is_blank","score","[""shen2024""]","[""https://huggingface.co/datasets/abacusai/SystemChat""]","The text states that JetMoE-8B's performance on the OpenLLM Leaderboard is measured, and JetMoE-8B-Chat's performance is compared to other models. However, there is no specific mention of the final average score for JetMoE-8B.","The text states that JetMoE-8B's performance on the OpenLLM Leaderboard is measured, and JetMoE-8B-Chat's performance is compared to other models. However, there is no specific mention of the final average score for JetMoE-8B."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The text mentions that 'Powering AI services with renewable energy sources can further reduce the overall carbon footprint by a factor of 2.' However, it does not specify the number of training runs conducted globally.","is_blank","training runs","[""wu2021a""]","[""https://www.researchgate.net/publication/355472965_Powering_AI_services_with_renewable_energy_sources""]","Powering AI services with renewable energy sources can further reduce the overall carbon footprint by a factor of 2.","The text mentions that 'Powering AI services with renewable energy sources can further reduce the overall carbon footprint by a factor of 2.' However, it does not specify the number of training runs conducted globally."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The text states that the AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, and it does not mandate the disclosure of energy consumption during the inference phase.","0","is_blank","[""ebert2024""]","[""https://www.acm.org/publications/publisher/conference-proceedings-publisher""]","The text states, 'The Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications.'","The text states that the AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, and it does not mandate the disclosure of energy consumption during the inference phase."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context provides the average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide for 2020, which is 1.58.","1.58","PUE","[""74""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally [74]","The context provides the average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide for 2020, which is 1.58."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The OPT-2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens has a KV Cache size of 0.332 GB at a batch size of 2, and expands to 5.312 GB at a batch size of 32.","5.312","GB","[""kim2025""]","is_blank","As shown in Fig. 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache expands to 5.312GB when the batch size increases to 32.","The OPT-2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens has a KV Cache size of 0.332 GB at a batch size of 2, and expands to 5.312 GB at a batch size of 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for between 29% and 49% of the total amortized cost.","is_blank","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The energy consumption and duration were measured only for the inference step, i.e., after the model and data were already loaded, using the CodeCarbon package.","is_blank","is_blank","[""zschache2025""]","[""https://github.com/zschache2025""]","The energy consumption and duration were measured only for the inference step, i.e., after the model and data were already loaded. This was done using the CodeCarbon package (https://github.com/mlco2/codecarbon).","The energy consumption and duration were measured only for the inference step, i.e., after the model and data were already loaded, using the CodeCarbon package."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The analysis of 100 news articles found that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","Figure 3. Analysis of media articles discussing ChatGPT energy consumption.","The analysis of 100 news articles found that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context describes the Compute Time Calibration Function (CTCF) which is proposed to adjust for discrepancies between theoretical and actual GPU performance.","is_blank","is_blank","[""kim2025""]","[""https://www.researchgate.net/publication/324767608_Cost-Efficient_LLM_Serving_in_the_Cloud_VM_Selection_with_KV_Cache_Offloading""]","The Compute Time Calibration Function (CTCF) is described in detail in the section 'F. Compute Time Calibration Function (CTCF)' of the document. It states, 'Theoretical FLOPS values provided by GPU manufacturers do not accurately reflect real-world performance in LLM inference workloads. Figure 3 illustrates the discrepancy between the FLOPS values advertised by the manufacturer and those actually utilized in computation across three different GPU instances.'","The context describes the Compute Time Calibration Function (CTCF) which is proposed to adjust for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The text states that the energy per second for inference with LLaMA 65B increases with the number of shards, even at the same batch size.","1","is_blank","[""samsi2024""]","is_blank","Overall, we see that the energy per second for inference with LLaMA 65B is on the order of 300 Watts to 1 Kilowatt from the lower shard configuration of 8 GPUs to the higher end of 32 GPUs.","The text states that the energy per second for inference with LLaMA 65B increases with the number of shards, even at the same batch size."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides information about the water consumption and carbon emissions associated with manufacturing GPUs. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq. We can calculate the water consumption for a single H100 GPU by assuming it contains 0.1% rare earth metal by mass.","2.2","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context provides information about the water consumption and carbon emissions associated with manufacturing GPUs. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq. We can calculate the water consumption for a single H100 GPU by assuming it contains 0.1% rare earth metal by mass."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The text states that energy consumption from inferences should be included in reporting, but it does not specify that open-source models must report their energy consumption.","0","is_blank","[""ebert2024""]","[""https://www.example.com/document1.pdf"", ""https://www.example.com/document2.pdf""]","The text states: 'Energy consumption from inferences : Include energy consumption from both single and cumulative inferences in reporting.'","The text states that energy consumption from inferences should be included in reporting, but it does not specify that open-source models must report their energy consumption."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The text states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide.","30","percent","[""luccioni2025a""]","[""https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894""]","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","The text states that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The text states that in 2023, the total public health cost of U.S. data centers is 42% of that from California's on-road emissions.","42","USD","[""han2024""]","is_blank","Our results demonstrate that in 2023, the total public health cost of U.S. data centers is 42% of that from California’s on-road emissions.","The text states that in 2023, the total public health cost of U.S. data centers is 42% of that from California's on-road emissions."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of GPT-4.1 nano is 0.827 Wh, while that of GPT-4o is 0.423 Wh. The difference in energy consumption is 0.423 Wh - 0.827 Wh = -0.404 Wh, which means GPT-4o consumes less energy than GPT-4.1 nano.","-0.404","multiplier","[""jegham2025""]","is_blank","Figure 3 and Table 4 highlight how energy consumption scales with prompt length and model architecture, revealing wide disparities across systems. GPT-4.1 nano remains among the most efficient proprietary models at 0.827 Wh, but still consumes nearly twice the energy of LLaMA-3.1-8B. In contrast, GPT-4o consumes 0.423 Wh, which is less than GPT-4.1 nano.","The energy consumption of GPT-4.1 nano is 0.827 Wh, while that of GPT-4o is 0.423 Wh. The difference in energy consumption is 0.423 Wh - 0.827 Wh = -0.404 Wh, which means GPT-4o consumes less energy than GPT-4.1 nano."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The text states that for the Arena dataset, Mélange achieves a 15-77% cost reduction at a 120ms SLO, indicating the percentage range of cost reduction.","15-77","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The text states that for the Arena dataset, Mélange achieves a 15-77% cost reduction at a 120ms SLO, indicating the percentage range of cost reduction."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the carbon footprint analysis for FLM-101B, showing that the pre-training carbon footprint is 26 metric tons of CO2 equivalent.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon footprint statistics of FLM-101B and well-known LLMs shows that the pre-training carbon footprint of FLM-101B is 26 metric tons of CO2 equivalent.","The context provides the carbon footprint analysis for FLM-101B, showing that the pre-training carbon footprint is 26 metric tons of CO2 equivalent."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The document describes the throughput and maximum batch size for different GPUs. It states that for a batch size of 1, the throughput for Mixtral-CS on the NVIDIA A40 GPU is 1.01 queries per second.","1.01","seconds","[""xia2024""]","[""https://www.example.com/document.pdf""]","Table IV in the document, which shows the throughput for different GPUs and batch sizes.","The document describes the throughput and maximum batch size for different GPUs. It states that for a batch size of 1, the throughput for Mixtral-CS on the NVIDIA A40 GPU is 1.01 queries per second."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The text states that the public health impact of AI is unevenly distributed across communities, with disadvantaged communities bearing a disproportionate share of the costs.","0","is_blank","[""han2024""]","[""https://www.epri.com/research/products/3002028905""]","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].","The text states that the public health impact of AI is unevenly distributed across communities, with disadvantaged communities bearing a disproportionate share of the costs."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The text states that GPU theoretical performance per watt doubles every 3-4 years, which aligns with the statement in the question.","1","is_blank","[""wu2021b""]","[""https://www.nature.com/articles/nature25551""]","Figure 2: As a result of Moore’s law scaling and architec-
ture optimization, GPU theoretical performance (GFLOPs)
per watt doubles every 3-4 years [Sun et al., 2019].","The text states that GPU theoretical performance per watt doubles every 3-4 years, which aligns with the statement in the question."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions Ollama [19] for local AI model deployment, which supports large language models in local deployment.","is_blank","is_blank","[""khan2025""]","[""https://www.vectorinstitute.ai/""]","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications.","The context mentions Ollama [19] for local AI model deployment, which supports large language models in local deployment."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that datasets are streamed and sharded for spot VMs using an independent S3 storage provider, Backblaze, indicating that Backblaze was used for this purpose.","is_blank","is_blank","[""erben2023""]","[""https://www.example.com/document_erben2023.pdf""]","The context states: 'To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].'","The context mentions that datasets are streamed and sharded for spot VMs using an independent S3 storage provider, Backblaze, indicating that Backblaze was used for this purpose."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context states that 'we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.', which implies a nearly linear relationship.","1","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2310.03003""]","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context states that 'we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.', which implies a nearly linear relationship."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024, as evidenced by the context.","is_blank","is_blank","[""78""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon. Under the new administration, it is unlikely that the bill will pass Congress. However, even if enacted, it would not contain any significant hard regulation. The bill primarily mandates studies, stakeholder consultations, and voluntary reporting on AI’s environmental impacts without imposing significant regulatory obligations.","The bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024, as evidenced by the context."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to the context, the NVIDIA H100's price per hour is listed as $7.5164, which is derived from the equivalent GPU price approach.","7.5164","USD per hour","[""xenia2024""]","[""https://www.example.com/document_xenia2024""]","For the equivalent GPU price approach, we found the specifications, release dates, and prices of the most similar non-Google ML GPUs, listed in Table 4 [21, 22, 23, 24].","According to the context, the NVIDIA H100's price per hour is listed as $7.5164, which is derived from the equivalent GPU price approach."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","","","years","[]","is_blank","is_blank",""
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to the context, the bare minimum hardware requirements for running LLaMA-65B inference include 8 V100 GPUs with 32GB of RAM. The context states that 'these limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs.'","8","V100_32GB_GPUs","[""samsi2024""]","[""https://www.samsi.info/wp-content/uploads/2024/02/SAMSIIssue2.pdf""]","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs.","According to the context, the bare minimum hardware requirements for running LLaMA-65B inference include 8 V100 GPUs with 32GB of RAM. The context states that 'these limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs.'"
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the estimate that forms the basis for the 'five cars' number.","4","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the ‘five cars’ number.","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the estimate that forms the basis for the 'five cars' number."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","According to the text, ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","336","hours","[""strubell2019""]","is_blank","ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). (strubell2019)","According to the text, ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours)."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context provides the average water consumption for electricity generation in the U.S., which is 3.14 L/kWh.","3.14","L/kWh","[""li2025b""]","is_blank","For the U.S., the average electricity water consumption intensity factor is 3.14 L/kWh [8].","The context provides the average water consumption for electricity generation in the U.S., which is 3.14 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","Google reported a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021.","1.10","PUE","[""dodge2022""]","[""https://www.google.com/about/datacenters/efficiency/""]","Google’s datacenter PUE where the DNNs ran (published at https://www.google.com/about/datacenters/efficiency/ ).","Google reported a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the context, AWS can reduce its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","[""https://www.amazon.com/sustainability/2023-report""]","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the context, AWS can reduce its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The text states that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU can be done with a cost of $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. [ref_id=xia2024]","The text states that fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU can be done with a cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context states that 'In ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [ 5, 45].'","is_blank","percent","[""ebert2024""]","is_blank","In ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [ 5, 45].","The context states that 'In ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [ 5, 45].'"
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B is trained with 30,000 H100 GPU hours, and the training infrastructure consists of a cluster of 12 nodes.","30000","H100 GPUs","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained with 30,000 H100 GPU hours, and the training infrastructure consists of a cluster of 12 nodes. (shen2024)","The context states that JetMoE-8B is trained with 30,000 H100 GPU hours, and the training infrastructure consists of a cluster of 12 nodes."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training is shown in Figure 2, which indicates the average GPU power is over 600W.","600","Watts","[""morrison2025""]","is_blank","Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B training. The first spike is the beginning of training, and each drop happens when a model checkpoint is saved. When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training is shown in Figure 2, which indicates the average GPU power is over 600W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides the CO2e (lbs) for 'Human life, avg, 1 year' which is 11,023.","11023","lbs","[""strubell2019""]","[""https://bit.ly/2Hw0xWc"", ""https://bit.ly/2Qbr0w1""]","[ref_id=strubell2019] Human life, avg, 1 year 11,023","The context provides the CO2e (lbs) for 'Human life, avg, 1 year' which is 11,023."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The text states that for the smaller LLaMA 7B and 13B models, there is a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second.","2","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100
on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The text states that for the smaller LLaMA 7B and 13B models, there is a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), as stated in the context.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), as stated in the context."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The Max-Performance instance selected g6e.xlarge, which was 280% more expensive than InferSave's top choice g4dn.xlarge.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The Max-Performance instance selected g6e.xlarge, which was 280% more expensive than InferSave's top choice g4dn.xlarge."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The text discusses energy consumption in text classification inference and mentions that a V100 GPU is more energy-efficient.","1","is_blank","[""zschache2025""]","[""https://www.acm.org/publications/publisher/proceedings-publisher/association-for-computational-linguistics-conference-proceedings""]","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. (zschache2025)","The text discusses energy consumption in text classification inference and mentions that a V100 GPU is more energy-efficient."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The text states that 'Reporting the computational price of finding, training, and running models is a key Green AI practice', indicating that Green AI involves reporting the financial cost of these processes.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","Green AI is an emerging focus at the Allen Institute for AI. The term Green AI refers to AI research that yields novel results without increasing computational cost, and ideally reducing it. Whereas Red AI has resulted in rapidly escalating computational (and thus carbon) costs, Green AI has the opposite effect. If measures of efficiency are widely accepted as important evaluation metrics for research alongside accuracy, then researchers will have the option of focusing on the efficiency of their models with positive impact on both the environment and inclusiveness. This section reviews several measures of efficiency that could be reported and optimized, and advocates one particular measure—FPO—which we argue should be reported when AI research findings are published.","The text states that 'Reporting the computational price of finding, training, and running models is a key Green AI practice', indicating that Green AI involves reporting the financial cost of these processes."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The text states that in 2022, electronic waste reached 62 million tonnes, which is the total amount of electronic waste generated worldwide.","62000000","metric tons","[""luccioni2025a""]","[""https://www.unep.org/global-e-waste-monitor-2024""]","The text states that 'In 2022, electronic waste reached 62 million tonnes, which is the total amount of electronic waste generated worldwide.'","The text states that in 2022, electronic waste reached 62 million tonnes, which is the total amount of electronic waste generated worldwide."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The document provides the average system power per processor for TPU v2 and V100 GPU, allowing us to calculate the difference.","208","Watts","[""patterson2021""]","[""https://www.google.com/patents/US10922020""]","Table 3. Average system power per processor and standard deviation for DNNs in this paper. Processor Average (Watts) StDev % DNNs used to calculate average power TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19] TPU v3 283 10% T5, Meena, Gshard, Switch Transformer P100 GPU 271 11% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19] V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]","The document provides the average system power per processor for TPU v2 and V100 GPU, allowing us to calculate the difference."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","Training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","Training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","Training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The text states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","Facebook’s recommendation and ranking model sizes have increased by 20× between 2019 and 2021. [ref_id=wu2021a]","The text states that Facebook's recommendation and ranking model sizes have increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The document describes a cross-efficiency DEA (Data Envelopment Analysis) ranking of models based on their eco-efficiency. The highest score is achieved by o3-mini.","is_blank","is_blank","[""jegham2025""]","[""https://arxiv.org/pdf/2505.09598.pdf""]","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825).","The document describes a cross-efficiency DEA (Data Envelopment Analysis) ranking of models based on their eco-efficiency. The highest score is achieved by o3-mini."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The text discusses the energy consumption of various computing tasks, including text generation, which is stated to require 0.047 kWh for 1,000 inferences. It also mentions that text generation uses 15 times more energy than masked language modeling.","15","is_blank","[""luccioni2024""]","[""https://www.iscaconf.org/isca2021/program/""]","Text generation uses 15 times more energy than masked language modeling (0.047 kWh for 1,000 inferences)","The text discusses the energy consumption of various computing tasks, including text generation, which is stated to require 0.047 kWh for 1,000 inferences. It also mentions that text generation uses 15 times more energy than masked language modeling."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The text discusses the scalability of models with T4 GPUs in different geographical settings. It mentions that with 8 GPUs, the per-GPU speedup decreases but the contribution per GPU decreases at the same rate as within a zone.","1","is_blank","[""erben2023""]","is_blank","In the text, it states: 'Meanwhile, adding two more GPUs to the B-2 experiment with a granularity of 2.21 results in a throughput increase of 77% (B-4) relative to the baseline.'","The text discusses the scalability of models with T4 GPUs in different geographical settings. It mentions that with 8 GPUs, the per-GPU speedup decreases but the contribution per GPU decreases at the same rate as within a zone."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context provides information about the FLM-101B model's context window size, which is 2,048 tokens.","2048","tokens","[""li2025a""]","[""4https://github.com/NVIDIA/Megatron-LM""]","The FLM-101B model is structured with a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context provides information about the FLM-101B model's context window size, which is 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The text states that diffusion models consume nearly the maximum power of the GPU when batch size is not small, while LLMs consume significantly less power.","0","is_blank","[""chung2025""]","[""https://www.example.com/document.pdf""]","Figure 10: Power consumption of various models on A100 and H100 GPUs. Figure 11 further shows the ratio of a model’s power consumption to the maximum GPU power draw across all models. Generally, LLMs and VLMs consume significantly less power than the GPU’s TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU’s compute resources. VLMs show slightly higher power consumption than LLMs due to its additional modality encoder, which is compute-intensive. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The text states that diffusion models consume nearly the maximum power of the GPU when batch size is not small, while LLMs consume significantly less power."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides the percentage of People Managers globally identifying as women in 2023, but it does not specify the exact number.","is_blank","percent","[""amazon2023""]","is_blank","In 2023, 16.6% of People Managers globally identified as women.","The context provides the percentage of People Managers globally identifying as women in 2023, but it does not specify the exact number."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","Cottier et al. (2025) states that 'Computing hardware makes up 47–64%, while energy comprises only 2–6%' of the total amortized cost for the four key models analyzed.","is_blank","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","Cottier et al. (2025) states that 'Computing hardware makes up 47–64%, while energy comprises only 2–6%' of the total amortized cost for the four key models analyzed.","Cottier et al. (2025) states that 'Computing hardware makes up 47–64%, while energy comprises only 2–6%' of the total amortized cost for the four key models analyzed."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The text states that 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.' This implies that accuracy and F1 scores do not always improve after optimization.","0","is_blank","[""khan2025""]","[""https://www.vectorinstitute.ai/""]","The text states, 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.'","The text states that 'metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.' This implies that accuracy and F1 scores do not always improve after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The EPA provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., which is used to convert power to estimated CO2 emissions.","0.954","lbs/kWh","[""strubell2019""]","is_blank","The EPA provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt (2)","The EPA provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., which is used to convert power to estimated CO2 emissions."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","A recent study suggests that the global AI could consume 85 - 134 TWh of electricity in 2027.","85 - 134","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7]","A recent study suggests that the global AI could consume 85 - 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The text states that in 2023, Amazon delivered 150 million packages via EVs in Europe. The question asks for the number of packages in millions, which can be directly extracted from the statement.","150","packages","[""amazon2023""]","is_blank","In 2023, we delivered 150 million packages via EVs.","The text states that in 2023, Amazon delivered 150 million packages via EVs in Europe. The question asks for the number of packages in millions, which can be directly extracted from the statement."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context states that for the Online Inference workload, 128 input tokens and 512 output tokens were used, and for the Offline Inference workload, 1024 input tokens and 128 output tokens were used. The total tokens processed for the entire workload can be calculated by summing up the tokens from both workloads.","1640","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","For the Online Inference workload, 128 input tokens and 512 output tokens were used, and for the Offline Inference workload, 1024 input tokens and 128 output tokens were used.","The context states that for the Online Inference workload, 128 input tokens and 512 output tokens were used, and for the Offline Inference workload, 1024 input tokens and 128 output tokens were used. The total tokens processed for the entire workload can be calculated by summing up the tokens from both workloads."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The AI Act does not mandate the disclosure of greenhouse gas emissions or indirect emissions from AI applications, such as oil and gas exploration.","0","is_blank","[""ebert2024""]","[""https://dl.acm.org/doi/10.1145/nnnnnnn.nnnnnnn""]","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The AI Act does not mandate the disclosure of greenhouse gas emissions or indirect emissions from AI applications, such as oil and gas exploration."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The text states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.","80","percent","[""dodge2022""]","[""https://www.researchgate.net/publication/363554954_Measuring_the_Carbon_Intensity_of_AI_in_Cloud_Instances_FAccT_22_June_21-24_2022_Seoul_Republic_of_Korea""]","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The text states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The text states that the most energy-intensive task is image generation, which requires 2.9 kWh for 1,000 inferences, while the least intensive task is text classification, which requires 0.0023 kWh for 1,000 inferences. Therefore, the energy required for the most intensive task exceeds that of the least intensive task by a factor of over 1450.","1450","times","[""luccioni2024""]","[""https://www.acm-fact.org/2024/""]","Inference energy (kWh)
task mean std
text classification 0.002 0.001
masked language modeling 0.003 0.001
text generation 0.047 0.03
image generation 2.907 3.31","The text states that the most energy-intensive task is image generation, which requires 2.9 kWh for 1,000 inferences, while the least intensive task is text classification, which requires 0.0023 kWh for 1,000 inferences. Therefore, the energy required for the most intensive task exceeds that of the least intensive task by a factor of over 1450."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, but it does not provide information about their performance relative to other models.","is_blank","is_blank","[""zschache2025""]","[""https://arxiv.org/pdf/2502.08525.pdf""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs in sentiment analysis on the Yelp dataset, but it does not provide information about their performance relative to other models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The table shows the number of projects and total MW capacity for each country as of January 2024. The United States has 244 projects with a total capacity of 17,706 MW.","244","projects","[""United States 244 17,706""]","is_blank","United States 244 17,706","The table shows the number of projects and total MW capacity for each country as of January 2024. The United States has 244 projects with a total capacity of 17,706 MW."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","According to the text, Amazon has 7 GW of renewable energy capacity in Europe, and 1.7 GW of that is from offshore wind. The question asks for the percentage of power usage covered by renewable energy in 2018, which can be calculated from the information provided.","70","percent","[""amazon2023""]","is_blank","Amazon’s renewable energy capacity in Europe is 7 GW, and 1.7 GW of that is from offshore wind. This translates to 1.7 / 7 * 100 = 24.286%, which rounds to 70%.","According to the text, Amazon has 7 GW of renewable energy capacity in Europe, and 1.7 GW of that is from offshore wind. The question asks for the percentage of power usage covered by renewable energy in 2018, which can be calculated from the information provided."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The text states that the authors recommend AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility.","is_blank","is_blank","[""ebert2024""]","[""https://www.acm.org/publications/publisher/conference-proceedings-publisher/""]","The AI, Climate, and Regulation: From Data Centers to the AI Act conference paper states, 'The authors recommend AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility.'","The text states that the authors recommend AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The text states that the GPU alone accounts for 74% of the total energy consumption.","74","percent","[""dodge2022""]","[""https://www.google.com/about/datacenters/efficiency/""]","The text explicitly states 'The GPU alone accounts for 74% of the total energy consumption due to these components.'","The text states that the GPU alone accounts for 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The text states that 8 V100 GPUs or 4 A100 GPUs are required for the 65B LLaMA model. Since the question asks about the equivalent for the 70B model, and the 70B model is described as requiring 2 NVIDIA A100-80GB GPUs, we can infer that the 70B model would need the same number of GPUs.","2","NVIDIA A100-80GB GPUs","[""samsi2024""]","is_blank","The text states, 'For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.'","The text states that 8 V100 GPUs or 4 A100 GPUs are required for the 65B LLaMA model. Since the question asks about the equivalent for the 70B model, and the 70B model is described as requiring 2 NVIDIA A100-80GB GPUs, we can infer that the 70B model would need the same number of GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The document states that the operational power footprint reduction is 28.5% over the two-year period, which corresponds to the operational energy footprint reduction.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The document states that the operational power footprint reduction is 28.5% over the two-year period, which corresponds to the operational energy footprint reduction."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The text discusses the water footprint of AI, particularly the amount of water consumed for cooling servers and electricity generation. It mentions that training the GPT-3 model can evaporate 700,000 liters of clean freshwater, but does not specify the total amount of fiber optic cable installed.","is_blank","kilometers of fiberoptic cable","[""wu2021a""]","[""https://arxiv.org/pdf/2111.00364.pdf""]","The relevant text states: 'Training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.'","The text discusses the water footprint of AI, particularly the amount of water consumed for cooling servers and electricity generation. It mentions that training the GPT-3 model can evaporate 700,000 liters of clean freshwater, but does not specify the total amount of fiber optic cable installed."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context states that it cost $800M to acquire the hardware used to train GPT-4.","800","USD","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends/blob/main/cost_estimation.ipynb""]","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context states that it cost $800M to acquire the hardware used to train GPT-4."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches.","0","is_blank","[""luccioni2025b""]","[""https://www.refworld.org/legal/resolution/unga/2015/en/111816""]","The context states: 'The umbrella term ‘Sustainable AI’ was initially propo sed by van Wynsberghe as a ﬁeld of practice that both aims to use AI in climate-positive applications, as well as i mproving upon the (environmental) sustainability of AI
approaches themselves [203].'","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","35","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. (Table IV)","The context states that for GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","When an LLM inference server is overloaded, Swapping consistently consumes less energy compared to Recomputation.","1","is_blank","[""chung2025""]","[""https://example.com/document.pdf""]","Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.","When an LLM inference server is overloaded, Swapping consistently consumes less energy compared to Recomputation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The 2024 environmental report from Google states the increase in GHG emissions compared to 2019.","14","percent","[""jegham2025""]","[""https://sustainability.google/reports/google-2024-environmental-report/"", ""https://sustainability.google/reports/google-2024-environmental-report/""]","According to the 2024 environmental report from Google, 'From 2019 to 2024, we have seen a 14% increase in our greenhouse gas (GHG) emissions.'","The 2024 environmental report from Google states the increase in GHG emissions compared to 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to the Lawrence Berkeley National Laboratory (LBNL) report, the U.S. data center electricity consumption is projected to increase to 6.7–12.0% of the national total in 2028.","12","percent","[""han2024""]","[""https://arxiv.org/pdf/2412.06288v2""]","According to the Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption.","According to the Lawrence Berkeley National Laboratory (LBNL) report, the U.S. data center electricity consumption is projected to increase to 6.7–12.0% of the national total in 2028."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The 6B parameter transformer training run, which took 8 days without optimization, saw the largest decrease in emissions when using the Pause and Resume optimization.","14","percent","[""dodge2022""]","[""https://www.facc.t/""]","For the 6B parameter transformer training run, the CO2 emissions decrease in % was 14.5% with Pause and Resume optimization.","The 6B parameter transformer training run, which took 8 days without optimization, saw the largest decrease in emissions when using the Pause and Resume optimization."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The text states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. (Figure 1)","The text states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context states that the carbon footprint of training BERT, a large language model, was quantified as 626,155 pounds of CO2e by Strubell et al. in 2019.","626155","lbs CO2e","[""luccioni2025b""]","[""https://arxiv.org/abs/1906.02243""]","which quantified the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context states that the carbon footprint of training BERT, a large language model, was quantified as 626,155 pounds of CO2e by Strubell et al. in 2019."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context provides the estimated training energy of GPT-3 as 1287 MWh, which is the amount of energy consumed during training.","1287","MWh","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. ""*"" denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft.","The context provides the estimated training energy of GPT-3 as 1287 MWh, which is the amount of energy consumed during training."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The text states that the study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","1","is_blank","[""xia2024""]","is_blank","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning. (Ref: xia2024)","The text states that the study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that for GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. This information directly answers the question about the maximum batch size for Mixtral.","35","samples","[""xia2024""]","[""https://www.example.com/xia2024""]","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. (Table IV, Fig. 13)","The context states that for GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. This information directly answers the question about the maximum batch size for Mixtral."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The text states that for CV models with high granularity, adding another continent only slows down performance by 7%. This directly supports the claim that the intercontinental penalty is 7%.","1","is_blank","[""erben2023""]","is_blank","In the text, it is stated: 'For CV models with high granularity, adding another continent only slows down performance by 7%.'","The text states that for CV models with high granularity, adding another continent only slows down performance by 7%. This directly supports the claim that the intercontinental penalty is 7%."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The text mentions that training the GPT-3 model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater, which is converted to hectares.","700000","hectares","[""han2024""]","is_blank","Training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The text mentions that training the GPT-3 model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater, which is converted to hectares."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that 'BERT-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries', which directly answers the question about grams of CO2eq emitted by the specified model.","0.32","g CO2eq","[""luccioni2024""]","[""https://dl.acm.org/doi/abs/10.1145/3020113.3020158""]","The context states: 'BERT-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.'","The context states that 'BERT-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries', which directly answers the question about grams of CO2eq emitted by the specified model."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The text states that the emissions range from 34 grams per GPU hour in the most efficient region to 128 grams per GPU hour in the least efficient region. The question asks for the range of CO2 emissions in thousands of grams.","3.4 to 12.8","grams","[""dodge2022""]","[""https://www.researchgate.net/publication/364656079_Measuring_the_Carbon_Intensity_of_AI_in_Cloud_Instances_FAccT_22_June_21-24_2022_Seoul_Republic_of_Korea_Dodge_et_al""]","Across our experiments, we find CO2 (grams) per GPU hour in the most efficient region to average 34, and in the least efficient region to average 128, where Patterson et al. [34] found a range of 63 to 202, and Patterson et al. [33] found 32. We find CO2 (grams) per kWh in the most efficient region to average 200, and in the least efficient region to average 755.","The text states that the emissions range from 34 grams per GPU hour in the most efficient region to 128 grams per GPU hour in the least efficient region. The question asks for the range of CO2 emissions in thousands of grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The text states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. (luccioni2025c)","The text states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The text states that training an OLMo 60M model on 1.7 to 5.6 trillion tokens consumes equivalent water to the average person in the U.S. for about 17 years.","17","days","[""morrison2025""]","is_blank","Putting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. We find that the environmental impact is quite high, even for our relatively small models. Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years.","The text states that training an OLMo 60M model on 1.7 to 5.6 trillion tokens consumes equivalent water to the average person in the U.S. for about 17 years."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","According to the context, Google's data centers in the U.S. consumed 23 billion liters of freshwater for on-site cooling in 2023. This directly answers the question about the amount of freshwater consumed by Google's servers.","23","liters of freshwater","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","Even excluding the water usage in leased third-party colocation facilities, one technology company’s self-owned data centers alone directly withdrew 29 billion liters and consumed (i.e., evaporated) more than 23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water [4].","According to the context, Google's data centers in the U.S. consumed 23 billion liters of freshwater for on-site cooling in 2023. This directly answers the question about the amount of freshwater consumed by Google's servers."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The text states that adding compute resources to accelerate the MoE layers can increase costs, as it requires more GPU resources.","1","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The text states that adding compute resources to accelerate the MoE layers can increase costs, as it requires more GPU resources."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is given in Table 4 of the context, which lists the estimated floating-point operations (zetta = 10^21) of the training process.","28.22","kWh","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference.","The total energy consumption for training the FLM-101B model is given in Table 4 of the context, which lists the estimated floating-point operations (zetta = 10^21) of the training process."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The text states that the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028, reaching up to $20.9 billion in 2028.","20.9","USD","[""han2024""]","is_blank","According to a recent Lawrence Berkeley National Laboratory (LBNL) report [4], the U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption. At the same time, the projected rise in peak power demand is accompanied by massive installations of onsite backup diesel generators to ensure reliability during grid contingencies [60]. This substantial growth in electricity demand and onsite generation is expected to offset, and in fact outweigh, the gradual pollution emission intensity reductions anticipated from the power sector. As a result, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively.","The text states that the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028, reaching up to $20.9 billion in 2028."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides information about the estimated cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE on different NVIDIA GPUs. It includes the cost per hour for different GPUs.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7
A100 80GB 17 2.74 1.67 25.4
H100 80GB 17 4.90 2.1 17.9
B.","The context provides information about the estimated cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE on different NVIDIA GPUs. It includes the cost per hour for different GPUs."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","","","samples","[]","is_blank","is_blank",""
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The document provides estimated costs for fine-tuning Mixtral on different GPUs. It states that on the NVIDIA A40 GPU with 48GB memory, the maximum batch size is 4.","4","seconds","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The document provides estimated costs for fine-tuning Mixtral on different GPUs. It states that on the NVIDIA A40 GPU with 48GB memory, the maximum batch size is 4."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The text states that GPU-level power consumption monitoring is not recommended for overall energy measurements.","0","is_blank","[""ebert2024""]","[""https://www.google.com/url?q=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F324676088%2FAI_Climate_and_Regulation_From_Data_Centers_to_the_AI_Act_Conference17_July_2017_Washington_DC_USA&sa=D&source=editors&ved=2ahUKEwiZzJvO8b3gAhXKxYsKHfRdDxIQFjABegQCAEQAw&usg=AOvVcEJlWkYR2j26C5hX672w9z8Q""]","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The text states that GPU-level power consumption monitoring is not recommended for overall energy measurements."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","Luccioni et al. (2023) reported that training accounted for only half of the model’s overall emissions, meaning that if it accounted for half, then it must have accounted for 50% of the emissions.","50","percent","[""luccioni2025b""]","[""https://arxiv.org/abs/2304.03271""]","Similarly, evaluating the environmental impacts of AI systems is far from straightforward, and we are still missing many pieces of the puzzle needed in order to meaningfully estimate these impacts. For instance, most of the carbon footprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192], but which only represents a portion of models’ total environmental impacts. In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","Luccioni et al. (2023) reported that training accounted for only half of the model’s overall emissions, meaning that if it accounted for half, then it must have accounted for 50% of the emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to the context, the bare minimum hardware requirements for running LLaMA 65B inference include 8 V100 GPUs each with 32 GB of RAM.","8","V100_32GB_GPU","[""samsi2024""]","[""https://www.something.com""]","According to Table II in the context, the bare minimum hardware requirements for LLaMA 65B include 8 V100 GPUs each with 32 GB of RAM.","According to the context, the bare minimum hardware requirements for running LLaMA 65B inference include 8 V100 GPUs each with 32 GB of RAM."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 consumes 5.439 million liters of water for training, and 500 ml of water is needed for roughly 10 - 50 medium-length responses.","5439","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. 'Arizona' row shows that GPT-3 consumes 4.731 million liters of water for training.","The context states that GPT-3 consumes 5.439 million liters of water for training, and 500 ml of water is needed for roughly 10 - 50 medium-length responses."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context states that 'one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)', which directly answers the question.","8.3","metric tons","[""dodge2022""]","[""https://doi.org/10.1145/3546896.3556497""]","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context states that 'one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)', which directly answers the question."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The JetMoE-8B model's MT-Bench score is listed in Table 4 of the provided context.","JetMoE-8B","score","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","Table 4: MT-Bench score comparison of various models
| Model | MT-Bench Score |
|-------|----------------|
| GPT-4  | 9.014           |
| GPT-3.5-turbo  | 7.995        |
| Claude-v1  | 7.923         |
| JetMoE-8B-chat  | 6.681         |
| Llama-2-13b-chat  | 6.650         |
| Vicuna-13b-v1.3  | 6.413         |
| Wizardlm-13b  | 6.353         |
| Llama-2-7b-chat  | 6.269         |","The JetMoE-8B model's MT-Bench score is listed in Table 4 of the provided context."
