"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q003","What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?","The context describes the ML.ENERGY Benchmark as a comprehensive benchmark suite for measuring inference energy consumption of generative AI models across multiple tasks and model architectures.","ML.ENERGY Benchmark","is_blank","[""chung2025""]","is_blank","is_blank","The context describes the ML.ENERGY Benchmark as a comprehensive benchmark suite for measuring inference energy consumption of generative AI models across multiple tasks and model architectures."
"q009","What were the net CO2e emissions from training the GShard-600B model?","The context shows a figure depicting the carbon emissions for various ML models, which includes the GShard-600B model. From Figure 4 in the context, the net CO2e emissions for GShard-600B appears to be approximately 0.5-0.6 million kg CO2e during training.","0.55","tCO2e","[""wu2021a""]","is_blank","is_blank","The context shows a figure depicting the carbon emissions for various ML models, which includes the GShard-600B model. From Figure 4 in the context, the net CO2e emissions for GShard-600B appears to be approximately 0.5-0.6 million kg CO2e during training."
"q054","What is the model size in gigabytes (GB) for the LLaMA-33B model?","The context from the chen2024 reference provides the model size for LLaMA-33B as 64.7 GB, specifically in Table 3 of the research paper.","64.7","GB","[""chen2024""]","is_blank","is_blank","The context from the chen2024 reference provides the model size for LLaMA-33B as 64.7 GB, specifically in Table 3 of the research paper."
"q062","What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?","There is no specific information in the provided context about the total electricity consumption of Google Cloud TPU pods worldwide in 2023. The context contains discussions about AI energy consumption and data center electricity usage, but no direct data on Google Cloud TPU electricity consumption.","is_blank","MWh","[""is_blank""]","is_blank","is_blank","There is no specific information in the provided context about the total electricity consumption of Google Cloud TPU pods worldwide in 2023. The context contains discussions about AI energy consumption and data center electricity usage, but no direct data on Google Cloud TPU electricity consumption."
"q075","True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.","The context from the Wu et al. 2021 paper clearly states that hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers, as measured by Power Usage Effectiveness (PUE).","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context from the Wu et al. 2021 paper clearly states that hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers, as measured by Power Usage Effectiveness (PUE)."
"q078","For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?","According to the context from Li et al. (2025b), GPT-3 needs to consume around half a liter of water (500 mL) for every 10-50 medium-length responses, depending on deployment conditions.","1","500 mL bottles","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","According to the context from Li et al. (2025b), GPT-3 needs to consume around half a liter of water (500 mL) for every 10-50 medium-length responses, depending on deployment conditions."
"q091","From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?","According to the context, in the's sample of 60 papers from top AI conferences, 75% of CVPR papers targeted target accuracy, while only 20% of CVPR papers argue for a new efficiency result. Therefore, the difference between percentage of CVPR papers targeting accuracy versus efficiency is  55 percentage points.","55","percent","[""schwartz2019""]","is_blank","is_blank","According to the context, in the's sample of 60 papers from top AI conferences, 75% of CVPR papers targeted target accuracy, while only 20% of CVPR papers argue for a new efficiency result. Therefore, the difference between percentage of CVPR papers targeting accuracy versus efficiency is  55 percentage points."
"q102","True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The context indicates that the AI Act does NOT make energy consumption data publicly available. The document explicitly states that where energy consumption is mandated to be disclosed, this information is restricted to authorities and is not accessible to the general public due to confidentiality clauses.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the AI Act does NOT make energy consumption data publicly available. The document explicitly states that where energy consumption is mandated to be disclosed, this information is restricted to authorities and is not accessible to the general public due to confidentiality clauses."
"q105","What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?","According to the context, the authors' analytical model predicts that for a GPU with 100GB memory capacity, the maximum batch size for fine-tuning the Mixtral model will be 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","According to the context, the authors' analytical model predicts that for a GPU with 100GB memory capacity, the maximum batch size for fine-tuning the Mixtral model will be 28 samples."
"q106","What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context shows that for the LLaMA 7B model, the A100 GPU provides a 2 times increase in inference throughput compared to the V100 GPU. This speedup is observed across words per second, tokens per second, and responses per second, particularly for the smaller 7B model.","2","multiplier","[""samsi2024""]","is_blank","is_blank","The context shows that for the LLaMA 7B model, the A100 GPU provides a 2 times increase in inference throughput compared to the V100 GPU. This speedup is observed across words per second, tokens per second, and responses per second, particularly for the smaller 7B model."
"q124","What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?","According to Table 1 in the document, the U.S. Average water consumption for training GPT-3 is 5.439 million liters, which breaks down to 0.708 million liters on-site and 4.731 million liters off-site.","5.439","liters","[""li2025b""]","is_blank","is_blank","According to Table 1 in the document, the U.S. Average water consumption for training GPT-3 is 5.439 million liters, which breaks down to 0.708 million liters on-site and 4.731 million liters off-site."
"q135","True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.","The authors explicitly propose that sustainability impact assessments (SIAs) should apply to all AI systems, not just high-risk ones. They argue that the carbon footprint of AI models is often unrelated to their risk classification, and therefore SIAs should cover the entire AI landscape.","1","is_blank","[""ebert2024""]","is_blank","is_blank","The authors explicitly propose that sustainability impact assessments (SIAs) should apply to all AI systems, not just high-risk ones. They argue that the carbon footprint of AI models is often unrelated to their risk classification, and therefore SIAs should cover the entire AI landscape."
"q139","As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?","The context clearly states that AWS improved its Water Use Effectiveness (WUE) to 0.18 liters per kilowatt-hour (L/kWh) in 2023, which represents a 5% improvement from 0.19 L/kWh in 2022 and a 28% improvement since 2021.","0.18","L/kWh","[""amazon2023""]","is_blank","is_blank","The context clearly states that AWS improved its Water Use Effectiveness (WUE) to 0.18 liters per kilowatt-hour (L/kWh) in 2023, which represents a 5% improvement from 0.19 L/kWh in 2022 and a 28% improvement since 2021."
"q146","True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.","The context directly supports the statement about local inference reducing network overhead and carbon footprint. Specifically, the paper describes local inference as a method that 'allows models to run directly on user devices' and 'significantly reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers'.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context directly supports the statement about local inference reducing network overhead and carbon footprint. Specifically, the paper describes local inference as a method that 'allows models to run directly on user devices' and 'significantly reduces both network overhead and carbon footprint by minimizing data transmission between clients and remote servers'."
"q153","True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.","The context from multiple sources suggests that tracking runtime is crucial for estimating compute costs and understanding computational expenses in GPU-based training. The Strubell et al. paper and Luccioni et al. paper both emphasize the importance of measuring training time and computational resources.","1","is_blank","[""strubell2019"", ""luccioni2023""]","is_blank","is_blank","The context from multiple sources suggests that tracking runtime is crucial for estimating compute costs and understanding computational expenses in GPU-based training. The Strubell et al. paper and Luccioni et al. paper both emphasize the importance of measuring training time and computational resources."
"q158","For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?","According to the context from the Chen et al. 2024 study, the LLaMA-65B model achieved a maximum performance improvement of 13.2% through automated resource utilization overlapping. This improvement was particularly notable for larger batch sizes, which produce larger key-value (KV) tensors and result in greater latency reduction.","13.2","percent","[""chen2024""]","is_blank","is_blank","According to the context from the Chen et al. 2024 study, the LLaMA-65B model achieved a maximum performance improvement of 13.2% through automated resource utilization overlapping. This improvement was particularly notable for larger batch sizes, which produce larger key-value (KV) tensors and result in greater latency reduction."
"q164","How much does an elephant weigh?","The context documents do not contain specific information about the weight of an elephant. While the Amazon sustainability report mentions Asian elephants in the Western Ghats region of India, it does not provide details about their weight.","is_blank","lbs","[""amazon2023""]","is_blank","is_blank","The context documents do not contain specific information about the weight of an elephant. While the Amazon sustainability report mentions Asian elephants in the Western Ghats region of India, it does not provide details about their weight."
"q166","Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?","Based on the energy consumption data in the context, DeepSeek-R1 (DS) has the highest energy consumption at 29.075 Wh for long prompts, which is significantly higher than other models mentioned. Among the models specifically cited in the question, DeepSeek-R1 appears to consume the most energy.","DeepSeek-R1","is_blank","[""jegham2025""]","is_blank","is_blank","Based on the energy consumption data in the context, DeepSeek-R1 (DS) has the highest energy consumption at 29.075 Wh for long prompts, which is significantly higher than other models mentioned. Among the models specifically cited in the question, DeepSeek-R1 appears to consume the most energy."
"q170","How many days of CO₂ emissions from an average American life are equivalent to training BERT base?","The context from Strubell et al. (2019) provides a comparison of CO₂ emissions for different activities, showing that training a BERT model with neural architecture search produces approximately 626,155 pounds of CO₂ emissions. This can be compared to the average annual CO₂ emissions for an average American life of 36,156 pounds.","17.3","days","[""strubell2019""]","is_blank","is_blank","The context from Strubell et al. (2019) provides a comparison of CO₂ emissions for different activities, showing that training a BERT model with neural architecture search produces approximately 626,155 pounds of CO₂ emissions. This can be compared to the average annual CO₂ emissions for an average American life of 36,156 pounds."
"q200","True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.","There is no clear evidence in the provided context that directly addresses the claim about the Transformer architecture outperforming the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task. The context mentions the Evolved Transformer in references but does not provide comparative performance details for the specified task.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","There is no clear evidence in the provided context that directly addresses the claim about the Transformer architecture outperforming the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task. The context mentions the Evolved Transformer in references but does not provide comparative performance details for the specified task."
"q202","What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?","The context does not specify a dataset of exactly 5,842 labeled entries used to test energy-efficient large language models in the financial domain. The documents discuss energy consumption of various models and inference techniques, but do not mention a specific financial domain dataset of that precise size.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not specify a dataset of exactly 5,842 labeled entries used to test energy-efficient large language models in the financial domain. The documents discuss energy consumption of various models and inference techniques, but do not mention a specific financial domain dataset of that precise size."
"q203","True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.","The paper provides evidence that eight T4 spot instances can be more cost-efficient than a DGX-2 node for distributed training, particularly for computer vision (CV) tasks. The authors specifically demonstrated that eight T4 instances can be 58% cheaper than a DGX-2 node while being only 37% slower for CV models.","1","is_blank","[""erben2023""]","is_blank","is_blank","The paper provides evidence that eight T4 spot instances can be more cost-efficient than a DGX-2 node for distributed training, particularly for computer vision (CV) tasks. The authors specifically demonstrated that eight T4 instances can be 58% cheaper than a DGX-2 node while being only 37% slower for CV models."
"q207","True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.","According to the context from the Luccioni et al. paper, the 2023 US Executive Order on AI explicitly did not mention AI's greenhouse gas emissions or energy usage. This is directly stated in the text, which highlights the lack of sustainability considerations in the order.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","According to the context from the Luccioni et al. paper, the 2023 US Executive Order on AI explicitly did not mention AI's greenhouse gas emissions or energy usage. This is directly stated in the text, which highlights the lack of sustainability considerations in the order."
"q211","True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","According to the context from the German 2023 Energy Efficiency Act section, data centers are required to run on 50% renewable energy, increasing to 100% by January 1, 2027.","1","is_blank","[""ebert2024""]","is_blank","is_blank","According to the context from the German 2023 Energy Efficiency Act section, data centers are required to run on 50% renewable energy, increasing to 100% by January 1, 2027."
"q215","Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?","According to the paper's analysis of papers from top AI conferences, out of the ACL (Association for Computational Linguistics) papers sampled, a small portion (specifically 10%) argued for a new efficiency result, while 90% of ACL papers targeted accuracy.","6","papers","[""schwartz2019""]","is_blank","is_blank","According to the paper's analysis of papers from top AI conferences, out of the ACL (Association for Computational Linguistics) papers sampled, a small portion (specifically 10%) argued for a new efficiency result, while 90% of ACL papers targeted accuracy."
"q221","According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?","Multiple sources consistently indicate that inference accounts for 80-90% of a model's total lifecycle energy use. AWS states inference makes up 80-90% of total ML cloud computing demand, while different company studies show varying but significant proportions of energy consumed during inference.","90","percent","[""jegham2025"", ""luccioni2024"", ""chung2025""]","is_blank","is_blank","Multiple sources consistently indicate that inference accounts for 80-90% of a model's total lifecycle energy use. AWS states inference makes up 80-90% of total ML cloud computing demand, while different company studies show varying but significant proportions of energy consumed during inference."
"q230","True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.","The paper indicates that the AI Act does not mandate disclosure of energy consumption during the inference phase. It highlights this as a significant gap in the Act's transparency requirements, noting that energy consumption during inference can often exceed the development phase energy usage.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The paper indicates that the AI Act does not mandate disclosure of energy consumption during the inference phase. It highlights this as a significant gap in the Act's transparency requirements, noting that energy consumption during inference can often exceed the development phase energy usage."
"q231","True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.","According to the document, the current AI Act does not currently mandate the disclosure of energy consumption during the inference phase of AI models. The authors propose that this is a significant omission and recommend including energy consumption from both single and overall inferences as a reporting category.","0","is_blank","[""ebert2024""]","is_blank","is_blank","According to the document, the current AI Act does not currently mandate the disclosure of energy consumption during the inference phase of AI models. The authors propose that this is a significant omission and recommend including energy consumption from both single and overall inferences as a reporting category."
"q246","True or False: New AI data centers often rely on air cooling due to high server power densities.","The context directly contradicts the statement. Specifically, the research document indicates that new AI data centers often rely on liquid cooling, not air cooling, due to high server power densities.","0","is_blank","[""li2025b""]","is_blank","is_blank","The context directly contradicts the statement. Specifically, the research document indicates that new AI data centers often rely on liquid cooling, not air cooling, due to high server power densities."
"q253","By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?","According to the context from Wu et al. (2021), platform-level caching improved power efficiency by a factor of 6.7x for the cross-lingual Transformer language model. This was achieved by pre-computing and caching frequently accessed embeddings for language translation tasks.","6.7","multiplier","[""wu2021a""]","is_blank","is_blank","According to the context from Wu et al. (2021), platform-level caching improved power efficiency by a factor of 6.7x for the cross-lingual Transformer language model. This was achieved by pre-computing and caching frequently accessed embeddings for language translation tasks."
"q262","What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?","According to the Strubell et al. (2019) paper in the context, the BERT base model trained on 64 Tesla V100 GPUs for 79.2 hours resulted in 1,438 lbs of CO2 emissions.","1438","lbs","[""strubell2019""]","is_blank","is_blank","According to the Strubell et al. (2019) paper in the context, the BERT base model trained on 64 Tesla V100 GPUs for 79.2 hours resulted in 1,438 lbs of CO2 emissions."
"q263","According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?","Multiple sources consistently report that ML inference accounts for 80-90% of total compute demand. This figure is cited by AWS, Chung et al.'s paper, and Luccioni et al.'s research, with slight variations from different companies.","80-90","percent","[""chung2025"", ""luccioni2024"", ""fernandez2025""]","is_blank","is_blank","Multiple sources consistently report that ML inference accounts for 80-90% of total compute demand. This figure is cited by AWS, Chung et al.'s paper, and Luccioni et al.'s research, with slight variations from different companies."
"q272","How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?","According to the context from the Dodge et al. (2022) paper, training a 6.1 billion parameter language model would consume approximately 103,500 kWh of electricity. This can be converted to household-years of electricity consumption.","9","household-years","[""dodge2022""]","is_blank","is_blank","According to the context from the Dodge et al. (2022) paper, training a 6.1 billion parameter language model would consume approximately 103,500 kWh of electricity. This can be converted to household-years of electricity consumption."
"q278","True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.","The context directly states that for NLP experiments, the external egress cost for Google Cloud is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). The authors emphasize that for some cloud providers like Google Cloud and Azure, egress costs can dominate the total expense of distributed training.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context directly states that for NLP experiments, the external egress cost for Google Cloud is $4.329/h, which is more than 90% of the total cost per VM ($4.804/h). The authors emphasize that for some cloud providers like Google Cloud and Azure, egress costs can dominate the total expense of distributed training."
"q280","Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.","The context specifies that JetMoE-8B was trained using 30,000 H100 GPU hours on a cluster with 12 nodes and 96 H100 GPUs. To estimate the wall-clock time, we need to divide total GPU hours by the number of GPUs used.","3.125","days","[""shen2024""]","is_blank","is_blank","The context specifies that JetMoE-8B was trained using 30,000 H100 GPU hours on a cluster with 12 nodes and 96 H100 GPUs. To estimate the wall-clock time, we need to divide total GPU hours by the number of GPUs used."
"q282","What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?","The context clearly defines water consumption as 'water withdrawal minus water discharge', which means the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment.","Water consumption","is_blank","[""li2025b""]","is_blank","is_blank","The context clearly defines water consumption as 'water withdrawal minus water discharge', which means the amount of water evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment."
"q296","What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?","The context indicates that for the LLaMA-65B model, the observed range of inference energy per second was from 300 Watts to 1 Kilowatt, varying across different GPU shard configurations from 8 to 32 GPUs.","300-1000","W","[""samsi2024""]","is_blank","is_blank","The context indicates that for the LLaMA-65B model, the observed range of inference energy per second was from 300 Watts to 1 Kilowatt, varying across different GPU shard configurations from 8 to 32 GPUs."
"q297","When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?","According to the context, the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, with only a minor accuracy reduction of 0.07 points.","7","multiplier","[""zschache2025""]","is_blank","is_blank","According to the context, the Qwen 2.5 7B model consumes seven times less energy than the Qwen 2.5 72B model, with only a minor accuracy reduction of 0.07 points."
"q304","By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?","","","percent","[]","is_blank","is_blank",""
"q306","How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?","According to the abstract of the paper, the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration.","40","models","[""chung2025""]","is_blank","is_blank","According to the abstract of the paper, the ML.ENERGY Benchmark includes energy measurements of 40 widely used model architectures across 6 different tasks in its early 2025 iteration."
"q316","In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?","The context clearly states that training a Llama-3.1 scale model in Iowa would result in a total health cost of $2.5 million, which is significantly higher compared to other locations like Oregon's $0.23 million due to factors such as wind direction and pollutant emission rates.","2.5","USD","[""han2024""]","is_blank","is_blank","The context clearly states that training a Llama-3.1 scale model in Iowa would result in a total health cost of $2.5 million, which is significantly higher compared to other locations like Oregon's $0.23 million due to factors such as wind direction and pollutant emission rates."
