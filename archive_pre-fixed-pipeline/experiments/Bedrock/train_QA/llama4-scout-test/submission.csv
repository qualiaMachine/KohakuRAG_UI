"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The total energy consumption of the US data centers increased by about 4% from 2010-2014.","4","percent","[""wu2021b""]","is_blank","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13,900","cars","[""amazon2023""]","is_blank","is_blank","The Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context clearly states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24.","24","data centers","[""amazon2023""]","is_blank","is_blank","The context clearly states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context provides information on the estimated embodied carbon emissions of GPUs. Morrison2025 mentions that the embodied carbon emissions of an H100 GPU are estimated to be 463 kg of CO2eq. The authors also assume a 4-year lifespan for their GPUs, leading to an embodied emissions of 0.013 kg of CO2eq per GPU hour.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","The context provides information on the estimated embodied carbon emissions of GPUs. Morrison2025 mentions that the embodied carbon emissions of an H100 GPU are estimated to be 463 kg of CO2eq. The authors also assume a 4-year lifespan for their GPUs, leading to an embodied emissions of 0.013 kg of CO2eq per GPU hour."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The provided context does not clearly state the estimated amortized training cost of GPT-4 or the total training budget for FLM-101B in a directly comparable format. However, GPT-4's amortized hardware and energy cost is mentioned as $40M, and FLM-101B's total cost is computed as 52.76 zettaFLOPs.","is_blank","ratio","[""is_blank""]","is_blank","is_blank","The provided context does not clearly state the estimated amortized training cost of GPT-4 or the total training budget for FLM-101B in a directly comparable format. However, GPT-4's amortized hardware and energy cost is mentioned as $40M, and FLM-101B's total cost is computed as 52.76 zettaFLOPs."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context provides an estimate of CO2e emissions for air travel between New York (NY) and San Francisco (SF). According to the document [ref_id=strubell2019], the CO2e for a 1 passenger, NY ↔ SF round trip is 1984 lbs.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","The context provides an estimate of CO2e emissions for air travel between New York (NY) and San Francisco (SF). According to the document [ref_id=strubell2019], the CO2e for a 1 passenger, NY ↔ SF round trip is 1984 lbs."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard is 43.94. This score is mentioned multiple times in the provided context snippets, specifically in Table 4 and various sections discussing the performance of FLM-101B.","43.94","score","[""li2025a""]","is_blank","is_blank","The final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard is 43.94. This score is mentioned multiple times in the provided context snippets, specifically in Table 4 and various sections discussing the performance of FLM-101B."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context clearly states that there was a more than 6,750 fold improvement in processor clock speed between the Intel 4004 (1971) and typical 2021 microprocessors.","6750","fold","[""wu2021b""]","is_blank","is_blank","The context clearly states that there was a more than 6,750 fold improvement in processor clock speed between the Intel 4004 (1971) and typical 2021 microprocessors."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context provides an estimate of the total training time of GPT-3 as over 3.5 million hours, which is equivalent to 14.8 days with 10,000 GPUs. The computational power of 10,000 V100 GPUs at 24.6 TeraFLOPS/sec is mentioned, but the exact training time based on this specific configuration is not directly provided.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context provides an estimate of the total training time of GPT-3 as over 3.5 million hours, which is equivalent to 14.8 days with 10,000 GPUs. The computational power of 10,000 V100 GPUs at 24.6 TeraFLOPS/sec is mentioned, but the exact training time based on this specific configuration is not directly provided."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The estimated GPU Power Usage in kWh for Llama 3.2 1B model at an 8 request/s frequency with 2400 prompts from ShareGPT is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The estimated GPU Power Usage in kWh for Llama 3.2 1B model at an 8 request/s frequency with 2400 prompts from ShareGPT is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context provides information about the total permitted annual emission limits for data center backup generators in Northern Virginia. It states that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx.","13000","tons","[""han2024""]","is_blank","is_blank","The context provides information about the total permitted annual emission limits for data center backup generators in Northern Virginia. It states that the total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context clearly states that using the growth strategy for the 101B model saved 72% of the time compared to training a 101B model from scratch.","72","percent","[""li2025a""]","is_blank","is_blank","The context clearly states that using the growth strategy for the 101B model saved 72% of the time compared to training a 101B model from scratch."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides information on the public health impacts of U.S. data centers, particularly focusing on scope-2 pollutants. It mentions that the growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028. However, it does not provide a specific number for premature deaths caused by scope-2 pollutants from U.S. data centers alone in 2030.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context provides information on the public health impacts of U.S. data centers, particularly focusing on scope-2 pollutants. It mentions that the growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028. However, it does not provide a specific number for premature deaths caused by scope-2 pollutants from U.S. data centers alone in 2030."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately 60 days. This is derived from the fact that their training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, which was 8 days.","60","days","[""dodge2022""]","is_blank","is_blank","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately 60 days. This is derived from the fact that their training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, which was 8 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","","","is_blank","[]","is_blank","is_blank",""
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","is_blank","is_blank","The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","","","percent","[]","is_blank","is_blank",""
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The energy consumption for pre-training the BLOOM model is mentioned in the context as 51,686 kWh.","51686","MWh","[""luccioni2024"", ""ebert2024""]","is_blank","is_blank","The energy consumption for pre-training the BLOOM model is mentioned in the context as 51,686 kWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","","","percent","[]","is_blank","is_blank",""
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context provides information about the JetMoE-8B architecture, specifically that it is a Mixture-of-Experts (MoE) model with 8 experts and a top-k of 2 for every layer.","8","experts","[""shen2024""]","is_blank","is_blank","The context provides information about the JetMoE-8B architecture, specifically that it is a Mixture-of-Experts (MoE) model with 8 experts and a top-k of 2 for every layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides information on the execution time breakdown for fine-tuning Mixtral and BlackMamba models on an NVIDIA A40 GPU with 48GB memory. However, it does not provide a direct value for the total execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with the specified GPU.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context provides information on the execution time breakdown for fine-tuning Mixtral and BlackMamba models on an NVIDIA A40 GPU with 48GB memory. However, it does not provide a direct value for the total execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with the specified GPU."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The cost of training the English portion of the FLM-101B model is explicitly stated as 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The cost of training the English portion of the FLM-101B model is explicitly stated as 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context does not clearly mention the specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification. However, it does mention the use of an 11th Gen Intel(R) Core(TM) i7-1165G7 processor in one of the references.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly mention the specific hardware processor used for the experimental setup of energy-efficient local inference in financial sentiment classification. However, it does mention the use of an 11th Gen Intel(R) Core(TM) i7-1165G7 processor in one of the references."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context does not provide a specific number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context does not provide a specific number of different machine learning models sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","","","multiplier","[]","is_blank","is_blank",""
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","","","multiplier","[]","is_blank","is_blank",""
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model is provided in the context. The model was trained for 8 days on 256 NVIDIA A100s, consuming 13.8 MWh of energy. It was not trained to completion but only for 13% of the time it would take to train to completion, which is estimated to be 60 days. Therefore, the total energy consumption for a full training run is estimated to be approximately 103.5 MWh or 103,500 kWh.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The estimated total energy consumption for a full training run of a 6.1 billion parameter transformer model is provided in the context. The model was trained for 8 days on 256 NVIDIA A100s, consuming 13.8 MWh of energy. It was not trained to completion but only for 13% of the time it would take to train to completion, which is estimated to be 60 days. Therefore, the total energy consumption for a full training run is estimated to be approximately 103.5 MWh or 103,500 kWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of the 2025 paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate' is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons' Paradox. This economic principle states that as the efficiency of a resource (like coal or AI computations) increases, it can paradoxically lead to an increase in its consumption, as cost savings spur higher overall demand.","is_blank","is_blank","[""luccioni2025a""]","is_blank","is_blank","The central argument of the 2025 paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate' is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons' Paradox. This economic principle states that as the efficiency of a resource (like coal or AI computations) increases, it can paradoxically lead to an increase in its consumption, as cost savings spur higher overall demand."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal by 2027.","4.2 - 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal by 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context clearly states that Red AI is on the rise despite the well-known diminishing returns of increased cost. This indicates that the statement 'Red AI is on the decline' is false.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context clearly states that Red AI is on the rise despite the well-known diminishing returns of increased cost. This indicates that the statement 'Red AI is on the decline' is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context clearly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days.","21.54","days","[""li2025a""]","is_blank","is_blank","The context clearly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context indicates that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving significant room for utilization and efficiency improvements. This information directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context indicates that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving significant room for utilization and efficiency improvements. This information directly contradicts the statement that a majority of model experimentation workflows utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The training of GPT-3 is estimated to have consumed 1287 MWh of electricity. This information is consistently mentioned in multiple references within the provided context.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The training of GPT-3 is estimated to have consumed 1287 MWh of electricity. This information is consistently mentioned in multiple references within the provided context."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the AI Energy Score project, which aims to establish a unified approach for comparing the inference efficiency of AI models.","is_blank","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context mentions the AI Energy Score project, which aims to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides information on the execution time breakdown for Mixtral and BlackMamba models, including a kernel-level analysis within the MoE layer. However, it does not provide specific execution times for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context provides information on the execution time breakdown for Mixtral and BlackMamba models, including a kernel-level analysis within the MoE layer. However, it does not provide specific execution times for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context provides the hyperparameters for the JetMoE-8B model, including the top-k value for expert selection. According to the table in the context, the top-k value is set to 2 for every layer.","2","experts","[""shen2024""]","is_blank","is_blank","The context provides the hyperparameters for the JetMoE-8B model, including the top-k value for expert selection. According to the table in the context, the top-k value is set to 2 for every layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context supports the answer by providing information about the increase in computational cost for deep learning models. According to [ref_id=schwartz2019], the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018. This trend is also mentioned in [ref_id=wu2021a].","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context supports the answer by providing information about the increase in computational cost for deep learning models. According to [ref_id=schwartz2019], the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018. This trend is also mentioned in [ref_id=wu2021a]."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context clearly states that the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction.","6.4","percent","[""wu2021b""]","is_blank","is_blank","The context clearly states that the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources. This achievement is mentioned multiple times in the provided context snippets.","22","data centers","[""amazon2023"", ""amazon2023""]","is_blank","is_blank","The context states that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources. This achievement is mentioned multiple times in the provided context snippets."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. The context mentions that the field of AI has been in existence for sixty years as of 2016.","69","years","[""stone2022""]","is_blank","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. The context mentions that the field of AI has been in existence for sixty years as of 2016."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The well-known 'five cars' carbon footprint estimate, originating from a 2019 study, is based on the energy required for neural architecture search (NAS), a specific and infrequently performed AI process.","is_blank","is_blank","[""luccioni2025c""]","is_blank","is_blank","The well-known 'five cars' carbon footprint estimate, originating from a 2019 study, is based on the energy required for neural architecture search (NAS), a specific and infrequently performed AI process."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context provides information about the energy consumption of Llama 3.1 8B model when targeting an average Time Per Output Token (TPOT) of 100 ms compared to minimizing latency. According to the text, targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","is_blank","The context provides information about the energy consumption of Llama 3.1 8B model when targeting an average Time Per Output Token (TPOT) of 100 ms compared to minimizing latency. According to the text, targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III of the provided context.","8","samples","[""xia2024""]","is_blank","is_blank","The maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is found in Table III of the provided context."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context provides information about Amazon's energy storage capacity as of 2023.","1.3","GW","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's energy storage capacity as of 2023."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide specific information about the percentage of AI inference workloads in Asia powered by coal in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the percentage of AI inference workloads in Asia powered by coal in 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context does not provide specific information about the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023. It mentions the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but it does not specify the PUE for AI-dedicated data centers.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023. It mentions the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but it does not specify the PUE for AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context clearly states that JetMoE-8B has 8B parameters and only activates 2B of them for each input token. This information is mentioned multiple times in the provided context snippets.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B has 8B parameters and only activates 2B of them for each input token. This information is mentioned multiple times in the provided context snippets."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The GHG emissions associated with pre-training the Llama 7B model are reported in the provided context.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The GHG emissions associated with pre-training the Llama 7B model are reported in the provided context."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides information on the number of Amazon electric delivery vans added in 2022 and 2023. In 2022, there were 2,600 electric delivery vans, and in 2023, the number increased to 11,800 in the U.S. alone. Additionally, Europe had 1,220 electric delivery vans in 2022 and 3,000+ in 2023, and India had 3,800 in 2022 and 7,200+ in 2023.","14020","electric delivery vans","[""amazon2023""]","is_blank","is_blank","The context provides information on the number of Amazon electric delivery vans added in 2022 and 2023. In 2022, there were 2,600 electric delivery vans, and in 2023, the number increased to 11,800 in the U.S. alone. Additionally, Europe had 1,220 electric delivery vans in 2022 and 3,000+ in 2023, and India had 3,800 in 2022 and 7,200+ in 2023."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context clearly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The energy consumption of the o3 model for a long prompt is provided in Table 4 of the context. The table lists the energy consumption for the o3 model as 12.222 Wh for long prompts (10k input-1.5k output).","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The energy consumption of the o3 model for a long prompt is provided in Table 4 of the context. The table lists the energy consumption for the o3 model as 12.222 Wh for long prompts (10k input-1.5k output)."
"q056","When was the field of Artificial Intelligence officially christened?","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","","","WUE","[]","is_blank","is_blank",""
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context clearly states that approximately 770 million people worldwide lack access to a stable supply of electricity, which directly supports the statement in question.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context clearly states that approximately 770 million people worldwide lack access to a stable supply of electricity, which directly supports the statement in question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context provides information on the energy consumption of LLaMA-65B at different generation lengths. Specifically, it mentions that for a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token.","3.5","joules per token","[""samsi2024""]","is_blank","is_blank","The context provides information on the energy consumption of LLaMA-65B at different generation lengths. Specifically, it mentions that for a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The overall model size of Facebook's second representative recommendation model (RM2) was reduced by 15% after being quantized from 32-bit to 16-bit numerical representation.","15","percent","[""wu2021a""]","is_blank","is_blank","The overall model size of Facebook's second representative recommendation model (RM2) was reduced by 15% after being quantized from 32-bit to 16-bit numerical representation."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context discusses a claim that AI can reduce global GHG emissions by 5-10%, which is traced back to a 2021 Boston Consulting Group (BCG) report and a 2023 BCG report commissioned by Google. However, the reasoning behind this estimate is unclear, and the underlying calculations are not detailed. The context also notes that applying observations made from individual projects to the entire planet's GHG emissions lacks scientific grounding.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context discusses a claim that AI can reduce global GHG emissions by 5-10%, which is traced back to a 2021 Boston Consulting Group (BCG) report and a 2023 BCG report commissioned by Google. However, the reasoning behind this estimate is unclear, and the underlying calculations are not detailed. The context also notes that applying observations made from individual projects to the entire planet's GHG emissions lacks scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context does not clearly support the statement that sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context does not clearly support the statement that sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The estimated cost of training AI2's Grover on 256 TPU chips for two weeks is $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The estimated cost of training AI2's Grover on 256 TPU chips for two weeks is $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1 on an NVIDIA A40-48GB GPU.","53","percent","[""xia2024""]","is_blank","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1 on an NVIDIA A40-48GB GPU."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The daily energy consumption of a large-scale service like Google Translate can be estimated using the average energy for a comparable multi-purpose model. According to the context, Google Translate performs 1 billion queries per day. The energy consumption for a multi-purpose model like Flan-T5-xxl is 0.083 kWh per 1,000 queries.","83.0","MWh","[""luccioni2024""]","is_blank","is_blank","The daily energy consumption of a large-scale service like Google Translate can be estimated using the average energy for a comparable multi-purpose model. According to the context, Google Translate performs 1 billion queries per day. The energy consumption for a multi-purpose model like Flan-T5-xxl is 0.083 kWh per 1,000 queries."
"q067","What was the average global data center PUE in 2023?","The average global data center PUE in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The average global data center PUE in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide specific information on the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","","","percent","[]","is_blank","is_blank",""
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context clearly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI consisted of seventeen members.","17","people","[""stone2022""]","is_blank","is_blank","The context clearly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI consisted of seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","is_blank","is_blank","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context indicates that models with more parameters do not always consume more energy during inference. For example, Phi-3 Small (7B) can consume less energy than Phi-3 Mini (3.8B) as batch size grows due to differences in attention mechanisms and memory consumption.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context indicates that models with more parameters do not always consume more energy during inference. For example, Phi-3 Small (7B) can consume less energy than Phi-3 Mini (3.8B) as batch size grows due to differences in attention mechanisms and memory consumption."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel from the 100 Year Study on AI found no cause for concern that AI is an imminent threat to humankind. The panel emphasized that AI systems are highly tailored to particular tasks and do not have self-sustaining long-term goals and intent. They also noted that while AI systems are not likely to autonomously choose to inflict harm on people, it will be possible for people to use AI-based systems for harmful as well as helpful purposes.","0","is_blank","[""stone2022""]","is_blank","is_blank","The Study Panel from the 100 Year Study on AI found no cause for concern that AI is an imminent threat to humankind. The panel emphasized that AI systems are highly tailored to particular tasks and do not have self-sustaining long-term goals and intent. They also noted that while AI systems are not likely to autonomously choose to inflict harm on people, it will be possible for people to use AI-based systems for harmful as well as helpful purposes."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide specific information on the metric tons of CO2 emitted by OpenAI's API requests in January 2024. The documents discuss the environmental impact of various AI models, including OLMo and Llama, but do not mention OpenAI's API requests for the specified time period.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the metric tons of CO2 emitted by OpenAI's API requests in January 2024. The documents discuss the environmental impact of various AI models, including OLMo and Llama, but do not mention OpenAI's API requests for the specified time period."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context provides information on the GHG emissions from the pre-training process for Meta's Llama 3 family of models, which is 11,390 tons CO2e. This value is compared to the 'five cars' estimate, with Meta's emissions being over 40x the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context provides information on the GHG emissions from the pre-training process for Meta's Llama 3 family of models, which is 11,390 tons CO2e. This value is compared to the 'five cars' estimate, with Meta's emissions being over 40x the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that the AlphaGo program, developed by Google Deepmind, defeated the human Go champion in a five-game match. This information is mentioned multiple times across different references.","1","is_blank","[""stone2022"", ""schwartz2019""]","is_blank","is_blank","The context clearly states that the AlphaGo program, developed by Google Deepmind, defeated the human Go champion in a five-game match. This information is mentioned multiple times across different references."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time.","is_blank","is_blank","[""fernandez2025""]","is_blank","is_blank","The context mentions that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context provides information about the JetMoE-8B model and its training process, including the time required for the entire alignment process.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context provides information about the JetMoE-8B model and its training process, including the time required for the entire alignment process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The Max-Performance policy selected the g6e.xlarge instance at a cost of $2.699, while InferSave selected the g4dn.xlarge instance at a cost of $2.13 for the 100 TPS SLO. The cost difference is $0.569. To find the percentage by which the Max-Performance policy's instance is more expensive: (($2.699 - $2.13) / $2.13) * 100 = 26.7%.","26.7","percent","[""kim2025""]","is_blank","is_blank","The Max-Performance policy selected the g6e.xlarge instance at a cost of $2.699, while InferSave selected the g4dn.xlarge instance at a cost of $2.13 for the 100 TPS SLO. The cost difference is $0.569. To find the percentage by which the Max-Performance policy's instance is more expensive: (($2.699 - $2.13) / $2.13) * 100 = 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides information on GPU energy usage for 1,000 queries, which ranges from 0.06 Wh to over 3,426 Wh, depending on the model size, architecture, and task complexity.","is_blank","Wh","[""luccioni2025c""]","is_blank","is_blank","The context provides information on GPU energy usage for 1,000 queries, which ranges from 0.06 Wh to over 3,426 Wh, depending on the model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context suggests that a universal, one-size-fits-all approach to AI ethics and sustainability cannot be developed. It emphasizes the need for a nuanced and context-specific approach to AI ethics, one that embraces varied ethical dimensions presented by AI, including its environmental implications.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context suggests that a universal, one-size-fits-all approach to AI ethics and sustainability cannot be developed. It emphasizes the need for a nuanced and context-specific approach to AI ethics, one that embraces varied ethical dimensions presented by AI, including its environmental implications."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context does not provide specific information about the gross carbon intensity of energy according to the U.S. average mix in 2021.","is_blank","kg of CO2e/KWh","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the gross carbon intensity of energy according to the U.S. average mix in 2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The decentralized PyTorch-based framework used to enable distributed spot instance training across clouds and continents is Hivemind.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The decentralized PyTorch-based framework used to enable distributed spot instance training across clouds and continents is Hivemind."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'.","is_blank","is_blank","[""luccioni2025b""]","is_blank","is_blank","The proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system is 'social transparency'."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context mentions that for the emotion classification task, the linear model with sentence embeddings is among the top-performing models. However, it does not specify which model achieved the highest accuracy in classification experiments on German public administration texts.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions that for the emotion classification task, the linear model with sentence embeddings is among the top-performing models. However, it does not specify which model achieved the highest accuracy in classification experiments on German public administration texts."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions the development of Lamina, an LLM inference system that incorporates model-attention disaggregation. The 2025 Chen et al. paper is referenced, which introduces model-attention disaggregation and describes Lamina as a system that uses this approach in a distributed heterogeneous cluster.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The context mentions the development of Lamina, an LLM inference system that incorporates model-attention disaggregation. The 2025 Chen et al. paper is referenced, which introduces model-attention disaggregation and describes Lamina as a system that uses this approach in a distributed heterogeneous cluster."
"q093","How many parameters does the largest T5 model have?","The context mentions that the OLMo series of transformer language models range in size from 20 million to 13 billion active parameters. Specifically, it states that the largest model has 13 billion active parameters.","13 billion","parameters","[""morrison2025""]","is_blank","is_blank","The context mentions that the OLMo series of transformer language models range in size from 20 million to 13 billion active parameters. Specifically, it states that the largest model has 13 billion active parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The total number of parameters in the JetMoE-8B model is 8B, as stated in Table 1 of the provided context.","8B","parameters","[""shen2024""]","is_blank","is_blank","The total number of parameters in the JetMoE-8B model is 8B, as stated in Table 1 of the provided context."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context clearly states that Google's data center water consumption increased by approximately 20% from 2021 to 2022.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The context clearly states that Google's data center water consumption increased by approximately 20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The emissions metric defined as 'CO₂ emissions per unit of electricity consumed' is referred to as Carbon Intensity. It is measured in gCO2/kWh.","is_blank","is_blank","[""khan2025""]","is_blank","is_blank","The emissions metric defined as 'CO₂ emissions per unit of electricity consumed' is referred to as Carbon Intensity. It is measured in gCO2/kWh."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The final growth stage of FLM-101B training achieved a FLOPs utilization percentage of 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","The final growth stage of FLM-101B training achieved a FLOPs utilization percentage of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The estimated amortized training costs for OpenAI's GPT-4 model are $40 million, which includes hardware, energy, and other expenses. This cost is mentioned in multiple references, including cottier2024. The amortized cost is distinguished from the acquisition cost, which is estimated to be $800 million.","40","USD","[""cottier2024""]","is_blank","is_blank","The estimated amortized training costs for OpenAI's GPT-4 model are $40 million, which includes hardware, energy, and other expenses. This cost is mentioned in multiple references, including cottier2024. The amortized cost is distinguished from the acquisition cost, which is estimated to be $800 million."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context provides specific information on the reduction of operational carbon footprint for a Transformer-based universal translation model through full-stack optimization. According to the context, optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context provides specific information on the reduction of operational carbon footprint for a Transformer-based universal translation model through full-stack optimization. According to the context, optimization across the axes of algorithms, platforms, infrastructures, and hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context provides information on the performance drop for NLP when training is spread across four continents instead of remaining local. According to the text, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","0.59","multiplier","[""erben2023""]","is_blank","is_blank","The context provides information on the performance drop for NLP when training is spread across four continents instead of remaining local. According to the text, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8)."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context clearly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3.5B","liters","[""amazon2023""]","is_blank","is_blank","The context clearly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context reveals that using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce the energy consumption of LLMs. The study shows that custom tags can significantly decrease energy consumption across these prompt engineering techniques.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context reveals that using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce the energy consumption of LLMs. The study shows that custom tags can significantly decrease energy consumption across these prompt engineering techniques."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million units than in 2023. This information is reported in a 2025 paper.","3.7 million","GPUs","[""luccioni2025a""]","is_blank","is_blank","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million units than in 2023. This information is reported in a 2025 paper."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context provides a clear percentage of the total amortized hardware and energy cost attributed to AI accelerator chips. According to the text, on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","is_blank","The context provides a clear percentage of the total amortized hardware and energy cost attributed to AI accelerator chips. According to the text, on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The Power Usage Effectiveness (PUE) for Facebook's data centers is about 1.10, indicating that Facebook's data centers are about 40% more efficient than small-scale, typical data centers.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","The Power Usage Effectiveness (PUE) for Facebook's data centers is about 1.10, indicating that Facebook's data centers are about 40% more efficient than small-scale, typical data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is called ETAIROS, which stands for Ethical AI for the Governance of the Society.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is called ETAIROS, which stands for Ethical AI for the Governance of the Society."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context provides information on the estimated training costs for Google's Gemini Ultra. According to the text, the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","30M","USD","[""cottier2024""]","is_blank","is_blank","The context provides information on the estimated training costs for Google's Gemini Ultra. According to the text, the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems. These provisions should consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8. However, the Act lacks sufficient emphasis on environmental factors and does not explicitly require the inclusion of environmental risks in risk assessments.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk and providers of HRAI systems. These provisions should consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8. However, the Act lacks sufficient emphasis on environmental factors and does not explicitly require the inclusion of environmental risks in risk assessments."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The EPA's recently tightened primary standard for the annual average limit of PM2.5 is 9µg/m³. This information is clearly stated in multiple references within the context.","9","µg/m³","[""han2024""]","is_blank","is_blank","The EPA's recently tightened primary standard for the annual average limit of PM2.5 is 9µg/m³. This information is clearly stated in multiple references within the context."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment found that 115 physical print books produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","is_blank","is_blank","A life cycle assessment found that 115 physical print books produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context clearly states that the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities could be 200 times more than that in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","is_blank","The context clearly states that the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities could be 200 times more than that in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is provided in the context. According to Table B1 in the appendix of the document, the energy consumed by DS Llama 70B for inference on the FKTG dataset is 702.06 Wh for a single run and 1899.86 Wh for a double run.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is provided in the context. According to Table B1 in the appendix of the document, the energy consumed by DS Llama 70B for inference on the FKTG dataset is 702.06 Wh for a single run and 1899.86 Wh for a double run."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not provide specific information about the 2022 paper by Dodge et al. or the total number of parameters in the large language model they analyzed.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the 2022 paper by Dodge et al. or the total number of parameters in the large language model they analyzed."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described as technological progress improving efficiency, which then results in increased usage and overall resource consumption, is known as Jevons' Paradox. This paradox occurs when the efficiency gains from technological advancements lead to increased consumption of resources, rather than a decrease. The context provided discusses Jevons' Paradox in relation to AI, citing examples such as AI-driven logistics systems reducing delivery times and fuel usage per vehicle, but encouraging more frequent online orders and increasing total miles driven.","is_blank","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The phenomenon described as technological progress improving efficiency, which then results in increased usage and overall resource consumption, is known as Jevons' Paradox. This paradox occurs when the efficiency gains from technological advancements lead to increased consumption of resources, rather than a decrease. The context provided discusses Jevons' Paradox in relation to AI, citing examples such as AI-driven logistics systems reducing delivery times and fuel usage per vehicle, but encouraging more frequent online orders and increasing total miles driven."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","","","multiplier","[]","is_blank","is_blank",""
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","","","kWh","[]","is_blank","is_blank",""
"q120","How many pounds of CO2e are estimated for an average American life in one year?","","","lbs","[]","is_blank","is_blank",""
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides a list of the top 10 counties by per-household health cost from US data centers. Marion County, WV, has the highest per-household health cost at $1218.3.","Marion","is_blank","[""han2024""]","is_blank","is_blank","The context provides a list of the top 10 counties by per-household health cost from US data centers. Marion County, WV, has the highest per-household health cost at $1218.3."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The carbon emissions of Mistral-small changed after optimization in the financial sentiment classification task. According to the provided table, the CO2 emissions per inference task for Mistral-small decreased from 0.020 kg to 0.015 kg after optimization.","0.25","multiplier","[""khan2025""]","is_blank","is_blank","The carbon emissions of Mistral-small changed after optimization in the financial sentiment classification task. According to the provided table, the CO2 emissions per inference task for Mistral-small decreased from 0.020 kg to 0.015 kg after optimization."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides specific energy costs for the training and fine-tuning of the BLOOMz-7B model. According to the context, the training energy cost for BLOOMz-7B was 51,686 kWh, and the fine-tuning energy cost was 7,571 kWh.","59257","kWh","[""luccioni2024""]","is_blank","is_blank","The context provides specific energy costs for the training and fine-tuning of the BLOOMz-7B model. According to the context, the training energy cost for BLOOMz-7B was 51,686 kWh, and the fine-tuning energy cost was 7,571 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The total number of parameters in the final FLM-101B model is 101 billion.","101","parameters","[""li2025a""]","is_blank","is_blank","The total number of parameters in the final FLM-101B model is 101 billion."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides information on the energy consumption of training a 6.1B parameter model. According to the text, the energy consumption for training this model for 8 days on 256 NVIDIA A100s was 13.8 MWh, and a full training run would consume approximately 103,500 kWh.","103500","inferences","[""dodge2022""]","is_blank","is_blank","The context provides information on the energy consumption of training a 6.1B parameter model. According to the text, the energy consumption for training this model for 8 days on 256 NVIDIA A100s was 13.8 MWh, and a full training run would consume approximately 103,500 kWh."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context clearly states that in the 2024 study 'Power Hungry Processing', the total amount of energy consumed for all model experimentation and evaluation was 754.66 kWh.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The context clearly states that in the 2024 study 'Power Hungry Processing', the total amount of energy consumed for all model experimentation and evaluation was 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) that lists the BLOOMz models with their training energy cost, fine-tuning energy cost, inference cost, and the number of inferences required to sum to the training cost. Specifically, for the BLOOMz-7B model, it is stated that 592,570,000 inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning.","592,570,000","inferences","[""luccioni2024""]","is_blank","is_blank","The context provides a table (Table 5) that lists the BLOOMz models with their training energy cost, fine-tuning energy cost, inference cost, and the number of inferences required to sum to the training cost. Specifically, for the BLOOMz-7B model, it is stated that 592,570,000 inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany, and the statements from the population were categorized, processed, and published as the FKTG-dataset.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany, and the statements from the population were categorized, processed, and published as the FKTG-dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The provided documents discuss the environmental impact of AI models, including water consumption, but do not offer direct data for Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The provided documents discuss the environmental impact of AI models, including water consumption, but do not offer direct data for Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide specific information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides a comparison of CO2 emissions for air travel between San Francisco and New York to the emissions from training NLP models. Using this comparison, we can calculate the equivalent number of passengers for a given CO2e emission.","3.55","passengers","[""strubell2019""]","is_blank","is_blank","The context provides a comparison of CO2 emissions for air travel between San Francisco and New York to the emissions from training NLP models. Using this comparison, we can calculate the equivalent number of passengers for a given CO2e emission."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","","","percent","[]","is_blank","is_blank",""
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information on the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA models of different sizes. According to Table II, for LLaMA-13B, the bare minimum hardware required is 1 A100 80GB GPU.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information on the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA models of different sizes. According to Table II, for LLaMA-13B, the bare minimum hardware required is 1 A100 80GB GPU."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides an estimate for the energy consumption of a 6.1 billion parameter transformer model. According to the text, a full training run for this model would consume approximately 103,500 kWh of electricity. To find the CO2 emissions in metric tons, we need to know the carbon intensity, which is not directly provided in the question or context.","is_blank","metric tons","[""dodge2022""]","is_blank","is_blank","The context provides an estimate for the energy consumption of a 6.1 billion parameter transformer model. According to the text, a full training run for this model would consume approximately 103,500 kWh of electricity. To find the CO2 emissions in metric tons, we need to know the carbon intensity, which is not directly provided in the question or context."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide specific information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. However, it mentions that quantization and strategic optimizations can reduce energy consumption and carbon emissions by up to 45%.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the total carbon emissions avoided by pruning and quantizing large language models in 2023. However, it mentions that quantization and strategic optimizations can reduce energy consumption and carbon emissions by up to 45%."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context highlights a specific scenario where using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy.","24","percent","[""griggs2024""]","is_blank","is_blank","The context highlights a specific scenario where using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","","","USD per hour","[]","is_blank","is_blank",""
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context indicates that most carbon footprint analyses for AI models gather information manually by writing to authors. This is supported by the statement 'most carbon footprint analyses gather the information manually by writing to authors' and an example of Luccioni and Hernandez-Garcia reaching out to over 500 authors.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context indicates that most carbon footprint analyses for AI models gather information manually by writing to authors. This is supported by the statement 'most carbon footprint analyses gather the information manually by writing to authors' and an example of Luccioni and Hernandez-Garcia reaching out to over 500 authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","","","percent","[]","is_blank","is_blank",""
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running LLaMA models of different sizes. According to Table II, for LLaMA-7B, the bare minimum number of NVIDIA A100 80GB GPUs required is 1.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information on the bare minimum hardware requirements for running LLaMA models of different sizes. According to Table II, for LLaMA-7B, the bare minimum number of NVIDIA A100 80GB GPUs required is 1."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The provided context clearly states that experimental results reveal strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","The provided context clearly states that experimental results reveal strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context provides information on a study where researchers reached out to over 500 authors of AI papers to gather information needed to estimate the carbon footprint of their models. They were able to collect 95 answers, which represents an author response rate of 15.4%.","95","answers","[""luccioni2023"", ""luccioni2025b""]","is_blank","is_blank","The context provides information on a study where researchers reached out to over 500 authors of AI papers to gather information needed to estimate the carbon footprint of their models. They were able to collect 95 answers, which represents an author response rate of 15.4%."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context provides information about the JetMoE-8B project, including its training budget of $100k and 30,000 H100 GPU hours. To estimate the approximate cost per H100 GPU-hour, we can divide the total budget by the total GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The context provides information about the JetMoE-8B project, including its training budget of $100k and 30,000 H100 GPU hours. To estimate the approximate cost per H100 GPU-hour, we can divide the total budget by the total GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides information on the health costs associated with training a Llama-3.1 scale model in various locations. Specifically, it mentions that the total health cost can exceed 120% of the electricity cost and vary widely depending on the training data center locations. For Altoona, Iowa, the health cost is 2.51 million dollars, which is 122% of the electricity cost.","122","percent","[""han2024""]","is_blank","is_blank","The context provides information on the health costs associated with training a Llama-3.1 scale model in various locations. Specifically, it mentions that the total health cost can exceed 120% of the electricity cost and vary widely depending on the training data center locations. For Altoona, Iowa, the health cost is 2.51 million dollars, which is 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context clearly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a table listing Amazon Renewable Energy Projects by location, including the number of projects and total MW capacity. For the United Kingdom, it is stated that there are 36 projects with a total capacity of 901 MW.","36","projects","[""amazon2023""]","is_blank","is_blank","The context provides a table listing Amazon Renewable Energy Projects by location, including the number of projects and total MW capacity. For the United Kingdom, it is stated that there are 36 projects with a total capacity of 901 MW."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides a table with gender representation data for Amazon's U.S. workforce in 2023. The table shows that 43.1% of Amazon's U.S. workforce across all levels identified as men.","43.1","percent","[""amazon2023""]","is_blank","is_blank","The context provides a table with gender representation data for Amazon's U.S. workforce in 2023. The table shows that 43.1% of Amazon's U.S. workforce across all levels identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","","","percent","[]","is_blank","is_blank",""
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides information on the execution time breakdown for sparse and dense fine-tuning of BlackMamba and Mixtral models on an NVIDIA A40 GPU with 48GB memory. However, it does not provide a direct answer to the total execution time for a sparse BlackMamba model fine-tuned with a batch size of 84.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides information on the execution time breakdown for sparse and dense fine-tuning of BlackMamba and Mixtral models on an NVIDIA A40 GPU with 48GB memory. However, it does not provide a direct answer to the total execution time for a sparse BlackMamba model fine-tuned with a batch size of 84."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity' metric, which is the ratio of calculation to communication time, to assess the suitability of models for distributed training across different hardware setups and continents.","is_blank","is_blank","[""erben2023""]","is_blank","is_blank","The context introduces the 'granularity' metric, which is the ratio of calculation to communication time, to assess the suitability of models for distributed training across different hardware setups and continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company's yearly carbon removal targets.","640","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","is_blank","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions compared to the company's yearly carbon removal targets."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is 'water withdrawal'.","is_blank","is_blank","[""li2025b""]","is_blank","is_blank","The term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is 'water withdrawal'."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The Standing Committee of the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","is_blank","The Standing Committee of the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context provides information from Deloitte's 2021 report, stating that the average U.S. household is equipped with an average of 25 connected devices.","25","devices","[""wu2021b""]","is_blank","is_blank","The context provides information from Deloitte's 2021 report, stating that the average U.S. household is equipped with an average of 25 connected devices."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides information on the energy consumption required to pre-train large language models (LLMs). According to the text, the energy required spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout).","[0.8, 3500]","MWh","[""luccioni2025c""]","is_blank","is_blank","The context provides information on the energy consumption required to pre-train large language models (LLMs). According to the text, the energy required spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","One study estimates that 10-50 queries to the GPT-3 model consume approximately half a liter of water.","10","queries","[""luccioni2025a""]","is_blank","is_blank","One study estimates that 10-50 queries to the GPT-3 model consume approximately half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context provides a comparison of MT-Bench scores for various models, including JetMoE-8B-chat and Llama-2-13b-chat. According to the table, JetMoE-8B-chat achieved a MT-Bench score of 6.681, surpassing Llama-2-13b-chat which scored 6.650.","6.681","score","[""shen2024""]","is_blank","is_blank","The context provides a comparison of MT-Bench scores for various models, including JetMoE-8B-chat and Llama-2-13b-chat. According to the table, JetMoE-8B-chat achieved a MT-Bench score of 6.681, surpassing Llama-2-13b-chat which scored 6.650."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses.","is_blank","responses","[""li2025b""]","is_blank","is_blank","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings.","77","percent","[""griggs2024""]","is_blank","is_blank","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to 77% in conversational chat settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context clearly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context clearly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context clearly states that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","is_blank","The context clearly states that training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context, NVIDIA estimated that inference processing made up 80 to 90% of the machine learning (ML) workload in 2019. This information is mentioned in multiple references, including [ref_id=luccioni2024], [ref_id=fernandez2025], and [ref_id=chung2025].","80","percent","[""luccioni2024"", ""fernandez2025"", ""chung2025""]","is_blank","is_blank","According to the context, NVIDIA estimated that inference processing made up 80 to 90% of the machine learning (ML) workload in 2019. This information is mentioned in multiple references, including [ref_id=luccioni2024], [ref_id=fernandez2025], and [ref_id=chung2025]."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context provides the total amount of energy used and carbon emissions generated throughout the 'Power Hungry Processing' (2024) study. The study used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides the total amount of energy used and carbon emissions generated throughout the 'Power Hungry Processing' (2024) study. The study used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The provided context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method. This is because TDP is the maximum theoretical power the GPU can draw, and in practice, GPUs rarely operate at this maximum power level. Estimations using TDP nearly always result in overestimation, sometimes by a significant factor.","0","is_blank","[""chung2025""]","is_blank","is_blank","The provided context indicates that estimating GPU energy consumption based on its Thermal Design Power (TDP) is not a reliable and accurate method. This is because TDP is the maximum theoretical power the GPU can draw, and in practice, GPUs rarely operate at this maximum power level. Estimations using TDP nearly always result in overestimation, sometimes by a significant factor."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context provides specific energy consumption values for GPT-4o and GPT-4o mini. GPT-4o consumes around 2.875 Wh per query, while GPT-4o mini consumes slightly more at 3.098 Wh per query due to deployment on A100 hardware instead of H100s.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context provides specific energy consumption values for GPT-4o and GPT-4o mini. GPT-4o consumes around 2.875 Wh per query, while GPT-4o mini consumes slightly more at 3.098 Wh per query due to deployment on A100 hardware instead of H100s."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides a figure (Fig. 8) showing the query throughput of Mixtral and BlackMamba models for different batch sizes. For the dense Mixtral-CS model on the A100-40GB GPU, the throughput at batch size 1 is clearly labeled as 0.3 queries/sec.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","The context provides a figure (Fig. 8) showing the query throughput of Mixtral and BlackMamba models for different batch sizes. For the dense Mixtral-CS model on the A100-40GB GPU, the throughput at batch size 1 is clearly labeled as 0.3 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context indicates that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models did not continue to increase. Instead, it dramatically reduced. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context indicates that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models did not continue to increase. Instead, it dramatically reduced. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The normalized on-demand hourly price for an H100 GPU was found in a table listing GPU specifications and prices.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","The normalized on-demand hourly price for an H100 GPU was found in a table listing GPU specifications and prices."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides information on the water consumption of GPT-3 and GPT-4o. GPT-3 was trained and deployed by OpenAI in Microsoft's data centers with an estimated training energy of 1287 MWh. The water consumption for GPT-3 training is estimated to be over 700 kiloliters (kL) of water for cooling alone. However, specific information about GPT-4's training water consumption is not provided.","is_blank","liters of water","[""jegham2025"", ""li2025b""]","is_blank","is_blank","The context provides information on the water consumption of GPT-3 and GPT-4o. GPT-3 was trained and deployed by OpenAI in Microsoft's data centers with an estimated training energy of 1287 MWh. The water consumption for GPT-3 training is estimated to be over 700 kiloliters (kL) of water for cooling alone. However, specific information about GPT-4's training water consumption is not provided."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs is over $5,200. To estimate the hourly cost, we assume 30 days in a month. Therefore, the daily cost is $5,200 / 30 = $173.33, and the hourly cost is $173.33 / 24 = $7.22.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs is over $5,200. To estimate the hourly cost, we assume 30 days in a month. Therefore, the daily cost is $5,200 / 30 = $173.33, and the hourly cost is $173.33 / 24 = $7.22."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context clearly states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000× larger in size is required.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context clearly states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, a model 1,000× larger in size is required."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context provides the CO2 emissions for training a Transformer model with Neural Architecture Search (NAS) as 626,155 lbs but does not provide the emissions-to-driving-distance ratio.","is_blank","miles","[""dodge2022, luccioni2023, strubell2019""]","is_blank","is_blank","The context provides the CO2 emissions for training a Transformer model with Neural Architecture Search (NAS) as 626,155 lbs but does not provide the emissions-to-driving-distance ratio."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model was downloaded 606,096 times as of November 2023. The energy required per inference for BLOOMz-7B is 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the number of downloads by the energy per inference and then by 1 million.","60.6096","MWh","[""luccioni2024""]","is_blank","is_blank","The BLOOMz-7B model was downloaded 606,096 times as of November 2023. The energy required per inference for BLOOMz-7B is 1.0 × 10−4 kWh. To estimate the total energy consumed if every download resulted in 1 million inferences, we multiply the number of downloads by the energy per inference and then by 1 million."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context clearly states that JetMoE-8B was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context clearly states that JetMoE-8B was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context clearly states that if the trend of growing development costs continues, the largest training runs will exceed a cost of one billion dollars by the year 2027.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The context clearly states that if the trend of growing development costs continues, the largest training runs will exceed a cost of one billion dollars by the year 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context provides information about the training energy of GPT-3, which was estimated to be 1287 MWh. However, it does not provide a direct estimate of the total number of floating point operations to train GPT-3. An estimate of 4.5e22 FLOP is mentioned for smaller versions of GPT-3.","is_blank","FLOPS","[""cottiier2024"", ""li2025b""]","is_blank","is_blank","The context provides information about the training energy of GPT-3, which was estimated to be 1287 MWh. However, it does not provide a direct estimate of the total number of floating point operations to train GPT-3. An estimate of 4.5e22 FLOP is mentioned for smaller versions of GPT-3."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context clearly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context clearly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context does not provide specific information about the throughput data for the final 101B training stage or the total computational work performed during this stage in zettaFLOPs.","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the throughput data for the final 101B training stage or the total computational work performed during this stage in zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","","","percent","[]","is_blank","is_blank",""
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 A800 GPUs, so 24 servers have a total of 24 * 8 = 192 A800 GPUs.","192","GPUs","[""li2025a""]","is_blank","is_blank","The context mentions that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 A800 GPUs, so 24 servers have a total of 24 * 8 = 192 A800 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context provides an estimate of CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation. According to Strubell et al. and Luccioni et al., the emissions are comparable to the lifetime carbon emissions of five cars, which is approximately 626,155 pounds (284 metric tons) of CO2-equivalent GHG emissions (CO2e).","284","lifetimes","[""luccioni2023"", ""strubell2019""]","is_blank","is_blank","The context provides an estimate of CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation. According to Strubell et al. and Luccioni et al., the emissions are comparable to the lifetime carbon emissions of five cars, which is approximately 626,155 pounds (284 metric tons) of CO2-equivalent GHG emissions (CO2e)."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context clearly states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The context clearly states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context provides information about Amazon's on-site solar energy systems, stating that 50 new systems became operational in 2023, adding 58 MW of capacity. These projects generate an estimated 123,000 MWh and avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023""]","is_blank","is_blank","The context provides information about Amazon's on-site solar energy systems, stating that 50 new systems became operational in 2023, adding 58 MW of capacity. These projects generate an estimated 123,000 MWh and avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions the deployment of large language models across multiple GPUs and nodes. Specifically, it references the use of Megatron-LM for efficient large-scale language model training on GPU clusters.","Megatron-LM","is_blank","[""shen2024"", ""fernandez2025""]","is_blank","is_blank","The context mentions the deployment of large language models across multiple GPUs and nodes. Specifically, it references the use of Megatron-LM for efficient large-scale language model training on GPU clusters."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context provides a table (Table B2) that compares the energy consumption of various models on a single node versus a double node deployment. For the Llama 3.1 70B model, the energy consumption on a single node is 48.60 Wh, and on two nodes, it is 94.88 Wh. This indicates that the energy consumption increased by a factor of 1.95 when deployed on two nodes instead of one.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","The context provides a table (Table B2) that compares the energy consumption of various models on a single node versus a double node deployment. For the Llama 3.1 70B model, the energy consumption on a single node is 48.60 Wh, and on two nodes, it is 94.88 Wh. This indicates that the energy consumption increased by a factor of 1.95 when deployed on two nodes instead of one."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific information on the gallons of water consumed per ChatGPT user session in 2023. However, it mentions that GPT-3 consumes around half a liter of water for 10-50 queries. Additionally, Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the gallons of water consumed per ChatGPT user session in 2023. However, it mentions that GPT-3 consumes around half a liter of water for 10-50 queries. Additionally, Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context indicates that 700 million daily GPT-4o queries would result in annual electricity use comparable to 35,000 U.S. residential homes.","35000","homes","[""jegham2025""]","is_blank","is_blank","The context indicates that 700 million daily GPT-4o queries would result in annual electricity use comparable to 35,000 U.S. residential homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","","","percent","[]","is_blank","is_blank",""
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context mentions that Google's Iowa datacenter, where the Evolved Transformer was run, has a PUE of 1.12. This information directly supports the answer.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","The context mentions that Google's Iowa datacenter, where the Evolved Transformer was run, has a PUE of 1.12. This information directly supports the answer."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context provides a specific estimate for the total number of GPT-4o queries in 2025. According to the text, a recent analysis yields a total of approximately 772 billion GPT-4o queries in 2025.","772 billion","queries","[""jegham2025""]","is_blank","is_blank","The context provides a specific estimate for the total number of GPT-4o queries in 2025. According to the text, a recent analysis yields a total of approximately 772 billion GPT-4o queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides information about JetMoE-8B's performance on the OpenLLM Leaderboard benchmark suite. It mentions that JetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leaderboard. The specific average score is provided in a table.","53.0","score","[""shen2024""]","is_blank","is_blank","The context provides information about JetMoE-8B's performance on the OpenLLM Leaderboard benchmark suite. It mentions that JetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leaderboard. The specific average score is provided in a table."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context discusses the AI Act and its provisions regarding the reporting of energy consumption by AI models. It mentions the elimination of the open-source exemption and the need for open-source models to adhere to the same reporting standards as proprietary models. The context also highlights that the AI Act mandates risk assessment and mitigation for providers of general-purpose AI models with systemic risk and high-risk AI systems, which should include environmental risks.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context discusses the AI Act and its provisions regarding the reporting of energy consumption by AI models. It mentions the elimination of the open-source exemption and the need for open-source models to adhere to the same reporting standards as proprietary models. The context also highlights that the AI Act mandates risk assessment and mitigation for providers of general-purpose AI models with systemic risk and high-risk AI systems, which should include environmental risks."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The provided context does not clearly support the US national datacenter average Power Usage Effectiveness (PUE) in 2020. The context mentions the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but does not provide specific information for the US in 2020.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The provided context does not clearly support the US national datacenter average Power Usage Effectiveness (PUE) in 2020. The context mentions the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but does not provide specific information for the US in 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context provides a figure and description of KV Cache size growth for the OPT-2.7B model. Specifically, it mentions that when the batch size increases to 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","is_blank","The context provides a figure and description of KV Cache size growth for the OPT-2.7B model. Specifically, it mentions that when the batch size increases to 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context provides a clear range for the percentage of R&D staff costs, including equity, for the four notable models studied in-depth by Cottier et al. (2025). The range is between 29% and 49% of the total amortized cost.","29-49","percent","[""cottier2024""]","is_blank","is_blank","The context provides a clear range for the percentage of R&D staff costs, including equity, for the four notable models studied in-depth by Cottier et al. (2025). The range is between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The software package used to measure energy consumption during inference runs is CodeCarbon. This is mentioned in multiple references, including zschache2025, morrison2025, and rubei2025.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""rubei2025""]","is_blank","is_blank","The software package used to measure energy consumption during inference runs is CodeCarbon. This is mentioned in multiple references, including zschache2025, morrison2025, and rubei2025."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context provides an analysis of 100 news articles on ChatGPT's energy use, stating that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The context provides an analysis of 100 news articles on ChatGPT's energy use, stating that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The Compute Time Calibration Function (CTCF) is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","CTCF","is_blank","[""kim2025""]","is_blank","is_blank","The Compute Time Calibration Function (CTCF) is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context indicates that increasing the number of GPU shards for LLaMA-65B increases the energy per second. However, it does not directly state the effect on energy cost per response. But it mentions that energy per second increases with the number of shards, and energy per token also tends to increase with the number of shards.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context indicates that increasing the number of GPU shards for LLaMA-65B increases the energy per second. However, it does not directly state the effect on energy cost per response. But it mentions that energy per second increases with the number of shards, and energy per token also tends to increase with the number of shards."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides information on the environmental impact of mining rare earth metals used in manufacturing an H100 GPU. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and that an H100 is 0.1% rare earth metal by mass. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.","11","kL","[""morrison2025""]","is_blank","is_blank","The context provides information on the environmental impact of mining rare earth metals used in manufacturing an H100 GPU. It states that mining 1 kg of rare earth materials consumes about 11 kL of water and that an H100 is 0.1% rare earth metal by mass. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The provided context discusses policy proposals for AI and environmental impact, including energy consumption reporting. It mentions the elimination of the open-source exemption and energy reporting for various AI systems, but does not clearly state that open-source general-purpose AI models must report their energy consumption under current EU rules.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The provided context discusses policy proposals for AI and environmental impact, including energy consumption reporting. It mentions the elimination of the open-source exemption and energy reporting for various AI systems, but does not clearly state that open-source general-purpose AI models must report their energy consumption under current EU rules."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context provides information that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context provides information that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all Power Purchase Agreements (PPAs) purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","","","USD","[]","is_blank","is_blank",""
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of the o3 model for a long prompt is 12.222 Wh, and that of GPT-4.1 nano is 0.827 Wh. To find the factor by which the energy consumption of the o3 model is greater than that of GPT-4.1 nano, we divide the energy consumption of o3 by GPT-4.1 nano: 12.222 Wh / 0.827 Wh ≈ 14.78.","14.78","multiplier","[""jegham2025""]","is_blank","is_blank","The energy consumption of the o3 model for a long prompt is 12.222 Wh, and that of GPT-4.1 nano is 0.827 Wh. To find the factor by which the energy consumption of the o3 model is greater than that of GPT-4.1 nano, we divide the energy consumption of o3 by GPT-4.1 nano: 12.222 Wh / 0.827 Wh ≈ 14.78."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","Mélange achieves 15-77% cost reduction compared to single-GPU baselines for short-context workloads (Arena dataset) with a 120ms SLO.","77","percent","[""griggs2024""]","is_blank","is_blank","Mélange achieves 15-77% cost reduction compared to single-GPU baselines for short-context workloads (Arena dataset) with a 120ms SLO."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides information about the carbon footprint of FLM-101B, stating that it yields only 1/10 of the pre-training carbon footprint of a typical LLM. However, the exact total estimated net carbon emissions for the pre-training of FLM-101B are provided in Table 3, which is referenced but not directly shown in the provided snippets.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The context provides information about the carbon footprint of FLM-101B, stating that it yields only 1/10 of the pre-training carbon footprint of a typical LLM. However, the exact total estimated net carbon emissions for the pre-training of FLM-101B are provided in Table 3, which is referenced but not directly shown in the provided snippets."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide a direct answer to the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with an NVIDIA A40-48 GB GPU. However, it does provide an execution time breakdown for Mixtral and BlackMamba models. For Mixtral with a batch size of 1, the total execution time breakdown is given as forward, backward, and optimizer stages.","is_blank","seconds","[""xia2024""]","is_blank","is_blank","The context does not provide a direct answer to the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned with an NVIDIA A40-48 GB GPU. However, it does provide an execution time breakdown for Mixtral and BlackMamba models. For Mixtral with a batch size of 1, the total execution time breakdown is given as forward, backward, and optimizer stages."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. This indicates that the public health costs of AI are not evenly distributed across communities in the U.S.","0","is_blank","[""han2024""]","is_blank","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. This indicates that the public health costs of AI are not evenly distributed across communities in the U.S."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context supports the answer with a statement from Sun et al., 2019, which indicates that GPU theoretical performance per watt doubles approximately every 3-4 years.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context supports the answer with a statement from Sun et al., 2019, which indicates that GPU theoretical performance per watt doubles approximately every 3-4 years."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama, an open-source platform, was used for local AI model deployment and quantization. Specifically, a 4-bit quantization strategy was applied through Ollama to support local deployment of large language models in the financial sentiment case study.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The context mentions that Ollama, an open-source platform, was used for local AI model deployment and quantization. Specifically, a 4-bit quantization strategy was applied through Ollama to support local deployment of large language models in the financial sentiment case study."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The storage service used to shard and stream datasets for spot VMs that could terminate at any time is Backblaze (B2), an independent S3 storage provider. This was chosen for its replicated data centers that can serve requests worldwide, guaranteeing a reasonable ingress rate from every continent, and its manageable cost of $0.01/GB rate for egress and $0.005/GB/month for storage.","is_blank","is_blank","[""erben2023""]","is_blank","is_blank","The storage service used to shard and stream datasets for spot VMs that could terminate at any time is Backblaze (B2), an independent S3 storage provider. This was chosen for its replicated data centers that can serve requests worldwide, guaranteeing a reasonable ingress rate from every continent, and its manageable cost of $0.01/GB rate for egress and $0.005/GB/month for storage."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates a strong correlation between inference energy consumption and model runtime, suggesting a nearly linear relationship between runtime and energy consumption.","1","is_blank","[""zschache2025"", ""samsi2024""]","is_blank","is_blank","The context indicates a strong correlation between inference energy consumption and model runtime, suggesting a nearly linear relationship between runtime and energy consumption."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The U.S. Senator who introduced the AI Environmental Impacts Act bill in February 2024 is Senator Edward J. Markey (D-MA).","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The U.S. Senator who introduced the AI Environmental Impacts Act bill in February 2024 is Senator Edward J. Markey (D-MA)."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is $11.06/hr.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is $11.06/hr."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The provided context does not contain specific information about the estimated average GPU lifetime before retirement in AI data centers in 2024. However, it mentions that AWS announced an improved average expected life of its servers from five to six years in February 2024.","is_blank","years","[""amazon2023""]","is_blank","is_blank","The provided context does not contain specific information about the estimated average GPU lifetime before retirement in AI data centers in 2024. However, it mentions that AWS announced an improved average expected life of its servers from five to six years in February 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running LLaMA models, including LLaMA-13B. According to Table II, for LLaMA-13B on V100 32GB GPUs, the minimum required is 2 GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context provides information on the bare minimum hardware requirements for running LLaMA models, including LLaMA-13B. According to Table II, for LLaMA-13B on V100 32GB GPUs, the minimum required is 2 GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context provides information on the GHG emissions from the pre-training process for Google's Gemma family of language models, which is 1247.61 tons CO2e. This value is compared to the 'five cars' estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context provides information on the GHG emissions from the pre-training process for Google's Gemma family of language models, which is 1247.61 tons CO2e. This value is compared to the 'five cars' estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context provides information about the training time of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. According to Peters et al. (2018), ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","The context provides information about the training time of ELMo on 3 NVIDIA GTX 1080 Ti GPUs. According to Peters et al. (2018), ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The reported PUE of Google's hyperscale data centers in 2021 was 1.10.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","The reported PUE of Google's hyperscale data centers in 2021 was 1.10."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the provided context, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","is_blank","According to the provided context, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context provides a specific cost for fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU. The cost is mentioned as $3460.","3460","USD","[""xia2024""]","is_blank","is_blank","The context provides a specific cost for fine-tuning a sparse Mixtral model using 2 million queries with an NVIDIA H100 GPU. The cost is mentioned as $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","","","percent","[]","is_blank","is_blank",""
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context mentions that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s. To find the total number of H100 GPUs used, we can directly infer it from this information.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The context mentions that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s. To find the total number of H100 GPUs used, we can directly infer it from this information."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context provides information about the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training. It states that when actively training, the average GPU power is over 600W, which is over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","is_blank","The context provides information about the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training. It states that when actively training, the average GPU power is over 600W, which is over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","","","lbs","[]","is_blank","is_blank",""
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context provides information on the inference performance of LLaMA models on V100 and A100 GPUs. For LLaMA-13B, it is mentioned that the A100 outperforms the V100, with a 1.25 times increase in inference latency across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The context provides information on the inference performance of LLaMA models on V100 and A100 GPUs. For LLaMA-13B, it is mentioned that the A100 outperforms the V100, with a 1.25 times increase in inference latency across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context provides a clear estimate of the energy consumption of a single short query to GPT-4o.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context provides a clear estimate of the energy consumption of a single short query to GPT-4o."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context provides a comparison of the costs of different instances selected by InferSave and Max-Performance policies for online inference workloads. Specifically, for an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The context states that the Max-Performance instance (g6e.xlarge) is about 280% more expensive than InferSave's top choice.","280","percent","[""kim2025""]","is_blank","is_blank","The context provides a comparison of the costs of different instances selected by InferSave and Max-Performance policies for online inference workloads. Specifically, for an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge at a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699. The context states that the Max-Performance instance (g6e.xlarge) is about 280% more expensive than InferSave's top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context suggests that for models generating a single token per inference, a V100 or A30 GPU is more efficient. This is supported by the text: 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.'","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","The context suggests that for models generating a single token per inference, a V100 or A30 GPU is more efficient. This is supported by the text: 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.'"
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context supports the answer by mentioning that reporting the financial cost of finding, training, and running models is a key Green AI practice. This is stated in the paper 'Green AI' by Schwartz et al. (2019), which advocates for making efficiency a more common evaluation criterion for AI papers alongside accuracy and related measures.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context supports the answer by mentioning that reporting the financial cost of finding, training, and running models is a key Green AI practice. This is stated in the paper 'Green AI' by Schwartz et al. (2019), which advocates for making efficiency a more common evaluation criterion for AI papers alongside accuracy and related measures."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context clearly states that the global generation of electronic waste reached 62 million tonnes in 2022.","62 million","metric tons","[""luccioni2025a""]","is_blank","is_blank","The context clearly states that the global generation of electronic waste reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","is_blank","is_blank","Watts","[""is_blank""]","is_blank","is_blank","is_blank"
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context clearly supports the answer that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","is_blank","The context clearly supports the answer that training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","","","multiplier","[]","is_blank","is_blank",""
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context provides information on a recent eco-efficiency analysis using Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. The model that ranked highest in this analysis is o3-mini with a cross-efficiency score of 0.884.","0.884","is_blank","[""jegham2025""]","is_blank","is_blank","The context provides information on a recent eco-efficiency analysis using Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. The model that ranked highest in this analysis is o3-mini with a cross-efficiency score of 0.884."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context supports the answer that smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns. The document [wu2021b] mentions 'current averages of less than 3 years for cell phones' and discusses the need for developing expandable hardware and software stacks to facilitate significantly longer lifetimes.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context supports the answer that smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns. The document [wu2021b] mentions 'current averages of less than 3 years for cell phones' and discusses the need for developing expandable hardware and software stacks to facilitate significantly longer lifetimes."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context discusses the intra-zone scaling with T4 GPUs for CV models, mentioning that CV's per-GPU speedup is almost linear. However, it also notes that the per-GPU contribution to the speedup decreases as more GPUs are added, and the granularity affects scalability.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context discusses the intra-zone scaling with T4 GPUs for CV models, mentioning that CV's per-GPU speedup is almost linear. However, it also notes that the per-GPU contribution to the speedup decreases as more GPUs are added, and the granularity affects scalability."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The FLM-101B model has a context window of 2,048 tokens as stated in the model configurations.","2048","tokens","[""li2025a""]","is_blank","is_blank","The FLM-101B model has a context window of 2,048 tokens as stated in the model configurations."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context supports the statement that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth. This is evidenced by the power consumption analysis in Figure 10 and the description that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, whereas diffusion models are significantly more compute-intensive.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context supports the statement that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth. This is evidenced by the power consumption analysis in Figure 10 and the description that LLM decoding is memory-intensive and does not fully utilize the GPU's compute resources, whereas diffusion models are significantly more compute-intensive."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The provided context does not clearly state the percentage of Amazon's People Managers globally who identified as women in 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not clearly state the percentage of Amazon's People Managers globally who identified as women in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context provides a breakdown of the total amortized model development costs for selected models, including GPT-3, OPT-175B, GPT-4, and Gemini Ultra. When excluding equity, the fraction of computing hardware costs rises to 61–76%.","61-76","percent","[""cottier2024""]","is_blank","is_blank","The context provides a breakdown of the total amortized model development costs for selected models, including GPT-3, OPT-175B, GPT-4, and Gemini Ultra. When excluding equity, the fraction of computing hardware costs rises to 61–76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context indicates that in the financial sentiment case study, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This suggests that accuracy and F1 scores do not always improve after optimization.","0","is_blank","[""khan2025""]","is_blank","is_blank","The context indicates that in the financial sentiment case study, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This suggests that accuracy and F1 scores do not always improve after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., which is used to convert power to estimated CO2 emissions. The conversion factor is given as CO2e = 0.954pt, taking into account the relative proportions of different energy sources consumed to produce energy in the United States.","0.954","lbs/kWh","[""strubell2019"", ""han2024""]","is_blank","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., which is used to convert power to estimated CO2 emissions. The conversion factor is given as CO2e = 0.954pt, taking into account the relative proportions of different energy sources consumed to produce energy in the United States."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","According to a recent study mentioned in the context, the global AI could consume 85 – 134 TWh of electricity in 2027.","85-134","TWh","[""li2025b""]","is_blank","is_blank","According to a recent study mentioned in the context, the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context clearly states that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","is_blank","The context clearly states that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload consists of 128 input tokens and 512 output tokens, evaluating a total of 3000 requests.","6400","tokens","[""kim2025""]","is_blank","is_blank","The online inference workload consists of 128 input tokens and 512 output tokens, evaluating a total of 3000 requests."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration. The Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration. It also neglects to address the substantial water consumption, a key concern in data center operations.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration. The Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration. It also neglects to address the substantial water consumption, a key concern in data center operations."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper shows that for a short job (DenseNet 201) in the West US region, the maximum potential percentage reduction in CO2 emissions is up to 80%.","80","percent","[""dodge2022""]","is_blank","is_blank","The 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper shows that for a short job (DenseNet 201) in the West US region, the maximum potential percentage reduction in CO2 emissions is up to 80%."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","","","times","[]","is_blank","is_blank",""
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context clearly states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This indicates that traditional models did not achieve accuracy comparable to large language models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context provides a table listing Amazon Renewable Energy Projects announced as of January 2024, including the number of projects and total MW capacity by location. The United States is listed with 244 projects and 17,706 MW capacity.","244","projects","[""amazon2023""]","is_blank","is_blank","The context provides a table listing Amazon Renewable Energy Projects announced as of January 2024, including the number of projects and total MW capacity by location. The United States is listed with 244 projects and 17,706 MW capacity."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide specific information on the percent of power usage Amazon's AWS covered with renewable energy in 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the percent of power usage Amazon's AWS covered with renewable energy in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend that AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility.","is_blank","is_blank","[""ebert2024""]","is_blank","is_blank","The authors recommend that AI energy consumption should be reported at the cumulative server level to balance accuracy and feasibility."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","","","percent","[]","is_blank","is_blank",""
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context provides information about the hardware requirements for serving large language models, including Llama2-70b. Specifically, it mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The context provides information about the hardware requirements for serving large language models, including Llama2-70b. Specifically, it mentions that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","","","percent","[]","is_blank","is_blank",""
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","is_blank","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","is_blank"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context provides a specific estimate for the upfront hardware acquisition cost to train GPT-4. According to the text, it cost $800M to acquire the hardware used to train GPT-4.","800000000","USD","[""cottier2024""]","is_blank","is_blank","The context provides a specific estimate for the upfront hardware acquisition cost to train GPT-4. According to the text, it cost $800M to acquire the hardware used to train GPT-4."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context clearly indicates that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. Therefore, the statement that 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications is false.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context clearly indicates that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. Therefore, the statement that 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context provides information about the maximum batch sizes for fine-tuning Mixtral on different NVIDIA GPUs. According to Fig. 13, the ground truth maximum batch size for fine-tuning Mixtral on an NVIDIA A100-40GB GPU is 5.","5","samples","[""xia2024""]","is_blank","is_blank","The context provides information about the maximum batch sizes for fine-tuning Mixtral on different NVIDIA GPUs. According to Fig. 13, the ground truth maximum batch size for fine-tuning Mixtral on an NVIDIA A100-40GB GPU is 5."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context provides a comparison of the energy consumption of two preemption mechanisms, Recomputation and Swapping, for LLM inference servers when overloaded. It is stated that Swapping consistently consumes less energy than Recomputation because Recomputation performs extra computation when restoring requests, whereas Swapping only copies data without running computation.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context provides a comparison of the energy consumption of two preemption mechanisms, Recomputation and Swapping, for LLM inference servers when overloaded. It is stated that Swapping consistently consumes less energy than Recomputation because Recomputation performs extra computation when restoring requests, whereas Swapping only copies data without running computation."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The provided context does not clearly support an answer to the question about Google's 2024 environmental report and the percentage increase in GHG emissions since 2019.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not clearly support an answer to the question about Google's 2024 environmental report and the percentage increase in GHG emissions since 2019."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","","","percent","[]","is_blank","is_blank",""
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context provides information on the emissions savings for the 'Pause and Resume' optimization for training a 6B parameter transformer. According to the text, for long jobs like the 6 billion parameter language model training run, which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","25","percent","[""dodge2022""]","is_blank","is_blank","The context provides information on the emissions savings for the 'Pause and Resume' optimization for training a 6B parameter transformer. According to the text, for long jobs like the 6 billion parameter language model training run, which ran for 8 days, doubling the duration can lead to significant savings up to about 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The JetMoE-8B architecture reduces inference computation by about 70% compared to the Llama2-7B model. This is stated multiple times in the context snippets.","70","percent","[""shen2024""]","is_blank","is_blank","The JetMoE-8B architecture reduces inference computation by about 70% compared to the Llama2-7B model. This is stated multiple times in the context snippets."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The carbon footprint of training the BERT large language model was quantified in a seminal 2019 study by Strubell et al. as reaching 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b"", ""zschache2025""]","is_blank","is_blank","The carbon footprint of training the BERT large language model was quantified in a seminal 2019 study by Strubell et al. as reaching 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The estimated training energy of the full GPT-3 model is clearly stated in the context as 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","is_blank","The estimated training energy of the full GPT-3 model is clearly stated in the context as 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context clearly indicates that the Mixture of Experts (MoE) layer is a crucial component in LLM fine-tuning and is often targeted for optimization to enhance performance. The MoE layer is identified as the most time-consuming part of LLM fine-tuning, accounting for 85% of the overall execution time on average.","1","is_blank","[""chen2024"", ""xia2024""]","is_blank","is_blank","The context clearly indicates that the Mixture of Experts (MoE) layer is a crucial component in LLM fine-tuning and is often targeted for optimization to enhance performance. The MoE layer is identified as the most time-consuming part of LLM fine-tuning, accounting for 85% of the overall execution time on average."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context does not provide a specific maximum batch size for fine-tuning Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU with 48 GB memory.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context does not provide a specific maximum batch size for fine-tuning Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU with 48 GB memory."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context clearly states that for high granularity tasks like CV, intercontinental training only slows down performance by 7% compared to local training.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context clearly states that for high granularity tasks like CV, intercontinental training only slows down performance by 7% compared to local training."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","","","hectares","[]","is_blank","is_blank",""
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context provides a specific value for the emissions of the BERT-based model bert-base-multilingual-uncased-sentiment for text classification tasks. According to the text, bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context provides a specific value for the emissions of the BERT-based model bert-base-multilingual-uncased-sentiment for text classification tasks. According to the text, bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context provides information on the carbon emissions from training a BERT model across different geographic regions. It mentions that the most efficient region had emissions of approximately 7k grams, while the least efficient region had emissions of about 26k grams.","19","grams","[""dodge2022""]","is_blank","is_blank","The context provides information on the carbon emissions from training a BERT model across different geographic regions. It mentions that the most efficient region had emissions of approximately 7k grams, while the least efficient region had emissions of about 26k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context clearly states that the direct release of environmental information for notable models peaked in 2022 and then declined. This is supported by the provided text which mentions 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.'","2022","year","[""luccioni2025c""]","is_blank","is_blank","The context clearly states that the direct release of environmental information for notable models peaked in 2022 and then declined. This is supported by the provided text which mentions 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.'"
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","","","days","[]","is_blank","is_blank",""
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information about the water consumption of Google's DeepMind AlphaFold servers in 2023.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the water consumption of Google's DeepMind AlphaFold servers in 2023."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs. However, it also mentions that 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.' This implies that adding compute resources can both increase and reduce costs depending on the context, but specifically for reducing costs in certain scenarios, it can be beneficial.","0","is_blank","[""xia2024""]","is_blank","is_blank","The context states that adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs. However, it also mentions that 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.' This implies that adding compute resources can both increase and reduce costs depending on the context, but specifically for reducing costs in certain scenarios, it can be beneficial."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The total energy consumption for training the FLM-101B model is provided in Table 3 of the context. According to the table, the energy consumption for FLM-101B is 40 MkWh.","40","kWh","[""li2025a""]","is_blank","is_blank","The total energy consumption for training the FLM-101B model is provided in Table 3 of the context. According to the table, the energy consumption for FLM-101B is 40 MkWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context mentions that the total public health costs attributable to U.S. data centers are projected to potentially triple from 2023 to 2028. Quantitatively, based on low- and high-growth scenarios, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively.","20000000000","USD","[""han2024""]","is_blank","is_blank","The context mentions that the total public health costs attributable to U.S. data centers are projected to potentially triple from 2023 to 2028. Quantitatively, based on low- and high-growth scenarios, the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU is provided in Table IV of the context. The cost is listed as $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU is provided in Table IV of the context. The cost is listed as $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides information on the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. However, it does not explicitly state the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with an NVIDIA A40-48 GB.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context provides information on the maximum batch size supported by different model and dataset combinations on an NVIDIA A40 GPU with 48GB memory. However, it does not explicitly state the batch size of the longest-running MoE layer for a sparse Mixtral model fine-tuned with an NVIDIA A40-48 GB."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide a direct answer to the total execution time of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10. However, it provides insights into the execution time breakdown and throughput for different models and GPUs.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide a direct answer to the total execution time of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10. However, it provides insights into the execution time breakdown and throughput for different models and GPUs."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level or other component-based power consumption tracking for overall energy measurements is not recommended. This is because it substantially under-represents the actual energy consumption, measuring just a single component.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that GPU-level or other component-based power consumption tracking for overall energy measurements is not recommended. This is because it substantially under-represents the actual energy consumption, measuring just a single component."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context mentions that in a 2023 article estimating the carbon footprint of BLOOM, Luccioni et al. found that training accounted for only half of the model's overall emissions.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The context mentions that in a 2023 article estimating the carbon footprint of BLOOM, Luccioni et al. found that training accounted for only half of the model's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context provides information on the bare minimum hardware requirements for running LLaMA models. According to the context, for LLaMA-7B, the bare minimum is 1 V100 32GB GPU.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context provides information on the bare minimum hardware requirements for running LLaMA models. According to the context, for LLaMA-7B, the bare minimum is 1 V100 32GB GPU."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provides information on GPT-3's water consumption for training and inference. It mentions that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. The specific location for training GPT-3 is not public, but data centers in various locations including Arizona are considered.","is_blank","requests","[""li2025b""]","is_blank","is_blank","The context provides information on GPT-3's water consumption for training and inference. It mentions that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. The specific location for training GPT-3 is not public, but data centers in various locations including Arizona are considered."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The estimated CO2 emission for one year of average US home energy use is 8.3 metric tons. This information is provided in the context snippet from dodge2022.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The estimated CO2 emission for one year of average US home energy use is 8.3 metric tons. This information is provided in the context snippet from dodge2022."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context does not provide specific information about the JetMoE-8B model's score on the GSM8k benchmark. It discusses the model's performance on other benchmarks and its general capabilities but does not mention GSM8k specifically.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the JetMoE-8B model's score on the GSM8k benchmark. It discusses the model's performance on other benchmarks and its general capabilities but does not mention GSM8k specifically."
