"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet from ref_id=wu2021b explicitly states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in electricity consumption during that period.","4","percent","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b explicitly states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in electricity consumption during that period."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e annually, equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","is_blank","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e annually, equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context explicitly states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This information is repeated in multiple context snippets from the same document (ref_id=amazon2023).","24","data centers","[""amazon2023""]","is_blank","is_blank","The context explicitly states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This information is repeated in multiple context snippets from the same document (ref_id=amazon2023)."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context from ref_id=morrison2025 states that NVIDIA does not release embodied carbon emissions data, but they assume 463 kg of CO2eq per GPU based on previous work. Additionally, they mention an amortized embodied emission of 0.013 kg CO2eq per GPU hour over a 4-year lifespan. However, the question asks for the total estimated embodied carbon emissions per GPU, not per hour. The 463 kg per GPU is a direct estimate provided in the context, which answers the question.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","The context from ref_id=morrison2025 states that NVIDIA does not release embodied carbon emissions data, but they assume 463 kg of CO2eq per GPU based on previous work. Additionally, they mention an amortized embodied emission of 0.013 kg CO2eq per GPU hour over a 4-year lifespan. However, the question asks for the total estimated embodied carbon emissions per GPU, not per hour. The 463 kg per GPU is a direct estimate provided in the context, which answers the question."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the estimated amortized training cost for GPT-4 is $40 million (from ref_id=cottier2024) and the total training budget for FLM-101B is $100,000 (from ref_id=li2025a). The factor is calculated by dividing $40,000,000 by $100,000, resulting in 400.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","The context states that the estimated amortized training cost for GPT-4 is $40 million (from ref_id=cottier2024) and the total training budget for FLM-101B is $100,000 (from ref_id=li2025a). The factor is calculated by dividing $40,000,000 by $100,000, resulting in 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context snippet from ref_id=strubell2019 provides a direct value for CO2e emissions of a round trip flight from NY to SF as 1984 lbs. The question asks for San Francisco (SF) to New York (NY), which matches the direction mentioned in the context. The answer is converted to tCO2e by converting pounds to metric tons (1984 lbs ≈ 0.9 tCO2e).","0.9","tCO2e","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 provides a direct value for CO2e emissions of a round trip flight from NY to SF as 1984 lbs. The question asks for San Francisco (SF) to New York (NY), which matches the direction mentioned in the context. The answer is converted to tCO2e by converting pounds to metric tons (1984 lbs ≈ 0.9 tCO2e)."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context explicitly states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. This is supported by Table 4 which lists the model's performance metrics, including the 'Average' column showing 43.94 for FLM-101B.","43.94","score","[""li2025a""]","is_blank","is_blank","The context explicitly states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. This is supported by Table 4 which lists the model's performance metrics, including the 'Average' column showing 43.94 for FLM-101B."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context explicitly states that there was a 'more than 6,750 fold improvement in processor clock speed' between microprocessors manufactured in 1971 (Intel 4004) and those in 2021.","6750","fold","[""wu2021b""]","is_blank","is_blank","The context explicitly states that there was a 'more than 6,750 fold improvement in processor clock speed' between microprocessors manufactured in 1971 (Intel 4004) and those in 2021."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context snippet from ref_id=luccioni2023 states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to 14.8 days with 10,000 GPUs. This directly answers the question about the number of days required using 10,000 V100 GPUs.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context snippet from ref_id=luccioni2023 states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to 14.8 days with 10,000 GPUs. This directly answers the question about the number of days required using 10,000 V100 GPUs."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context from ref_id=morrison2025 includes Table 3 which lists GPU Power Usage (kWh) for the Llama 3.2 1B model at 8 requests/s as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The context from ref_id=morrison2025 includes Table 3 which lists GPU Power Usage (kWh) for the Llama 3.2 1B model at 8 requests/s as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context explicitly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. This information is provided in the context snippets from ref_id=han2024.","13000","tons","[""han2024""]","is_blank","is_blank","The context explicitly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. This information is provided in the context snippets from ref_id=han2024."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly answers the question about the percentage of time saved by using the growth strategy for the 101B model.","72","percent","[""li2025a""]","is_blank","is_blank","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly answers the question about the percentage of time saved by using the growth strategy for the 101B model."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context states that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-1 and scope-2 pollutants. However, the question specifically asks about scope-2 pollutants alone. The context does not provide a separate figure for scope-2 pollutants alone in 2030, so the answer cannot be determined from the provided information.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context states that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-1 and scope-2 pollutants. However, the question specifically asks about scope-2 pollutants alone. The context does not provide a separate figure for scope-2 pollutants alone in 2030, so the answer cannot be determined from the provided information."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context snippet from ref_id=dodge2022 states that the 6 billion parameter model's training run was only 13% complete, taking 8 days. Extrapolating to 100% completion would require approximately 60 days (8 days / 0.13 ≈ 61.5 days).","60","days","[""dodge2022""]","is_blank","is_blank","The context snippet from ref_id=dodge2022 states that the 6 billion parameter model's training run was only 13% complete, taking 8 days. Extrapolating to 100% completion would require approximately 60 days (8 days / 0.13 ≈ 61.5 days)."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","","","is_blank","[]","is_blank","is_blank",""
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This information is repeated multiple times in the provided snippets from the same document (ref_id=stone2022).","2014","year","[""stone2022""]","is_blank","is_blank","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This information is repeated multiple times in the provided snippets from the same document (ref_id=stone2022)."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context snippet from ref_id=luccioni2025a explicitly states that the UN's Global E-Waste Monitor 2024 reported about 22% of e-waste being formally collected and recycled. This information is directly provided in the text and referenced to source [10], which corresponds to the Global E-Waste Monitor 2024 document.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a explicitly states that the UN's Global E-Waste Monitor 2024 reported about 22% of e-waste being formally collected and recycled. This information is directly provided in the text and referenced to source [10], which corresponds to the Global E-Waste Monitor 2024 document."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context from ref_id=luccioni2024 provides a table (Table 5) listing the training energy for BLOOMz-7B as 51,686 kWh. Since the question asks for the energy consumption in MWh, converting 51,686 kWh to MWh by dividing by 1,000 gives 51.686 MWh.","51.686","MWh","[""luccioni2024""]","is_blank","is_blank","The context from ref_id=luccioni2024 provides a table (Table 5) listing the training energy for BLOOMz-7B as 51,686 kWh. Since the question asks for the energy consumption in MWh, converting 51,686 kWh to MWh by dividing by 1,000 gives 51.686 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context mentions the Switch Transformer model with 1.5 trillion parameters and compares its carbon emissions to GPT-3, but does not provide information about the percentage of parameters activated per token.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions the Switch Transformer model with 1.5 trillion parameters and compares its carbon emissions to GPT-3, but does not provide information about the percentage of parameters activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context includes a table (Table 1) under section 4.2 Hyper-parameters which lists 'Nexperts' as 8 for JetMoE-8B. This directly answers the question about the number of experts per MoE layer.","8","experts","[""shen2024""]","is_blank","is_blank","The context includes a table (Table 1) under section 4.2 Hyper-parameters which lists 'Nexperts' as 8 for JetMoE-8B. This directly answers the question about the number of experts per MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context mentions that for BlackMamba-D (dense) with a batch size of 30, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for dense BlackMamba with batch size 30 is approximately 2.0 seconds.","2.0","second","[""xia2024""]","is_blank","is_blank","The context mentions that for BlackMamba-D (dense) with a batch size of 30, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for dense BlackMamba with batch size 30 is approximately 2.0 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context explicitly states that the total cost of training FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated to the English portion.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context explicitly states that the total cost of training FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated to the English portion."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context from samsi2024 describes the experimental setup using the MIT Supercloud HPC system with Intel Xeon Gold 6248 CPUs and NVIDIA V100 GPUs. The Financial Sentiment Classification is mentioned in khan2025's dataset section, which used an 11th Gen Intel Core i7-1165G7 processor for local inference.","11th Gen Intel Core i7-1165G7 processor","is_blank","[""samsi2024"", ""khan2025""]","is_blank","is_blank","The context from samsi2024 describes the experimental setup using the MIT Supercloud HPC system with Intel Xeon Gold 6248 CPUs and NVIDIA V100 GPUs. The Financial Sentiment Classification is mentioned in khan2025's dataset section, which used an 11th Gen Intel Core i7-1165G7 processor for local inference."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context mentions that the study compared the inference costs of various categories of ML systems, including task-specific and general-purpose models. However, the exact number of different machine learning models sampled and analyzed is not explicitly stated in the provided context snippets.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context mentions that the study compared the inference costs of various categories of ML systems, including task-specific and general-purpose models. However, the exact number of different machine learning models sampled and analyzed is not explicitly stated in the provided context snippets."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% results in the overall carbon footprint decreasing by 3×. This directly answers the question by providing the factor of reduction.","3","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that increasing GPU utilization up to 80% results in the overall carbon footprint decreasing by 3×. This directly answers the question by providing the factor of reduction."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, with a median of 2.2x. This range is derived from a log-normal distribution used in the study.","1.2-4","multiplier","[""cottier2024""]","is_blank","is_blank","The context states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, with a median of 2.2x. This range is derived from a log-normal distribution used in the study."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context from dodge2022 explicitly states that training a 6.1 billion parameter transformer model for a full run would consume approximately 103.5 MWh. This is calculated by scaling up the partial training energy consumption (13.8 MWh for 13% completion) to 100%.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The context from dodge2022 explicitly states that training a 6.1 billion parameter transformer model for a full run would consume approximately 103.5 MWh. This is calculated by scaling up the partial training energy consumption (13.8 MWh for 13% completion) to 100%."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context explicitly states that the paper examines Jevons' Paradox in the context of AI, where efficiency gains lead to increased consumption rather than environmental benefits. The paradox is described as a key economic principle undermining the assumption that technical efficiency alone reduces environmental harm.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The context explicitly states that the paper examines Jevons' Paradox in the context of AI, where efficiency gains lead to increased consumption rather than environmental benefits. The paradox is described as a key economic principle undermining the assumption that technical efficiency alone reduces environmental harm."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, as per multiple references to the same document (li2025b).","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, as per multiple references to the same document (li2025b)."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the claim that Red AI is on the decline due to diminishing returns.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the claim that Red AI is on the decline due to diminishing returns."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days.","21.54","days","[""li2025a""]","is_blank","is_blank","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a significant portion of machine learning model experimentation at Facebook uses GPUs at only 30-50% capacity, which directly contradicts the claim of over 80% utilization.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context states that a significant portion of machine learning model experimentation at Facebook uses GPUs at only 30-50% capacity, which directly contradicts the claim of over 80% utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippets from ref_id=li2025b and ref_id=jegham2025 both explicitly state that the estimated training energy consumption for GPT-3 is 1287 MWh. This value is directly provided in the text and tables of these documents.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context snippets from ref_id=li2025b and ref_id=jegham2025 both explicitly state that the estimated training energy consumption for GPT-3 is 1287 MWh. This value is directly provided in the text and tables of these documents."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score 21' project multiple times, explicitly stating that it aims to establish a standardized method for comparing the inference efficiency of AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context mentions the 'AI Energy Score 21' project multiple times, explicitly stating that it aims to establish a standardized method for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context mentions that for BlackMamba with a dense setup and batch size 30, the kernel-level MoE time breakdown shows the longest kernel execution time. Specifically, in Figure 6, the 'matmul(w1)' kernel for BlackMamba-Dense(bsz=30) has the highest execution time of 2000 microseconds.","2000","microseconds","[""xia2024""]","is_blank","is_blank","The context mentions that for BlackMamba with a dense setup and batch size 30, the kernel-level MoE time breakdown shows the longest kernel execution time. Specifically, in Figure 6, the 'matmul(w1)' kernel for BlackMamba-Dense(bsz=30) has the highest execution time of 2000 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context includes a table (Table 1) in the 'Hyper-parameters' section of the JetMoE-8B model documentation, which explicitly lists 'Top-k' as 2. Additionally, the text states that 'we set the same number of experts to 8 and top-k to 2 for every layer.'","2","experts","[""shen2024""]","is_blank","is_blank","The context includes a table (Table 1) in the 'Hyper-parameters' section of the JetMoE-8B model documentation, which explicitly lists 'Top-k' as 2. Additionally, the text states that 'we set the same number of experts to 8 and top-k to 2 for every layer.'"
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context from Schwartz et al. (2019) explicitly states in Figure 1 that the compute used for training deep learning models increased 300,000x over 6 years (2012-2018). This directly supports the claim of a 200,000x increase mentioned in the question, making the statement true.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context from Schwartz et al. (2019) explicitly states in Figure 1 that the compute used for training deep learning models increased 300,000x over 6 years (2012-2018). This directly supports the claim of a 200,000x increase mentioned in the question, making the statement true."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from ref_id=wu2021b explicitly states that global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b explicitly states that global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, which is an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions achieving 100% renewable energy matching in 2023.","22","data centers","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, which is an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions achieving 100% renewable energy matching in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. Since the question asks for the approximate age of the field in 2025, subtracting 1956 from 2025 gives 69 years.","69","years","[""stone2022""]","is_blank","is_blank","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. Since the question asks for the approximate age of the field in 2025, subtracting 1956 from 2025 gives 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context states that the 'five cars' carbon footprint estimate from the 2019 study by Strubell et al. was based on neural architecture search (NAS), a process used to identify optimized model architectures. The NAS approach was highlighted as a large-scale, infrequently performed procedure due to its resource-intensive nature and intended reusability to reduce subsequent emissions.","Neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that the 'five cars' carbon footprint estimate from the 2019 study by Strubell et al. was based on neural architecture search (NAS), a process used to identify optimized model architectures. The NAS approach was highlighted as a large-scale, infrequently performed procedure due to its resource-intensive nature and intended reusability to reduce subsequent emissions."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption per generation by 44% compared to minimizing latency.","44","percent","[""chung2025""]","is_blank","is_blank","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption per generation by 44% compared to minimizing latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that Table III shows the maximum batch size supported by LLM fine-tuning on an NVIDIA A40 GPU with 48GB memory. For BlackMamba-S (sparse setup) on the GS dataset (GSM8K), the maximum batch size is 20 samples.","20","samples","[""xia2024""]","is_blank","is_blank","The context states that Table III shows the maximum batch size supported by LLM fine-tuning on an NVIDIA A40 GPU with 48GB memory. For BlackMamba-S (sparse setup) on the GS dataset (GSM8K), the maximum batch size is 20 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","","","GW","[]","is_blank","is_blank",""
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o's annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o's annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The provided context snippets discuss the energy consumption of AI workloads and data centers, including projections for the US energy demand by 2030. However, none of the documents mention the percentage of AI inference workloads in Asia powered by coal in 2023. The context focuses on US energy projections, water usage, and environmental impacts but does not provide specific data on coal usage for AI inference in Asia.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context snippets discuss the energy consumption of AI workloads and data centers, including projections for the US energy demand by 2030. However, none of the documents mention the percentage of AI inference workloads in Asia powered by coal in 2023. The context focuses on US energy projections, water usage, and environmental impacts but does not provide specific data on coal usage for AI inference in Asia."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","","","PUE","[]","is_blank","is_blank",""
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context explicitly states multiple times that JetMoE-8B has 8B parameters but only activates 2B parameters for each input token during inference. For example, one snippet mentions 'JetMoE-8B has 8B parameters while only activating 2B for each input token' and another repeats 'activating 2B for each input token'.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context explicitly states multiple times that JetMoE-8B has 8B parameters but only activates 2B parameters for each input token during inference. For example, one snippet mentions 'JetMoE-8B has 8B parameters while only activating 2B for each input token' and another repeats 'activating 2B for each input token'."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context snippet from ref_id=luccioni2025c includes a table titled 'Range of Pre-Training Environmental Impacts (Representative Models Displayed)' which lists the Llama 7B model with GHG emissions of 14 tCO2e. This directly answers the question about the emissions associated with pre-training the Llama 7B model.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025c includes a table titled 'Range of Pre-Training Environmental Impacts (Representative Models Displayed)' which lists the Llama 7B model with GHG emissions of 14 tCO2e. This directly answers the question about the emissions associated with pre-training the Llama 7B model."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides specific numbers for electric delivery vans in the U.S. for 2022 and 2023. The U.S. fleet had 2,600 vans in 2022 and 11,800 in 2023. However, the question asks for the total across 2022 and 2023, which requires adding the annual additions. The 2023 number (11,800) is stated as 'up from more than 2,600 in 2022', indicating that 2,600 was the 2022 figure. Adding these gives 14,400. The context does not provide data for other regions' annual additions, only their 2023 totals, so only U.S. numbers are used.","14400","electric delivery vans","[""amazon2023""]","is_blank","is_blank","The context provides specific numbers for electric delivery vans in the U.S. for 2022 and 2023. The U.S. fleet had 2,600 vans in 2022 and 11,800 in 2023. However, the question asks for the total across 2022 and 2023, which requires adding the annual additions. The 2023 number (11,800) is stated as 'up from more than 2,600 in 2022', indicating that 2,600 was the 2022 figure. Adding these gives 14,400. The context does not provide data for other regions' annual additions, only their 2023 totals, so only U.S. numbers are used."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from servers and data center cooling. Specifically, it mentions 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling' (ref_id=morrison2025). This directly contradicts the statement in the question that these emissions are not included.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from servers and data center cooling. Specifically, it mentions 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling' (ref_id=morrison2025). This directly contradicts the statement in the question that these emissions are not included."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context includes Table 4 which lists energy consumption for the o3 model under the '10k input-1.5k output' prompt size as 12.222 ± 1.082 Wh. This directly answers the question about energy consumption for a long prompt.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The context includes Table 4 which lists energy consumption for the o3 model under the '10k input-1.5k output' prompt size as 12.222 ± 1.082 Wh. This directly answers the question about energy consumption for a long prompt."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. Multiple context snippets from the same source (ref_id=stone2022) confirm this date and event.","1956","year","[""stone2022""]","is_blank","is_blank","The context explicitly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. Multiple context snippets from the same source (ref_id=stone2022) confirm this date and event."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context mentions Microsoft's data centers having low on-site WUE and provides a table with WUE values for various locations, but there is no specific mention of Google's AI-dedicated data centers' average WUE in 2024.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context mentions Microsoft's data centers having low on-site WUE and provides a table with WUE values for various locations, but there is no specific mention of Google's AI-dedicated data centers' average WUE in 2024."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people do not have access to a stable supply of electricity, citing the International Energy Agency.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context explicitly states that approximately 770 million people do not have access to a stable supply of electricity, citing the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context explicitly states that for LLaMA-65B with a maximum generation length of 512 tokens, the energy per output token is about 3-4 Joules. This is mentioned in the section discussing energy per token comparisons between generation lengths 512 and 1024.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","The context explicitly states that for LLaMA-65B with a maximum generation length of 512 tokens, the energy per output token is about 3-4 Joules. This is mentioned in the section discussing energy per token comparisons between generation lengths 512 and 1024."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context snippet with ref_id=wu2021a explicitly states that converting the numerical representation from 32-bit to 16-bit reduced the overall RM2 model size by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","The context snippet with ref_id=wu2021a explicitly states that converting the numerical representation from 32-bit to 16-bit reduced the overall RM2 model size by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context states that the 5-10% GHG reduction claim originates from BCG reports, but the underlying calculations are unclear and lack scientific grounding. The BCG reports do not provide detailed calculations or global-scale translations, and applying individual project observations globally lacks scientific support.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that the 5-10% GHG reduction claim originates from BCG reports, but the underlying calculations are unclear and lack scientific grounding. The BCG reports do not provide detailed calculations or global-scale translations, and applying individual project observations globally lacks scientific support."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from ref_id=wu2021a states that training large, sparsely-activated neural networks achieves higher accuracy at lower operational energy footprint. Additionally, ref_id=shen2024 mentions that JetMoE-8B, which uses sparse activation, reduces inference computation by about 70% compared to Llama2-7B. These points support the claim that sparsely activated DNNs consume significantly less energy without sacrificing accuracy.","1","is_blank","[""wu2021a"", ""shen2024""]","is_blank","is_blank","The context from ref_id=wu2021a states that training large, sparsely-activated neural networks achieves higher accuracy at lower operational energy footprint. Additionally, ref_id=shen2024 mentions that JetMoE-8B, which uses sparse activation, reduces inference computation by about 70% compared to Llama2-7B. These points support the claim that sparsely activated DNNs consume significantly less energy without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context from ref_id=schwartz2019 explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The context from ref_id=schwartz2019 explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context explicitly states that the optimizer stage in BlackMamba sparse fine-tuning takes up to 53% of the running time when the batch size is 1. This information is provided in the execution time breakdown analysis using an NVIDIA A40-48GB GPU.","53","percent","[""xia2024""]","is_blank","is_blank","The context explicitly states that the optimizer stage in BlackMamba sparse fine-tuning takes up to 53% of the running time when the batch size is 1. This information is provided in the execution time breakdown analysis using an NVIDIA A40-48GB GPU."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the energy consumption rate for Flan-T5-xxl as 0.083 kWh per 1,000 queries. Given 1 billion queries per day, the calculation is (1,000,000,000 / 1,000) * 0.083 kWh = 83,000 kWh, which converts to 83 MWh.","83","MWh","[""luccioni2024""]","is_blank","is_blank","The context provides the energy consumption rate for Flan-T5-xxl as 0.083 kWh per 1,000 queries. Given 1 billion queries per day, the calculation is (1,000,000,000 / 1,000) * 0.083 kWh = 83,000 kWh, which converts to 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context snippet from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally, which directly answers the question.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context snippet from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally, which directly answers the question."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context snippets do not mention Microsoft contracting wind turbines for Azure AI clusters in 2023. The Amazon2023 document details Amazon's renewable energy projects, while other contexts discuss GPT-3's water consumption and general AI sustainability issues without specifying Microsoft's wind turbine contracts.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention Microsoft contracting wind turbines for Azure AI clusters in 2023. The Amazon2023 document details Amazon's renewable energy projects, while other contexts discuss GPT-3's water consumption and general AI sustainability issues without specifying Microsoft's wind turbine contracts."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49% when equity is included, which directly answers the question about the percentage attributed to R&D staff in the total development costs.","49","percent","[""cottier2024""]","is_blank","is_blank","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49% when equity is included, which directly answers the question about the percentage attributed to R&D staff in the total development costs."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI was composed of seventeen members. This information is found in multiple references from the same document (stone2022).","17","people","[""stone2022""]","is_blank","is_blank","The context explicitly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI was composed of seventeen members. This information is found in multiple references from the same document (stone2022)."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context snippet from ref_id=wu2021a states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.' This directly answers the question regarding the percentage of a client device's total carbon footprint from manufacturing.","74","percent","[""wu2021a""]","is_blank","is_blank","The context snippet from ref_id=wu2021a states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.' This directly answers the question regarding the percentage of a client device's total carbon footprint from manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context explicitly states that 'models with more parameters consume more energy, but this is not always the case' and provides an example where a model with more parameters (Phi-3 Small) consumes less energy than a smaller model (Phi-3 Mini) under certain conditions due to architectural differences affecting memory usage and batch size scalability.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context explicitly states that 'models with more parameters consume more energy, but this is not always the case' and provides an example where a model with more parameters (Phi-3 Small) consumes less energy than a smaller model (Phi-3 Mini) under certain conditions due to architectural differences affecting memory usage and batch size scalability."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the notion that they are concerned about it.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the notion that they are concerned about it."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The provided context does not mention any specific CO2 emissions data for OpenAI's API requests in January 2024. The morrison2025 paper discusses emissions from OLMo model training and development but doesn't reference OpenAI's API usage. The dodge2022 paper focuses on cloud instance carbon intensity measurement frameworks. The jegham2025 snippet mentions GPT-4o's emissions but doesn't provide January 2024 API request data.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context does not mention any specific CO2 emissions data for OpenAI's API requests in January 2024. The morrison2025 paper discusses emissions from OLMo model training and development but doesn't reference OpenAI's API usage. The dodge2022 paper focuses on cloud instance carbon intensity measurement frameworks. The jegham2025 snippet mentions GPT-4o's emissions but doesn't provide January 2024 API request data."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context snippet from ref_id=luccioni2025c explicitly states that Meta's Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. This directly answers both parts of the question.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025c explicitly states that Meta's Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. This directly answers both parts of the question."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over a 1.5-year period between 2019 and 2021. Multiple references from wu2021a confirm this figure, specifically mentioning the time frame and the multiplier.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over a 1.5-year period between 2019 and 2021. Multiple references from wu2021a confirm this figure, specifically mentioning the time frame and the multiplier."
"q079","How many miles is the Earth from the Sun?","The provided context does not mention the Earth's distance from the Sun. All references to miles in the context pertain to distances between cities (e.g., LA-NYC round trip) or emission rates per mile. There is no information about astronomical distances.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context does not mention the Earth's distance from the Sun. All references to miles in the context pertain to distances between cities (e.g., LA-NYC round trip) or emission rates per mile. There is no information about astronomical distances."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that AlphaGo, developed by Google Deepmind, defeated the human Go champion in a five-game match. Additionally, it mentions that the AlphaGo program used machine learning algorithms and search procedures to achieve this victory.","1","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that AlphaGo, developed by Google Deepmind, defeated the human Go champion in a five-game match. Additionally, it mentions that the AlphaGo program used machine learning algorithms and search procedures to achieve this victory."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly states that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. This is directly mentioned in the fernandez2025 document.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","The context explicitly states that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. This is directly mentioned in the fernandez2025 document."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This information is directly provided in the section discussing the alignment details.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context states that the entire alignment process for JetMoE-8B, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This information is directly provided in the section discussing the alignment details."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context from Table VI shows that for the 100 TPS SLO, Max-Performance selected g6e.xlarge with a total price of $2.699, while InferSave-1st selected g4dn.xlarge at $2.13. The percentage increase is calculated as ((2.699 - 2.13)/2.13)*100 ≈ 26.7%.","26.7","percent","[""kim2025""]","is_blank","is_blank","The context from Table VI shows that for the 100 TPS SLO, Max-Performance selected g6e.xlarge with a total price of $2.699, while InferSave-1st selected g4dn.xlarge at $2.13. The percentage increase is calculated as ((2.699 - 2.13)/2.13)*100 ≈ 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context from ref_id=luccioni2024 explicitly states that the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context from ref_id=luccioni2024 explicitly states that the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context snippet from luccioni2025c explicitly states that GPU energy usage for 1,000 queries ranges from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size and other factors. This directly answers the question about the range of GPU energy usage for 1,000 inference queries based on the 2025 study's appendix.","0.06-3426","Wh","[""luccioni2025c""]","is_blank","is_blank","The context snippet from luccioni2025c explicitly states that GPU energy usage for 1,000 queries ranges from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size and other factors. This directly answers the question about the range of GPU energy usage for 1,000 inference queries based on the 2025 study's appendix."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states, 'There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly contradicts the idea of a universal approach to AI ethics and sustainability.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states, 'There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly contradicts the idea of a universal approach to AI ethics and sustainability."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context mentions that the AWS us-west-2 region (Oregon) had an average carbon intensity of 297.6 grams of CO2eq per kWh in 2021. The question asks for the U.S. average mix, and the provided value from AWS's Oregon region serves as the specific example given in the documents.","297.6","kg of CO2e/KWh","[""luccioni2024""]","is_blank","is_blank","The context mentions that the AWS us-west-2 region (Oregon) had an average carbon intensity of 297.6 grams of CO2eq per kWh in 2021. The question asks for the U.S. average mix, and the provided value from AWS's Oregon region serves as the specific example given in the documents."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as a PyTorch-based framework designed for collaborative DL training in a decentralized fashion, handling peer dropouts and enabling distributed training across heterogeneous hardware. This directly answers the question about the decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions Hivemind as a PyTorch-based framework designed for collaborative DL training in a decentralized fashion, handling peer dropouts and enabling distributed training across heterogeneous hardware. This directly answers the question about the decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context mentions that the notion of transparency in AI can be expanded to encompass 'social transparency', which integrates socio-technical aspects and includes the societal and environmental footprint of AI systems. This is explicitly stated in the provided snippets from ref_id=luccioni2025b.","social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context mentions that the notion of transparency in AI can be expanded to encompass 'social transparency', which integrates socio-technical aspects and includes the societal and environmental footprint of AI systems. This is explicitly stated in the provided snippets from ref_id=luccioni2025b."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that for emotion classification, the linear model with sentence embeddings is among the top-performing models. Additionally, it explicitly mentions that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in the FKTG dataset experiments.","is_blank","is_blank","[""zschache2025""]","is_blank","is_blank","The context states that for emotion classification, the linear model with sentence embeddings is among the top-performing models. Additionally, it explicitly mentions that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in the FKTG dataset experiments."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that the LLM inference system developed in the Chen et al. (2024) paper, which uses model-attention disaggregation, is named Lamina. Specifically, it states: 'To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.'","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The context mentions that the LLM inference system developed in the Chen et al. (2024) paper, which uses model-attention disaggregation, is named Lamina. Specifically, it states: 'To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.'"
"q093","How many parameters does the largest T5 model have?","The context mentions the OLMo series of models ranging from 20 million to 13 billion active parameters. Specifically, it states 'ranging in size from 20 million to 13 billion active parameters' and 'OLMo 2 13B was trained on 5.6 trillion tokens', indicating the largest model has 13 billion parameters.","13 billion","parameters","[""morrison2025""]","is_blank","is_blank","The context mentions the OLMo series of models ranging from 20 million to 13 billion active parameters. Specifically, it states 'ranging in size from 20 million to 13 billion active parameters' and 'OLMo 2 13B was trained on 5.6 trillion tokens', indicating the largest model has 13 billion parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states in Table 1 of the hyperparameters section that JetMoE-8B has a Ptotal of 8B, which refers to the total number of parameters. Additionally, multiple sections mention that the model has 8B parameters while activating only 2B per token.","8B","parameters","[""shen2024""]","is_blank","is_blank","The context explicitly states in Table 1 of the hyperparameters section that JetMoE-8B has a Ptotal of 8B, which refers to the total number of parameters. Additionally, multiple sections mention that the model has 8B parameters while activating only 2B per token."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context snippet from ref_id=li2025b states that Google's data center water consumption increased by ∼20% from 2021 to 2022. Additionally, ref_id=luccioni2025a corroborates this with a specific mention of a 20% uptick in the same period.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=li2025b states that Google's data center water consumption increased by ∼20% from 2021 to 2022. Additionally, ref_id=luccioni2025a corroborates this with a specific mention of a 20% uptick in the same period."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context snippet from ref_id=khan2025's Table I explicitly defines 'Carbon Intensity' as 'CO₂ emissions per unit of electricity consumed' with the unit 'gCO2/kWh'. This matches the description in the question, confirming the metric's name.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","The context snippet from ref_id=khan2025's Table I explicitly defines 'Carbon Intensity' as 'CO₂ emissions per unit of electricity consumed' with the unit 'gCO2/kWh'. This matches the description in the question, confirming the metric's name."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context includes Table 2 which lists the FLOPs utilization percentages for each growth stage of FLM-101B training. The final 101B stage shows a FLOPs utilization of 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","The context includes Table 2 which lists the FLOPs utilization percentages for each growth stage of FLM-101B training. The final 101B stage shows a FLOPs utilization of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context from ref_id=cottier2024 states that the amortized hardware CapEx + energy cost for training GPT-4 is $40 million. Another context from ref_id=xia2024 mentions the training cost exceeding $100 million, but this is not specified as amortized. The question specifically asks for amortized training costs, which is addressed in cottier2024.","40000000","USD","[""cottier2024""]","is_blank","is_blank","The context from ref_id=cottier2024 states that the amortized hardware CapEx + energy cost for training GPT-4 is $40 million. Another context from ref_id=xia2024 mentions the training cost exceeding $100 million, but this is not specified as amortized. The question specifically asks for amortized training costs, which is addressed in cottier2024."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that optimization across algorithms, platforms, infrastructures, and hardware can reduce the operational carbon footprint for the Transformer-based universal translation model by 810× compared to a CPU server baseline. Figure 7 in the document explicitly shows an 810× reduction through platform-level caching, GPU acceleration, and algorithmic changes.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that optimization across algorithms, platforms, infrastructures, and hardware can reduce the operational carbon footprint for the Transformer-based universal translation model by 810× compared to a CPU server baseline. Figure 7 in the document explicitly shows an 810× reduction through platform-level caching, GPU acceleration, and algorithmic changes."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that when training NLP models across four continents (C-8 experiment), there's a 41% performance drop compared to the local experiment (A-8). This means the achieved throughput is 59% of the local throughput (100% - 41% = 59%).","0.59","multiplier","[""erben2023""]","is_blank","is_blank","The context states that when training NLP models across four continents (C-8 experiment), there's a 41% performance drop compared to the local experiment (A-8). This means the achieved throughput is 59% of the local throughput (100% - 41% = 59%)."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This figure is directly mentioned in multiple snippets from the 'amazon2023' reference.","3500000000","liters","[""amazon2023""]","is_blank","is_blank","The context explicitly states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities. This figure is directly mentioned in multiple snippets from the 'amazon2023' reference."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context states that using custom tags in prompts can reduce the energy consumption of LLMs during code completion tasks. Specifically, the study found that custom tags led to reductions in energy consumption across zero-shot, one-shot, and few-shots techniques.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context states that using custom tags in prompts can reduce the energy consumption of LLMs during code completion tasks. Specifically, the study found that custom tags led to reductions in energy consumption across zero-shot, one-shot, and few-shots techniques."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context snippet from ref_id=luccioni2025a states that NVIDIA shipped 3.7 million GPUs in 2024, which is a million more units than in 2023. This directly answers the question about the number of data center GPUs shipped in 2024.","3700000","GPUs","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a states that NVIDIA shipped 3.7 million GPUs in 2024, which is a million more units than in 2023. This directly answers the question about the number of data center GPUs shipped in 2024."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the total amortized hardware CapEx + energy cost is attributed to AI accelerator chips. This is explicitly mentioned in the section titled '3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips'.","44","percent","[""cottier2024""]","is_blank","is_blank","The context states that on average, 44% of the total amortized hardware CapEx + energy cost is attributed to AI accelerator chips. This is explicitly mentioned in the section titled '3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips'."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10. This is mentioned in multiple references, including [wu2021a] and [wu2021b], which both confirm the PUE value of 1.10 for Facebook's data centers.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10. This is mentioned in multiple references, including [wu2021a] and [wu2021b], which both confirm the PUE value of 1.10 for Facebook's data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project, which stands for Ethical AI for the Governance of the Society, and explicitly states that it proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context mentions the Finnish ETAIROS project, which stands for Ethical AI for the Governance of the Society, and explicitly states that it proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs include Google's Gemini Ultra at $30M. This figure is explicitly mentioned in the document as the estimated hardware acquisition cost for Gemini Ultra.","30000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that the most expensive publicly-announced training runs include Google's Gemini Ultra at $30M. This figure is explicitly mentioned in the document as the estimated hardware acquisition cost for Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessments for providers of GPAI models with systemic risk but notes that these provisions lack sufficient emphasis on environmental factors. However, it also argues that environmental risks should be included in these assessments based on the Act's objectives, though the practical integration remains unclear.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the AI Act mandates risk assessments for providers of GPAI models with systemic risk but notes that these provisions lack sufficient emphasis on environmental factors. However, it also argues that environmental risks should be included in these assessments based on the Act's objectives, though the practical integration remains unclear."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA's recently tightened primary standard for the annual average PM2.5 concentration is 9µg/m³, as mentioned in multiple snippets from the same source (han2024).","9","µg/m³","[""han2024""]","is_blank","is_blank","The context explicitly states that the EPA's recently tightened primary standard for the annual average PM2.5 concentration is 9µg/m³, as mentioned in multiple snippets from the same source (han2024)."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context explicitly states that a life cycle assessment found 115 print books produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","is_blank","is_blank","The context explicitly states that a life cycle assessment found 115 print books produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context explicitly states that in disadvantaged communities, the per-household health burden from air pollutants could be 200 times higher than in less-impacted communities. This is mentioned multiple times in the provided snippets from the Han et al. (2024) study.","200","multiplier","[""han2024""]","is_blank","is_blank","The context explicitly states that in disadvantaged communities, the per-household health burden from air pollutants could be 200 times higher than in less-impacted communities. This is mentioned multiple times in the provided snippets from the Han et al. (2024) study."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context snippet from ref_id=zschache2025 in Table B1 explicitly lists the energy consumption of the DS Llama 70B model for the inference task on the FKTG dataset as 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","The context snippet from ref_id=zschache2025 in Table B1 explicitly lists the energy consumption of the DS Llama 70B model for the inference task on the FKTG dataset as 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions that the OLMo series of transformer language models analyzed in the 2022 paper by Dodge et al. (referenced as OLMo et al., 2025) ranges in size from 20 million to 13 billion active parameters. This directly answers the question about the total number of parameters in the large language model they analyzed.","13B","parameters","[""morrison2025""]","is_blank","is_blank","The context mentions that the OLMo series of transformer language models analyzed in the 2022 paper by Dodge et al. (referenced as OLMo et al., 2025) ranges in size from 20 million to 13 billion active parameters. This directly answers the question about the total number of parameters in the large language model they analyzed."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context explicitly mentions Jevons' Paradox as the phenomenon where technological progress improving efficiency leads to increased usage and overall resource consumption. Both luccioni2025a and morrison2025 reference Jevons' Paradox in this context.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The context explicitly mentions Jevons' Paradox as the phenomenon where technological progress improving efficiency leads to increased usage and overall resource consumption. Both luccioni2025a and morrison2025 reference Jevons' Paradox in this context."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that training GPT-3 consumed 1287 MWh of energy (ref_id=li2025b) and that training one Meena model is equivalent to 242,231 miles driven by a car (ref_id=wu2021a). However, there is no direct comparison of energy consumption between Meena and GPT-3 in the provided context. The energy equivalence for Meena is given in miles driven, not MWh, making a direct calculation impossible without additional conversion data.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context states that training GPT-3 consumed 1287 MWh of energy (ref_id=li2025b) and that training one Meena model is equivalent to 242,231 miles driven by a car (ref_id=wu2021a). However, there is no direct comparison of energy consumption between Meena and GPT-3 in the provided context. The energy equivalence for Meena is given in miles driven, not MWh, making a direct calculation impossible without additional conversion data."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides Table 2 from the 2024 study by Luccioni et al, which lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences. This value is explicitly stated in the table and referenced multiple times in the text discussing energy consumption comparisons.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","The context provides Table 2 from the 2024 study by Luccioni et al, which lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences. This value is explicitly stated in the table and referenced multiple times in the text discussing energy consumption comparisons."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context snippet from ref_id=strubell2019 explicitly states that the estimated CO2e for an average American life in one year is 36,156 lbs.","36156","lbs","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 explicitly states that the estimated CO2e for an average American life in one year is 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context includes a table in Figure 6(c) listing the top-10 counties by per-household health cost, where Marion County, WV has the highest value at $1218.3. The question specifically asks about 2030 projections, but the provided context only includes data up to 2023 and projections for 2028. However, since the user's question mentions 'a recent paper's 2030 projections' and the context refers to 2028 projections from the same paper, it is reasonable to infer that Marion County, WV is the answer based on the existing data.","Marion","is_blank","[""han2024""]","is_blank","is_blank","The context includes a table in Figure 6(c) listing the top-10 counties by per-household health cost, where Marion County, WV has the highest value at $1218.3. The question specifically asks about 2030 projections, but the provided context only includes data up to 2023 and projections for 2028. However, since the user's question mentions 'a recent paper's 2030 projections' and the context refers to 2028 projections from the same paper, it is reasonable to infer that Marion County, WV is the answer based on the existing data."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context from ref_id=khan2025's Table III shows Mistral-small's CO2 emissions decreased from 0.020 kg to 0.015 kg per inference task after optimization. The multiplier is calculated as 0.015/0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","is_blank","The context from ref_id=khan2025's Table III shows Mistral-small's CO2 emissions decreased from 0.020 kg to 0.015 kg per inference task after optimization. The multiplier is calculated as 0.015/0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context from ref_id=ebert2024 and ref_id=luccioni2024 provides specific energy consumption figures for training and fine-tuning the BLOOMz-7B model. The training energy is 51,686 kWh and the fine-tuning energy is 7,571 kWh. Adding these together gives the combined total.","59257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","is_blank","The context from ref_id=ebert2024 and ref_id=luccioni2024 provides specific energy consumption figures for training and fine-tuning the BLOOMz-7B model. The training energy is 51,686 kWh and the fine-tuning energy is 7,571 kWh. Adding these together gives the combined total."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that the FLM-101B model is structured with 80 layers and 80 attention heads, and mentions the model's name as FLM-101B, which indicates 101 billion parameters.","101","parameters","[""li2025a""]","is_blank","is_blank","The context explicitly states that the FLM-101B model is structured with 80 layers and 80 attention heads, and mentions the model's name as FLM-101B, which indicates 101 billion parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the training energy for a 6.1B parameter model (103,500 kWh) from Dodge et al. (2022) and the inference energy (1.0 × 10⁻⁴ kWh) and cost parity (592,570,000 inferences) for BLOOMz-7B from Luccioni et al. (2024). Using BLOOMz-7B's inference energy as a comparable model, the number of inferences needed is calculated by dividing the training energy by the inference energy: 103,500 kWh / 1.0 × 10⁻⁴ kWh = 1,035,000,000 inferences.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","The context provides the training energy for a 6.1B parameter model (103,500 kWh) from Dodge et al. (2022) and the inference energy (1.0 × 10⁻⁴ kWh) and cost parity (592,570,000 inferences) for BLOOMz-7B from Luccioni et al. (2024). Using BLOOMz-7B's inference energy as a comparable model, the number of inferences needed is calculated by dividing the training energy by the inference energy: 103,500 kWh / 1.0 × 10⁻⁴ kWh = 1,035,000,000 inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that the total energy consumed for all model experimentation and evaluation in the study was 754.66 kWh. This information is found in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the provided document snippets.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The context explicitly states that the total energy consumed for all model experimentation and evaluation in the study was 754.66 kWh. This information is found in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the provided document snippets."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) from the study by Luccioni et al. (2024) that lists the 'Cost parity (# inferences)' for BLOOMz-7B as 592,570,000. This value represents the number of inferences required for the cumulative deployment energy cost to equal the initial training and fine-tuning costs.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","The context provides a table (Table 5) from the study by Luccioni et al. (2024) that lists the 'Cost parity (# inferences)' for BLOOMz-7B as 592,570,000. This value represents the number of inferences required for the cumulative deployment energy cost to equal the initial training and fine-tuning costs."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the dataset used for classifying German nuclear waste site objections is called the FKTG-dataset, which was published after processing statements from the population during the repository site selection process.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that the dataset used for classifying German nuclear waste site objections is called the FKTG-dataset, which was published after processing statements from the population during the repository site selection process."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context discusses water consumption for AI models like GPT-3 and mentions Llama-3-70B's inference energy use, but does not provide specific data on Meta's Llama 3 inference clusters' freshwater consumption in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context discusses water consumption for AI models like GPT-3 and mentions Llama-3-70B's inference energy use, but does not provide specific data on Meta's Llama 3 inference clusters' freshwater consumption in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context mentions that NVIDIA does not release information about the embodied carbon emissions or water consumption of their hardware, and the analysis assumes an H100 is 0.1% rare earth metal by mass. However, there is no mention of recycled rare earth metals being used in the manufacturing of H100 GPUs in 2024.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that NVIDIA does not release information about the embodied carbon emissions or water consumption of their hardware, and the analysis assumes an H100 is 0.1% rare earth metal by mass. However, there is no mention of recycled rare earth metals being used in the manufacturing of H100 GPUs in 2024."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","","","passengers","[]","is_blank","is_blank",""
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context snippet from ref_id=luccioni2025c explicitly states that in May 2025 data from OpenRouter28, 84% of LLM token usage occurred through models with no disclosure of environmental impact. This directly answers the question.","84","percent","[""luccioni2025c""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025c explicitly states that in May 2025 data from OpenRouter28, 84% of LLM token usage occurred through models with no disclosure of environmental impact. This directly answers the question."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states in Table II that for the LLaMA 13B model, the bare minimum hardware required is 1 A100 80GB GPU when using no compression or quantization. This directly answers the question about the minimum number of A100 80GB GPUs needed for LLaMA-13B inference.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context states in Table II that for the LLaMA 13B model, the bare minimum hardware required is 1 A100 80GB GPU when using no compression or quantization. This directly answers the question about the minimum number of A100 80GB GPUs needed for LLaMA-13B inference."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides energy consumption (103,500 kWh) for a complete training run of a 6.1B parameter model but does not specify CO2 emissions conversion factors or final emission values. While Section 5 discusses emissions calculations, no actual emission numbers or conversion rates are provided for this specific model.","is_blank","metric tons","[""is_blank""]","is_blank","is_blank","The context provides energy consumption (103,500 kWh) for a complete training run of a 6.1B parameter model but does not specify CO2 emissions conversion factors or final emission values. While Section 5 discusses emissions calculations, no actual emission numbers or conversion rates are provided for this specific model."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context mentions that quantization can reduce energy consumption and carbon emissions by up to 45%, but it does not provide a specific total amount of emissions avoided in 2023. The documents discuss the percentage reduction and the impact of optimization techniques but do not state an absolute value for the total emissions avoided.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context mentions that quantization can reduce energy consumption and carbon emissions by up to 45%, but it does not provide a specific total amount of emissions avoided in 2023. The documents discuss the percentage reduction and the impact of optimization techniques but do not state an absolute value for the total emissions avoided."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states in the 'Mixing GPU Types' section that using a combination of 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly supported by the reference griggs2024.","24","percent","[""griggs2024""]","is_blank","is_blank","The context explicitly states in the 'Mixing GPU Types' section that using a combination of 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly supported by the reference griggs2024."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context from Chen et al. (2024) provides a table listing the price per chip for NVIDIA H20 as $4.63/hr, with a note indicating this is an estimate based on system cost relative to H100.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","The context from Chen et al. (2024) provides a table listing the price per chip for NVIDIA H20 as $4.63/hr, with a note indicating this is an estimate based on system cost relative to H100."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors', which directly contradicts the claim that they gather information automatically without contacting authors. The example of Luccioni and Hernandez-Garcia reaching out to 500 authors further supports this manual process.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that 'most carbon footprint analyses gather the information manually by writing to authors', which directly contradicts the claim that they gather information automatically without contacting authors. The example of Luccioni and Hernandez-Garcia reaching out to 500 authors further supports this manual process."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the U.S. data centers' public health cost was equivalent to approximately 44% of their total electricity cost using the average attribution method.","44","percent","[""han2024""]","is_blank","is_blank","The context states that in 2023, the U.S. data centers' public health cost was equivalent to approximately 44% of their total electricity cost using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states in Table II that for the LLaMA 7B model, the bare minimum hardware required is 1 A100 80GB GPU with a maximum batch size of 64. This information is provided under the assumption of no compression or quantization, directly addressing the question's requirements.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context states in Table II that for the LLaMA 7B model, the bare minimum hardware required is 1 A100 80GB GPU with a maximum batch size of 64. This information is provided under the assumption of no compression or quantization, directly addressing the question's requirements."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context explicitly states that experimental results reveal quantization methods can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context explicitly states that experimental results reveal quantization methods can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that researchers reached out to over 500 authors and collected 95 answers.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","The context states that researchers reached out to over 500 authors and collected 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget and 30,000 H100 GPU hours. To calculate the cost per GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 hours = approximately $3.33 per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B was trained with a $100k budget and 30,000 H100 GPU hours. To calculate the cost per GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 hours = approximately $3.33 per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost. This is explicitly shown in Table 2 under the entry for Altoona, IA, where the '% of Electricity Cost' column lists 122%.","122","percent","[""han2024""]","is_blank","is_blank","The context states that when training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost. This is explicitly shown in Table 2 under the entry for Altoona, IA, where the '% of Electricity Cost' column lists 122%."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states in multiple sections that JetMoE-8B was trained on 1.25T tokens. Specifically, the 'Training Data Mixture' section and the abstract mention this figure.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The context explicitly states in multiple sections that JetMoE-8B was trained on 1.25T tokens. Specifically, the 'Training Data Mixture' section and the abstract mention this figure."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the number of projects in the United Kingdom as 36.","36","projects","[""amazon2023""]","is_blank","is_blank","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the number of projects in the United Kingdom as 36."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context includes a table under 'Amazon Representation by the Numbers' for the U.S. in 2023, which lists the percentage of men in the Amazon Workforce (All Levels) as 31.1%.","31.1","percent","[""amazon2023""]","is_blank","is_blank","The context includes a table under 'Amazon Representation by the Numbers' for the U.S. in 2023, which lists the percentage of men in the Amazon Workforce (All Levels) as 31.1%."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context snippet from ref_id=li2025b explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","The context snippet from ref_id=li2025b explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context mentions that for BlackMamba sparse fine-tuning with a batch size of 84, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for this configuration is approximately 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","is_blank","The context mentions that for BlackMamba sparse fine-tuning with a batch size of 84, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for this configuration is approximately 2.0 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context mentions that the authors introduced the 'granularity metric' which is defined as the ratio of calculation to communication time. This metric is used to assess model suitability for distributed spot training and predict performance with different hardware setups.","granularity metric","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that the authors introduced the 'granularity metric' which is defined as the ratio of calculation to communication time. This metric is used to assess model suitability for distributed spot training and predict performance with different hardware setups."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated the Exxon Mobil deal could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets. This percentage directly answers the question about the multiplier effect.","640","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","is_blank","The context states that a coalition of Microsoft employees estimated the Exxon Mobil deal could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets. This percentage directly answers the question about the multiplier effect."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses such as agricultural, industrial, or municipal purposes. This directly matches the question's description.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The context defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses such as agricultural, industrial, or municipal purposes. This directly matches the question's description."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context explicitly states that the Standing Committee forms a Study Panel every five years as part of its core activity to assess the current state of AI.","5","years","[""stone2022""]","is_blank","is_blank","The context explicitly states that the Standing Committee forms a Study Panel every five years as part of its core activity to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that in the US, the average household has an average of 25 connected devices as reported by Deloitte in 2021. This information is mentioned in multiple snippets from the same reference (wu2021b).","25","devices","[""wu2021b""]","is_blank","is_blank","The context explicitly states that in the US, the average household has an average of 25 connected devices as reported by Deloitte in 2021. This information is mentioned in multiple snippets from the same reference (wu2021b)."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context from ref_id=luccioni2025c explicitly states the range of energy consumption for pre-training LLMs as 0.8 MWh to 3,500 MWh, citing specific models. Additionally, ref_id=dodge2022 provides an example of a 6B parameter model requiring 103.5 MWh when fully trained, which falls within the broader range mentioned in luccioni2025c.","0.8-3500","MWh","[""luccioni2025c"", ""dodge2022""]","is_blank","is_blank","The context from ref_id=luccioni2025c explicitly states the range of energy consumption for pre-training LLMs as 0.8 MWh to 3,500 MWh, citing specific models. Additionally, ref_id=dodge2022 provides an example of a 6B parameter model requiring 103.5 MWh when fully trained, which falls within the broader range mentioned in luccioni2025c."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This directly contradicts the statement in the question that Watson did NOT beat human contenders, making the correct answer False.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This directly contradicts the statement in the question that Watson did NOT beat human contenders, making the correct answer False."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from ref_id=luccioni2025a states that one paper suggests 10–50 queries on GPT-3 consume around half a liter of water. This directly answers the question by providing the range of queries (10–50) associated with the specified water consumption.","10–50","queries","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a states that one paper suggests 10–50 queries on GPT-3 consume around half a liter of water. This directly answers the question by providing the range of queries (10–50) associated with the specified water consumption."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context explicitly states that JetMoE-8B-Chat achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment. Table 4 in the context lists the MT-Bench scores, showing JetMoE-8B-chat at 6.681 and Llama-2-13b-chat at 6.650.","6.681","score","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B-Chat achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment. Table 4 in the context lists the MT-Bench scores, showing JetMoE-8B-chat at 6.681 and Llama-2-13b-chat at 6.650."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to consume a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment conditions.","10–50","responses","[""li2025b""]","is_blank","is_blank","The context states that GPT-3 needs to consume a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment conditions."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context snippets from the Griggs et al. 2024 paper explicitly state that Mélange reduces deployment costs by up to 77% in conversational settings. This is mentioned multiple times in different sections of the paper, including the abstract, evaluation results, and conclusion.","77","percent","[""griggs2024""]","is_blank","is_blank","The context snippets from the Griggs et al. 2024 paper explicitly state that Mélange reduces deployment costs by up to 77% in conversational settings. This is mentioned multiple times in different sections of the paper, including the abstract, evaluation results, and conclusion."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in multiple sections of the samsi2024 document and also referenced in rubei2025.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in multiple sections of the samsi2024 document and also referenced in rubei2025."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context from multiple references (han2024 and luccioni2025c) explicitly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","is_blank","The context from multiple references (han2024 and luccioni2025c) explicitly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from ref_id=luccioni2024 states that according to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand. Additionally, ref_id=fernandez2025 and ref_id=chung2025 corroborate this range by mentioning AWS cloud computing demand for ML inference as 80-90%.","80-90","percent","[""luccioni2024"", ""fernandez2025"", ""chung2025""]","is_blank","is_blank","The context from ref_id=luccioni2024 states that according to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand. Additionally, ref_id=fernandez2025 and ref_id=chung2025 corroborate this range by mentioning AWS cloud computing demand for ML inference as 80-90%."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context states that for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted. This directly answers the question about the total CO2 equivalent emissions generated in the study.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The context states that for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted. This directly answers the question about the total CO2 equivalent emissions generated in the study."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that estimations using TDP are nearly always overestimations because GPUs rarely draw maximum power continuously. It also mentions that such estimations can lead to significant overestimation factors, up to 4.1x in some cases.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context states that estimations using TDP are nearly always overestimations because GPUs rarely draw maximum power continuously. It also mentions that such estimations can lead to significant overestimation factors, up to 4.1x in some cases."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries and that its consumption is slightly higher (3.098 Wh vs. 2.875 Wh) due to older hardware deployment.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries and that its consumption is slightly higher (3.098 Wh vs. 2.875 Wh) due to older hardware deployment."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context includes a figure (Fig. 8) showing the throughput of Mixtral-CS-A100-40GB with different batch sizes. Under 'Dense(bsz=1)', the corresponding value is 0.3 queries/sec, which directly answers the question.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","The context includes a figure (Fig. 8) showing the throughput of Mixtral-CS-A100-40GB with different batch sizes. Under 'Dense(bsz=1)', the corresponding value is 0.3 queries/sec, which directly answers the question."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that direct environmental disclosures peaked in 2022 with 10% of notable models that year releasing some degree of information. However, after 2022, the introduction of commercial and proprietary models led to a notable reversal, reducing direct disclosures. By Q1 2025, most models fell under 'no disclosure'.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that direct environmental disclosures peaked in 2022 with 10% of notable models that year releasing some degree of information. However, after 2022, the introduction of commercial and proprietary models led to a notable reversal, reducing direct disclosures. By Q1 2025, most models fell under 'no disclosure'."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context from Griggs et al. (2024) includes a table (Table 1) that lists the on-demand prices for four GPU types, including the H100 at $7.5164 per hour. This directly answers the question about the normalized on-demand hourly price for an H100 GPU.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","The context from Griggs et al. (2024) includes a table (Table 1) that lists the on-demand prices for four GPU types, including the H100 at $7.5164 per hour. This directly answers the question about the normalized on-demand hourly price for an H100 GPU."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context snippets from ref_id=li2025b mention that GPT-3's training consumed millions of liters of water, with specific values in Table 1 showing 'Water for Training' ranging from 0.708 to 15.285 million liters depending on location. Additionally, ref_id=jegham2025 states that GPT-3 required over 700 kiloliters (700,000 liters) for cooling alone. However, the question specifically asks about GPT-4, and the context does not provide direct data on GPT-4's water usage, only mentioning GPT-3 and GPT-4o's projected annual water consumption for inference. Since the question refers to GPT-4, which is not explicitly covered in the provided context, the answer cannot be confidently determined.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context snippets from ref_id=li2025b mention that GPT-3's training consumed millions of liters of water, with specific values in Table 1 showing 'Water for Training' ranging from 0.708 to 15.285 million liters depending on location. Additionally, ref_id=jegham2025 states that GPT-3 required over 700 kiloliters (700,000 liters) for cooling alone. However, the question specifically asks about GPT-4, and the context does not provide direct data on GPT-4's water usage, only mentioning GPT-3 and GPT-4o's projected annual water consumption for inference. Since the question refers to GPT-4, which is not explicitly covered in the provided context, the answer cannot be confidently determined."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context from griggs2024 states that serving Llama2-70b with 2 NVIDIA A100 GPUs costs over $5,200 per month. To calculate the hourly cost, divide $5,200 by 30 days and 24 hours: 5200 / (30*24) ≈ $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context from griggs2024 states that serving Llama2-70b with 2 NVIDIA A100 GPUs costs over $5,200 per month. To calculate the hourly cost, divide $5,200 by 30 days and 24 hours: 5200 / (30*24) ≈ $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that increasing the BLEU score from 5 to 40 for GPT-3-based language translation requires a model 1,000× larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context explicitly states that increasing the BLEU score from 5 to 40 for GPT-3-based language translation requires a model 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","","","miles","[]","is_blank","is_blank",""
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context states that the BLOOMz-7B model requires 1.0 × 10⁻⁴ kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy is calculated as 606,096,000,000 inferences × 1.0 × 10⁻⁴ kWh/inference = 60,609,600 kWh, which converts to 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","is_blank","The context states that the BLOOMz-7B model requires 1.0 × 10⁻⁴ kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy is calculated as 606,096,000,000 inferences × 1.0 × 10⁻⁴ kWh/inference = 60,609,600 kWh, which converts to 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is mentioned in multiple sections of the provided document snippets from ref_id=shen2024.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is mentioned in multiple sections of the provided document snippets from ref_id=shen2024."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is explicitly mentioned in multiple sections of the Cottier et al. (2024) document, including the abstract and section 3.2.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is explicitly mentioned in multiple sections of the Cottier et al. (2024) document, including the abstract and section 3.2."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context snippets from ref_id=li2025b mention that GPT-3's training energy was estimated at 1287 MWh, but there is no mention of floating point operations (FLOPS) in any of the provided documents. Other references discuss costs, energy, or water consumption but not FLOPS.","is_blank","FLOPS","[""is_blank""]","is_blank","is_blank","The context snippets from ref_id=li2025b mention that GPT-3's training energy was estimated at 1287 MWh, but there is no mention of floating point operations (FLOPS) in any of the provided documents. Other references discuss costs, energy, or water consumption but not FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that, at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in both the samsi2024 and rubei2025 documents.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context explicitly states that, at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in both the samsi2024 and rubei2025 documents."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context from li2025a provides the necessary data to calculate the total computational work for the 101B training stage. Table 2 shows that the 101B stage used 192 GPUs (12 Data Parallel Size × 16 GPUs per entry) with a throughput of 165 teraFLOPs/sec per GPU. The training duration can be derived from the 21.54 days total training time, but the exact duration for the 101B stage is not explicitly provided. However, Table 4 lists the estimated floating-point operations for FLM-101B as 28.22 zettaFLOPs, which directly answers the question.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context from li2025a provides the necessary data to calculate the total computational work for the 101B training stage. Table 2 shows that the 101B stage used 192 GPUs (12 Data Parallel Size × 16 GPUs per entry) with a throughput of 165 teraFLOPs/sec per GPU. The training duration can be derived from the 21.54 days total training time, but the exact duration for the 101B stage is not explicitly provided. However, Table 4 lists the estimated floating-point operations for FLM-101B as 28.22 zettaFLOPs, which directly answers the question."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions that AlexNet (2012) has more parameters than ResNet but lower accuracy and dramatically less FPO. However, the exact top-1 accuracy for AlexNet on ImageNet is not explicitly provided in the given context snippets. The snippets discuss ResNet models and other architectures but do not state AlexNet's specific accuracy.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that AlexNet (2012) has more parameters than ResNet but lower accuracy and dramatically less FPO. However, the exact top-1 accuracy for AlexNet on ImageNet is not explicitly provided in the given context snippets. The snippets discuss ResNet models and other architectures but do not state AlexNet's specific accuracy."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 GPUs, so 24 servers × 8 GPUs per server equals 192 total A800 GPUs.","192","GPUs","[""li2025a""]","is_blank","is_blank","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 GPUs, so 24 servers × 8 GPUs per server equals 192 total A800 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","","","lifetimes","[]","is_blank","is_blank",""
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context snippet from ref_id=schwartz2019 explicitly states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The context snippet from ref_id=schwartz2019 explicitly states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context explicitly states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This figure is directly mentioned in the provided snippets from the Amazon 2023 Sustainability Report.","47400","metric tons","[""amazon2023""]","is_blank","is_blank","The context explicitly states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This figure is directly mentioned in the provided snippets from the Amazon 2023 Sustainability Report."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions the use of Megatron-LM for efficient large-scale language model training on GPU clusters across multiple references. Specifically, ref_id=shen2024 and ref_id=fernandez2025 explicitly state that Megatron-LM was used for this purpose.","Megatron-LM","is_blank","[""shen2024"", ""fernandez2025""]","is_blank","is_blank","The context mentions the use of Megatron-LM for efficient large-scale language model training on GPU clusters across multiple references. Specifically, ref_id=shen2024 and ref_id=fernandez2025 explicitly state that Megatron-LM was used for this purpose."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context from zschache2025's Table B2 shows that when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95. The 'single' configuration consumed 48.60 Wh, while the 'double' configuration consumed 94.88 Wh, resulting in a ratio of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","The context from zschache2025's Table B2 shows that when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95. The 'single' configuration consumed 48.60 Wh, while the 'double' configuration consumed 94.88 Wh, resulting in a ratio of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context from ref_id=li2025b's Table 1 provides water consumption per request for GPT-3, showing values like 16.904 mL per request. The context from ref_id=luccioni2025a states that 10–50 queries on GPT-3 consume around half a liter (500 mL) of water, which aligns with approximately 10-50 queries per 500 mL. However, the question specifically asks about 2023, and the context refers to GPT-3 and GPT-4o in 2025. There is no direct mention of 2023 user sessions, leading to insufficient evidence.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context from ref_id=li2025b's Table 1 provides water consumption per request for GPT-3, showing values like 16.904 mL per request. The context from ref_id=luccioni2025a states that 10–50 queries on GPT-3 consume around half a liter (500 mL) of water, which aligns with approximately 10-50 queries per 500 mL. However, the question specifically asks about 2023, and the context refers to GPT-3 and GPT-4o in 2025. There is no direct mention of 2023 user sessions, leading to insufficient evidence."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that scaling 700 million daily GPT-4o queries results in annual electricity consumption comparable to 35,000 U.S. residential households. This is directly mentioned in the abstract and in section 6.2 of the document.","35000","homes","[""jegham2025""]","is_blank","is_blank","The context states that scaling 700 million daily GPT-4o queries results in annual electricity consumption comparable to 35,000 U.S. residential households. This is directly mentioned in the abstract and in section 6.2 of the document."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context snippet from ref_id=luccioni2025a explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, which directly answers the question.","34","percent","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022, which directly answers the question."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context from morrison2025 states that the Augusta data center in Iowa had a trailing twelve-month average PUE of 1.12. The Evolved Transformer experiments were run in the Augusta cluster located in Iowa.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","The context from morrison2025 states that the Augusta data center in Iowa had a trailing twelve-month average PUE of 1.12. The Evolved Transformer experiments were run in the Augusta cluster located in Iowa."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context explicitly states that a recent analysis projects approximately 772 billion GPT-4o queries in 2025, considering growth patterns from January to December 2025.","772000000000","queries","[""jegham2025""]","is_blank","is_blank","The context explicitly states that a recent analysis projects approximately 772 billion GPT-4o queries in 2025, considering growth patterns from January to December 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context states in Table 3 that JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0, which is the final average score for the model on the OpenLLM Leaderboard benchmark suite.","53.0","score","[""shen2024""]","is_blank","is_blank","The context states in Table 3 that JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0, which is the final average score for the model on the OpenLLM Leaderboard benchmark suite."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific data on the number of AI training runs conducted globally on renewable-only power in 2022. While several references discuss renewable energy's role in reducing AI's carbon footprint and mention training runs' environmental impacts, none quantify the exact number of such runs for the specified year.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on the number of AI training runs conducted globally on renewable-only power in 2022. While several references discuss renewable energy's role in reducing AI's carbon footprint and mention training runs' environmental impacts, none quantify the exact number of such runs for the specified year."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that there is a proposal to eliminate the open-source exemption from reporting obligations under the AI Act, arguing that open-source models should adhere to the same reporting standards as proprietary models. This indicates that currently, open-source models are exempt unless systemic risk is considered, but the proposal seeks to remove this exemption.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that there is a proposal to eliminate the open-source exemption from reporting obligations under the AI Act, arguing that open-source models should adhere to the same reporting standards as proprietary models. This indicates that currently, open-source models are exempt unless systemic risk is considered, but the proposal seeks to remove this exemption."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context mentions that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but does not provide specific data for the US national average in 2020. The provided references [74] and [26] are cited for global and EU averages in 2023, not US 2020 data.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context mentions that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU, but does not provide specific data for the US national average in 2020. The provided references [74] and [26] are cited for global and EU averages in 2023, not US 2020 data."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context explicitly states that for the OPT-2.7B model running on a g4dn.xlarge instance, when the batch size increases to 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","is_blank","The context explicitly states that for the OPT-2.7B model running on a g4dn.xlarge instance, when the batch size increases to 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context explicitly states that for the four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is directly mentioned in multiple sections of the provided context snippets.","29-49","percent","[""cottier2024""]","is_blank","is_blank","The context explicitly states that for the four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is directly mentioned in multiple sections of the provided context snippets."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the energy consumption during inference was measured using the CodeCarbon package in multiple instances (zschache2025, morrison2025, rubei2025). Specifically, zschache2025 states 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package' and morrison2025 notes 'we measure cumulative energy consumption using CodeCarbon tracking'. Additionally, rubei2025 also uses CodeCarbon for energy measurement during inference.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""rubei2025""]","is_blank","is_blank","The context mentions that the energy consumption during inference was measured using the CodeCarbon package in multiple instances (zschache2025, morrison2025, rubei2025). Specifically, zschache2025 states 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package' and morrison2025 notes 'we measure cumulative energy consumption using CodeCarbon tracking'. Additionally, rubei2025 also uses CodeCarbon for energy measurement during inference."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles, 53% cited the 3 Wh per query or the '10 times more than a Google search' estimate.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles, 53% cited the 3 Wh per query or the '10 times more than a Google search' estimate."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly mentions the Compute Time Calibration Function (CTCF) as the function proposed to adjust for discrepancies between theoretical and actual GPU performance, thereby improving instance selection accuracy.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The context explicitly mentions the Compute Time Calibration Function (CTCF) as the function proposed to adjust for discrepancies between theoretical and actual GPU performance, thereby improving instance selection accuracy."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards for LLaMA-65B leads to higher energy per second (watts) and energy per decoded token. Specifically, it mentions that 'increasing the number of shards always increases the wattage' and 'energy per second increases with the number of shards even at the same batch size'. Additionally, Figure 8 and Figure 9 in the context show energy per response estimates across shard configurations, supporting the conclusion that more shards increase energy costs per response.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context states that increasing the number of shards for LLaMA-65B leads to higher energy per second (watts) and energy per decoded token. Specifically, it mentions that 'increasing the number of shards always increases the wattage' and 'energy per second increases with the number of shards even at the same batch size'. Additionally, Figure 8 and Figure 9 in the context show energy per response estimates across shard configurations, supporting the conclusion that more shards increase energy costs per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes 11 kL of water. For an H100 GPU that is 0.1% rare earth metal by mass, the calculation results in 0.0022 kL (2.2 liters) of water consumption per GPU, as explicitly mentioned in the document.","0.0022","kL","[""morrison2025""]","is_blank","is_blank","The context states that mining 1 kg of rare earth materials consumes 11 kL of water. For an H100 GPU that is 0.1% rare earth metal by mass, the calculation results in 0.0022 kL (2.2 liters) of water consumption per GPU, as explicitly mentioned in the document."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses a policy proposal to eliminate the open-source exemption from reporting obligations, indicating that under current EU rules, open-source models are exempt. The proposal suggests removing this exemption, implying that currently, open-source models do not have to report energy consumption.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context discusses a policy proposal to eliminate the open-source exemption from reporting obligations, indicating that under current EU rules, open-source models are exempt. The proposal suggests removing this exemption, implying that currently, open-source models do not have to report energy consumption."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippets from luccioni2025a and wu2021b both explicitly state that Amazon, Microsoft, Meta (Facebook), and Google accounted for almost 30% of all corporate PPAs purchased worldwide in 2020.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context snippets from luccioni2025a and wu2021b both explicitly state that Amazon, Microsoft, Meta (Facebook), and Google accounted for almost 30% of all corporate PPAs purchased worldwide in 2020."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context explicitly states that U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023 using the average attribution method.","6.7","USD","[""han2024""]","is_blank","is_blank","The context explicitly states that U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023 using the average attribution method."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides energy consumption data for various models in Table 4. For long prompts (10k input-1.5k output), GPT-4.1 nano consumes 0.827 Wh, while the o3 model consumes 12.222 Wh. Dividing 12.222 by 0.827 gives approximately 14.78, indicating the o3 model's energy consumption is about 14.8 times greater.","14.8","multiplier","[""jegham2025""]","is_blank","is_blank","The context provides energy consumption data for various models in Table 4. For long prompts (10k input-1.5k output), GPT-4.1 nano consumes 0.827 Wh, while the o3 model consumes 12.222 Wh. Dividing 12.222 by 0.827 gives approximately 14.78, indicating the o3 model's energy consumption is about 14.8 times greater."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context explicitly states that for the short-context dataset (Arena) with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. This information is directly provided in the 'Short-context Dataset (Arena)' section of the Cost Savings Analysis.","15-77","percent","[""griggs2024""]","is_blank","is_blank","The context explicitly states that for the short-context dataset (Arena) with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. This information is directly provided in the 'Short-context Dataset (Arena)' section of the Cost Savings Analysis."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context includes Table 3 which lists the net carbon emissions (net tCO2e) for FLM-101B as 26 metric tons of CO2 equivalent.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The context includes Table 3 which lists the net carbon emissions (net tCO2e) for FLM-101B as 26 metric tons of CO2 equivalent."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context mentions that for a sparse Mixtral model on the MATH dataset with a batch size of 1, the throughput is 0.3 queries per second. Additionally, it states that fine-tuning generally takes less than ten epochs to reach peak accuracy. However, the exact number of queries or data size is not provided, making it impossible to calculate the total execution time in seconds.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context mentions that for a sparse Mixtral model on the MATH dataset with a batch size of 1, the throughput is 0.3 queries per second. Additionally, it states that fine-tuning generally takes less than ten epochs to reach peak accuracy. However, the exact number of queries or data size is not provided, making it impossible to calculate the total execution time in seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is 'highly unevenly distributed across different counties and communities' and 'disproportionately affecting certain (often low-income) communities'. Additionally, it mentions that health costs are 'not evenly distributed' with per-household impacts varying by up to 200 times between communities.","0","is_blank","[""han2024""]","is_blank","is_blank","The context explicitly states that the public health impact of AI is 'highly unevenly distributed across different counties and communities' and 'disproportionately affecting certain (often low-income) communities'. Additionally, it mentions that health costs are 'not evenly distributed' with per-household impacts varying by up to 200 times between communities."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet from ref_id=wu2021b states that GPU theoretical performance per watt doubles every 3-4 years, citing Sun et al., 2019. This directly supports the statement in the question regarding 2019 product data.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b states that GPU theoretical performance per watt doubles every 3-4 years, citing Sun et al., 2019. This directly supports the statement in the question regarding 2019 product data."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that in the financial sentiment case study, the framework applies 4-bit quantization through Ollama, an open-source platform known for supporting edge computing and privacy-centric deployments. This tool is specifically used for local AI model deployment, aligning with the requirements of the question.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The context mentions that in the financial sentiment case study, the framework applies 4-bit quantization through Ollama, an open-source platform known for supporting edge computing and privacy-centric deployments. This tool is specifically used for local AI model deployment, aligning with the requirements of the question."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that when running experiments on spot instances which can terminate anytime, they used an independent S3 storage provider, Backblaze (B2), accessed via shards in the tar format with the WebDataset library. This indicates that Backblaze B2 storage was used for sharding and streaming datasets.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that when running experiments on spot instances which can terminate anytime, they used an independent S3 storage provider, Backblaze (B2), accessed via shards in the tar format with the WebDataset library. This indicates that Backblaze B2 storage was used for sharding and streaming datasets."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context from ref_id=zschache2025 states there is a 'strong correlation between inference energy consumption and model runtime', indicating a nearly linear relationship as runtime can serve as a proxy for energy use.","1","is_blank","[""zschache2025""]","is_blank","is_blank","The context from ref_id=zschache2025 states there is a 'strong correlation between inference energy consumption and model runtime', indicating a nearly linear relationship as runtime can serve as a proxy for energy use."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Edward J. Markey (D-MA)","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context from chen2024 includes a table listing the price per chip for NVIDIA H100 as $11.06/hr. The table is explicitly referenced in the context, and the price is provided under the 'Price per chip [2]' row for H100.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","The context from chen2024 includes a table listing the price per chip for NVIDIA H100 as $11.06/hr. The table is explicitly referenced in the context, and the price is provided under the 'Price per chip [2]' row for H100."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context from ref_id=amazon2023 states that in February 2024, AWS announced the average expected server lifetime improved from five to six years. However, the question specifically asks about GPU lifetime in AI data centers. The context mentions servers but does not explicitly state GPU lifetime. Another context (ref_id=cottier2024) discusses hardware failures and mentions an expected hardware lifetime of 3.7 years under catastrophic failure scenarios, but this is not directly about retirement in AI data centers. The provided contexts do not clearly support the GPU lifetime estimate requested.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context from ref_id=amazon2023 states that in February 2024, AWS announced the average expected server lifetime improved from five to six years. However, the question specifically asks about GPU lifetime in AI data centers. The context mentions servers but does not explicitly state GPU lifetime. Another context (ref_id=cottier2024) discusses hardware failures and mentions an expected hardware lifetime of 3.7 years under catastrophic failure scenarios, but this is not directly about retirement in AI data centers. The provided contexts do not clearly support the GPU lifetime estimate requested."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for the 13B LLaMA model, the bare minimum hardware required is 2 V100 32GB GPUs according to Table II in the samsi2024 document.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context states that for the 13B LLaMA model, the bare minimum hardware required is 2 V100 32GB GPUs according to Table II in the samsi2024 document."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reported GHG emissions of 1247.61 tons CO2e for the Gemma family, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e.","1247.61 tCO2e, 4x","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context states that Google reported GHG emissions of 1247.61 tons CO2e for the Gemma family, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","is_blank","is_blank","hours","[""is_blank""]","is_blank","is_blank","is_blank"
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context explicitly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh according to reference [8] in the document li2025b.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context explicitly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh according to reference [8] in the document li2025b."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context snippets from ref_id=wu2021b and ref_id=dodge2022 both explicitly state that Google's hyperscale data centers achieved a PUE of 1.10 in 2021. The first reference mentions the improvement from 1.21 in 2008 to 1.10 in 2021, while the second reference confirms the 1.10 PUE for the 12 months ending in Q1 2021.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","The context snippets from ref_id=wu2021b and ref_id=dodge2022 both explicitly state that Google's hyperscale data centers achieved a PUE of 1.10 in 2021. The first reference mentions the improvement from 1.21 in 2008 to 1.10 in 2021, while the second reference confirms the 1.10 PUE for the 12 months ending in Q1 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when using 100% renewable energy, which was achieved in 2023.","96","percent","[""amazon2023""]","is_blank","is_blank","The context states that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when using 100% renewable energy, which was achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model with 2 million queries using an NVIDIA H100 GPU has a net cost of $3460. This is mentioned in multiple sections of the document, including the example provided in the introduction and the conclusions.","3460","USD","[""xia2024""]","is_blank","is_blank","The context explicitly states that fine-tuning a sparse Mixtral model with 2 million queries using an NVIDIA H100 GPU has a net cost of $3460. This is mentioned in multiple sections of the document, including the example provided in the introduction and the conclusions."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context from ref_id=dodge2022 states that in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU. Specifically, Table 1 shows the GPU accounts for 74% of the total energy consumption in a training experiment. Additionally, it mentions that GPUs account for almost 3/4 of electricity consumption, which aligns with 74%.","74","percent","[""dodge2022""]","is_blank","is_blank","The context from ref_id=dodge2022 states that in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU. Specifically, Table 1 shows the GPU accounts for 74% of the total energy consumption in a training experiment. Additionally, it mentions that GPUs account for almost 3/4 of electricity consumption, which aligns with 74%."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that the training infrastructure for JetMoE-8B involved a cluster of 12 nodes and 96 H100 GPUs. This directly answers the question about the total number of H100 GPUs used.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The context states that the training infrastructure for JetMoE-8B involved a cluster of 12 nodes and 96 H100 GPUs. This directly answers the question about the total number of H100 GPUs used."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context explicitly states that during active training, the average GPU power for a single node is over 600W, which directly answers the question about the first 300 logging steps of OLMo 2 7B training.","600","Watts","[""morrison2025""]","is_blank","is_blank","The context explicitly states that during active training, the average GPU power for a single node is over 600W, which directly answers the question about the first 300 logging steps of OLMo 2 7B training."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context snippet from ref_id=strubell2019 explicitly lists 'Human life, avg, 1 year' with a CO2e value of 11,023 lbs in Table 1.","11023","lbs","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 explicitly lists 'Human life, avg, 1 year' with a CO2e value of 11,023 lbs in Table 1."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B and 13B models, there was a 1.25 times increase in inference latency on the A100 compared to the V100. Since latency improvement inversely relates to throughput speedup, a 1.25x latency improvement would correspond to a 1.25x throughput speedup.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The context states that for the smaller LLaMA 7B and 13B models, there was a 1.25 times increase in inference latency on the A100 compared to the V100. Since latency improvement inversely relates to throughput speedup, a 1.25x latency improvement would correspond to a 1.25x throughput speedup."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states multiple times that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly mentioned in sections 5.3 Validation Against Public Disclosures and 6.1 Energy Cost of a Single GPT-4o User Session, as well as in Figure 5.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context explicitly states multiple times that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly mentioned in sections 5.3 Validation Against Public Disclosures and 6.1 Energy Cost of a Single GPT-4o User Session, as well as in Figure 5."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with a 400 TPS SLO, Max-Performance's choice (g6e.xlarge) was about 280% more expensive than InferSave's top choice (g4dn.xlarge).","280","percent","[""kim2025""]","is_blank","is_blank","The context states that with a 400 TPS SLO, Max-Performance's choice (g6e.xlarge) was about 280% more expensive than InferSave's top choice (g4dn.xlarge)."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context mentions that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly addresses the question about energy-efficient GPU architectures for single classification token generation.","V100 or A30 GPU","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference. This directly addresses the question about energy-efficient GPU architectures for single classification token generation."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context from ref_id=schwartz2019 explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice. This directly supports the statement in the question.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context from ref_id=schwartz2019 explicitly states that reporting the computational price tag of finding, training, and running models is a key Green AI practice. This directly supports the statement in the question."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context snippet from ref_id=luccioni2025a explicitly states that global electronic waste (e-waste) reached 62 million tonnes in 2022. This directly answers the question about the total amount of e-waste generated worldwide in that year.","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a explicitly states that global electronic waste (e-waste) reached 62 million tonnes in 2022. This directly answers the question about the total amount of e-waste generated worldwide in that year."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context does not provide specific average system power per processor values for TPU v2 and V100 GPU. While some power ratings (TDP) are mentioned for other GPUs and TPUs, there is no direct comparison between TPU v2 and V100 GPU in terms of average system power per processor.","is_blank","Watts","[""is_blank""]","is_blank","is_blank","The context does not provide specific average system power per processor values for TPU v2 and V100 GPU. While some power ratings (TDP) are mentioned for other GPUs and TPUs, there is no direct comparison between TPU v2 and V100 GPU in terms of average system power per processor."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is explicitly mentioned in the abstract and later referenced in the case study section.","700000","liters","[""li2025b""]","is_blank","is_blank","The context states that training the GPT-3 model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is explicitly mentioned in the abstract and later referenced in the case study section."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. For example, one snippet mentions 'Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021' and another reiterates 'Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11]'.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The context explicitly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. For example, one snippet mentions 'Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021' and another reiterates 'Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11]'."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context from jegham2025 states that in the Cross-efficiency DEA Results, o3-mini achieved the highest cross-efficiency score of 0.884. This directly answers the question about which model ranked highest in the recent eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","The context from jegham2025 states that in the Cross-efficiency DEA Results, o3-mini achieved the highest cross-efficiency score of 0.884. This directly answers the question about which model ranked highest in the recent eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context snippet from ref_id=wu2021b states that cell phones currently have average lifetimes of less than 3 years, which contributes to e-waste concerns. This directly supports the assertion in the question.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b states that cell phones currently have average lifetimes of less than 3 years, which contributes to e-waste concerns. This directly supports the assertion in the question."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context discusses intra-zone scaling with T4 GPUs in Section 4 (GEO-DISTRIBUTED PERFORMANCE), specifically in the (A) Intra-zone scalability experiments. It states that CV models achieved almost linear per-GPU speedup (0.43, 0.42, 0.43, 0.41, 0.41 for 2,3,4,6,8 GPUs), while NLP models showed a decreasing trend. The question specifically asks about CV models, which the context supports as achieving nearly linear speedup with T4 GPUs.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context discusses intra-zone scaling with T4 GPUs in Section 4 (GEO-DISTRIBUTED PERFORMANCE), specifically in the (A) Intra-zone scalability experiments. It states that CV models achieved almost linear per-GPU speedup (0.43, 0.42, 0.43, 0.41, 0.41 for 2,3,4,6,8 GPUs), while NLP models showed a decreasing trend. The question specifically asks about CV models, which the context supports as achieving nearly linear speedup with T4 GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","is_blank","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and does not fully utilize GPU compute resources, leading to lower power draw compared to diffusion models which are compute-intensive and reach near-maximum GPU power consumption. Specifically, it mentions that diffusion models consume nearly maximum GPU power when batch size is not small, while LLMs are bottlenecked by VRAM bandwidth.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context states that LLM decoding is memory-intensive and does not fully utilize GPU compute resources, leading to lower power draw compared to diffusion models which are compute-intensive and reach near-maximum GPU power consumption. Specifically, it mentions that diffusion models consume nearly maximum GPU power when batch size is not small, while LLMs are bottlenecked by VRAM bandwidth."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context includes a table under 'People Managers' with gender data for 2023. The 'Women' percentage is listed as 31.6% globally.","31.6","percent","[""amazon2023""]","is_blank","is_blank","The context includes a table under 'People Managers' with gender data for 2023. The 'Women' percentage is listed as 31.6% globally."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction of computing hardware costs for the four key models (GPT-3, OPT-175B, GPT-4, Gemini Ultra) analyzed by Cottier et al. (2025) rises to 61–76% of the total amortized cost.","61–76","percent","[""cottier2024""]","is_blank","is_blank","The context states that when excluding equity, the fraction of computing hardware costs for the four key models (GPT-3, OPT-175B, GPT-4, Gemini Ultra) analyzed by Cottier et al. (2025) rises to 61–76% of the total amortized cost."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context explicitly states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a trade-off between energy efficiency and predictive performance. Additionally, Table III shows specific models where accuracy and F1 scores decreased post-optimization.","0","is_blank","[""khan2025""]","is_blank","is_blank","The context explicitly states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a trade-off between energy efficiency and predictive performance. Additionally, Table III shows specific models where accuracy and F1 scores decreased post-optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context snippet from ref_id=strubell2019 explicitly states that the U.S. EPA provides an average CO2 production rate of 0.954 pounds per kilowatt-hour (lbs/kWh) for power consumed in the U.S., which is used in their calculations.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 explicitly states that the U.S. EPA provides an average CO2 production rate of 0.954 pounds per kilowatt-hour (lbs/kWh) for power consumed in the U.S., which is used in their calculations."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context mentions a recent study projecting that global AI could consume 85–134 TWh of electricity in 2027, as stated in multiple sections of the document with ref_id li2025b.","85–134","TWh","[""li2025b""]","is_blank","is_blank","The context mentions a recent study projecting that global AI could consume 85–134 TWh of electricity in 2027, as stated in multiple sections of the document with ref_id li2025b."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context explicitly states under the Europe section that Amazon delivered 150 million packages via EVs in Europe in 2023.","150","packages","[""amazon2023""]","is_blank","is_blank","The context explicitly states under the Europe section that Amazon delivered 150 million packages via EVs in Europe in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context from kim2025 specifies that the online inference workload processed 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request are 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 tokens.","1920000","tokens","[""kim2025""]","is_blank","is_blank","The context from kim2025 specifies that the online inference workload processed 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request are 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas emissions from AI applications in sectors like oil and gas exploration. Additionally, it mentions that the Act does not mandate disclosure of GHG emissions, creating a reporting gap.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the AI Act fails to address greenhouse gas emissions from AI applications in sectors like oil and gas exploration. Additionally, it mentions that the Act does not mandate disclosure of GHG emissions, creating a reporting gap."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context explicitly states that for the DenseNet 201 experiment in the West US region using the Flexible Start optimization, the maximum CO2 emissions reduction is up to 80%. This is supported by Figure 3(a) and the accompanying text in the Dodge et al. paper.","80","percent","[""dodge2022""]","is_blank","is_blank","The context explicitly states that for the DenseNet 201 experiment in the West US region using the Flexible Start optimization, the maximum CO2 emissions reduction is up to 80%. This is supported by Figure 3(a) and the accompanying text in the Dodge et al. paper."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the least energy-intensive task is text classification with a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task is image generation with 2.9 kWh. The factor is calculated by dividing 2.9 by 0.002, resulting in 1450 times more energy required.","1450","times","[""luccioni2024""]","is_blank","is_blank","The context states that the least energy-intensive task is text classification with a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task is image generation with 2.9 kWh. The factor is calculated by dividing 2.9 by 0.002, resulting in 1450 times more energy required."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the United States with 244 projects under the 'Number of Projects' column.","244","projects","[""amazon2023""]","is_blank","is_blank","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the United States with 244 projects under the 'Number of Projects' column."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context mentions that Amazon achieved 100% renewable energy matching for AWS data centers in 2023 and discusses renewable energy procurement. However, there is no specific mention of the percentage for AWS's renewable energy usage in 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that Amazon achieved 100% renewable energy matching for AWS data centers in 2023 and discusses renewable energy procurement. However, there is no specific mention of the percentage for AWS's renewable energy usage in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context from ebert2024 explicitly states that for energy consumption reporting, the authors recommend measurement at the cumulative server level to balance accuracy and feasibility. This is supported by the recommendation to report energy consumption at this level to capture total computation-related power usage and optimize energy efficiency.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The context from ebert2024 explicitly states that for energy consumption reporting, the authors recommend measurement at the cumulative server level to balance accuracy and feasibility. This is supported by the recommendation to report energy consumption at this level to capture total computation-related power usage and optimize energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that in an experiment training a BERT-base model, the GPU accounted for 74% of the total electricity consumption. This is explicitly shown in Table 1, which provides the fraction of electricity consumption by each hardware component.","74","percent","[""dodge2022""]","is_blank","is_blank","The context states that in an experiment training a BERT-base model, the GPU accounted for 74% of the total electricity consumption. This is explicitly shown in Table 1, which provides the fraction of electricity consumption by each hardware component."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from ref_id=samsi2024 states that 4 A100-80GB GPUs are required for meaningful inferences with the 65B LLaMA model. However, ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The discrepancy between sources creates uncertainty, but the question specifically asks about Llama2-70b which is directly addressed in griggs2024.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The context from ref_id=samsi2024 states that 4 A100-80GB GPUs are required for meaningful inferences with the 65B LLaMA model. However, ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The discrepancy between sources creates uncertainty, but the question specifically asks about Llama2-70b which is directly addressed in griggs2024."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over a two-year period, specifically mentioned in Section III-B and Figure 8.","28.5","percent","[""wu2021a""]","is_blank","is_blank","The context explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over a two-year period, specifically mentioned in Section III-B and Figure 8."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context does not mention any specific data regarding the installation of fiber optic cables globally in 2023 to support AI workloads. The documents discuss growth in model sizes, data ingestion, infrastructure capacity, and environmental impacts like carbon and water footprints, but there is no information on kilometers of fiber optic cable installed.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context does not mention any specific data regarding the installation of fiber optic cables globally in 2023 to support AI workloads. The documents discuss growth in model sizes, data ingestion, infrastructure capacity, and environmental impacts like carbon and water footprints, but there is no information on kilometers of fiber optic cable installed."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800 million, as provided in the example comparing it to the amortized hardware CapEx + energy cost of $40M.","800000000","USD","[""cottier2024""]","is_blank","is_blank","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800 million, as provided in the example comparing it to the amortized hardware CapEx + energy cost of $40M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the term 'Sustainable AI' was proposed by van Wynsberghe as a field that both aims to use AI in climate-positive applications and improve the environmental sustainability of AI approaches themselves. This directly contradicts the statement that it was proposed to only encompass climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that the term 'Sustainable AI' was proposed by van Wynsberghe as a field that both aims to use AI in climate-positive applications and improve the environmental sustainability of AI approaches themselves. This directly contradicts the statement that it was proposed to only encompass climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context mentions that for NVIDIA A100-40GB GPU, the maximum batch size (MBS) for fine-tuning Mixtral is determined by their analytical model. Table IV in the context lists 'A100 80GB' with MBS 17, but the A100-40GB is not explicitly listed. However, Figure 13 shows 'A100-40GB' in the graph with a projected ground truth value. The text states that for current GPUs (up to 80GB), the ground truth values are correlated with projections, implying the A100-40GB's MBS is part of the demonstrated results.","17","samples","[""xia2024""]","is_blank","is_blank","The context mentions that for NVIDIA A100-40GB GPU, the maximum batch size (MBS) for fine-tuning Mixtral is determined by their analytical model. Table IV in the context lists 'A100 80GB' with MBS 17, but the A100-40GB is not explicitly listed. However, Figure 13 shows 'A100-40GB' in the graph with a projected ground truth value. The text states that for current GPUs (up to 80GB), the ground truth values are correlated with projections, implying the A100-40GB's MBS is part of the demonstrated results."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Swapping copies data without running extra computation, which is more energy-efficient than Recomputation's approach of recomputing from scratch.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","The context states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Swapping copies data without running extra computation, which is more energy-efficient than Recomputation's approach of recomputing from scratch."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context includes multiple references to Google's 2024 environmental report, specifically citing the URL https://sustainability.google/reports/google-2024-environmental-report/ in ref_id=jegham2025 and ref_id=luccioni2025a. However, none of the provided context snippets explicitly mention the percentage increase in GHG emissions since 2019 reported by Google.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context includes multiple references to Google's 2024 environmental report, specifically citing the URL https://sustainability.google/reports/google-2024-environmental-report/ in ref_id=jegham2025 and ref_id=luccioni2025a. However, none of the provided context snippets explicitly mention the percentage increase in GHG emissions since 2019 reported by Google."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context from [ref_id=han2024] states that U.S. data center electricity consumption is projected to increase from 4.4% in 2023 to 6.7–12.0% in 2028. However, the question asks about 2030 projections. Another context snippet from [ref_id=fernandez2025] mentions projections of 9.1% to 11.7% by 2030. These ranges from different sources provide the answer.","9.1–11.7","percent","[""han2024"", ""fernandez2025""]","is_blank","is_blank","The context from [ref_id=han2024] states that U.S. data center electricity consumption is projected to increase from 4.4% in 2023 to 6.7–12.0% in 2028. However, the question asks about 2030 projections. Another context snippet from [ref_id=fernandez2025] mentions projections of 9.1% to 11.7% by 2030. These ranges from different sources provide the answer."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context mentions that for the 6B parameter transformer training run, the Pause and Resume optimization can lead to significant savings up to about 25% when the duration is doubled. Additionally, Table 9 shows a 11.4% decrease in CO2 emissions for the 6B Transformer under P&R with a 100% increase in job duration.","25","percent","[""dodge2022""]","is_blank","is_blank","The context mentions that for the 6B parameter transformer training run, the Pause and Resume optimization can lead to significant savings up to about 25% when the duration is doubled. Additionally, Table 9 shows a 11.4% decrease in CO2 emissions for the 6B Transformer under P&R with a 100% increase in job duration."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is mentioned in multiple instances across the provided snippets from the same source (shen2024).","70","percent","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is mentioned in multiple instances across the provided snippets from the same source (shen2024)."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context snippet from ref_id=luccioni2025b explicitly states that the 2019 study by Strubell et al. quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions. This matches the question's requirements regarding the model and the study year.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025b explicitly states that the 2019 study by Strubell et al. quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions. This matches the question's requirements regarding the model and the study year."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context snippet from ref_id=li2025b explicitly states that GPT-3's estimated training energy was 1287 MWh, which directly answers the question.","1287","MWh","[""li2025b""]","is_blank","is_blank","The context snippet from ref_id=li2025b explicitly states that GPT-3's estimated training energy was 1287 MWh, which directly answers the question."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context from xia2024 states that the MoE layer is the most time-consuming component in LLM fine-tuning, accounting for 85% of execution time, and is a prime target for optimization. Additionally, it mentions that optimizing the MoE layer is crucial for improving performance.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context from xia2024 states that the MoE layer is the most time-consuming component in LLM fine-tuning, accounting for 85% of execution time, and is a prime target for optimization. Additionally, it mentions that optimizing the MoE layer is crucial for improving performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context mentions that experiments were conducted on an NVIDIA A40 GPU with 48GB memory, fine-tuning Mixtral using different batch sizes. However, the specific maximum batch size for the dense setup on the Hellaswag dataset is not explicitly stated in the provided snippets. The available information discusses maximum batch sizes for other setups (e.g., sparse setup) and GPUs, but not the dense setup on Hellaswag with A40.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context mentions that experiments were conducted on an NVIDIA A40 GPU with 48GB memory, fine-tuning Mixtral using different batch sizes. However, the specific maximum batch size for the dense setup on the Hellaswag dataset is not explicitly stated in the provided snippets. The available information discusses maximum batch sizes for other setups (e.g., sparse setup) and GPUs, but not the dense setup on Hellaswag with A40."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to fully local training. This directly supports the statement in the question.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to fully local training. This directly supports the statement in the question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context explicitly states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 text classification queries, as part of a comparison with multi-purpose models.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context explicitly states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 text classification queries, as part of a comparison with multi-purpose models."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the most efficient regions emitted approximately 7k grams and the least efficient regions emitted 26k grams of CO2 during BERT model training. The range between these values is 19,000 grams (26k - 7k).","19000","grams","[""dodge2022""]","is_blank","is_blank","The context states that the most efficient regions emitted approximately 7k grams and the least efficient regions emitted 26k grams of CO2 during BERT model training. The range between these values is 19,000 grams (26k - 7k)."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context explicitly states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.' This indicates that 2022 was the peak year before the decline.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The context explicitly states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.' This indicates that 2022 was the peak year before the decline."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context from morrison2025's Table 2 lists the OLMo 60M† model with water consumption equivalent to 5 days for one person in the U.S. when trained on 1.7 to 5.6 trillion tokens.","5","days","[""morrison2025""]","is_blank","is_blank","The context from morrison2025's Table 2 lists the OLMo 60M† model with water consumption equivalent to 5 days for one person in the U.S. when trained on 1.7 to 5.6 trillion tokens."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context mentions that one technology company's self-owned data centers alone consumed more than 23 billion liters of freshwater for on-site cooling in 2023. This aligns with the question about Google's DeepMind AlphaFold servers, as Google is a major technology company with AI services.","23000000000","liters of freshwater","[""li2025b""]","is_blank","is_blank","The context mentions that one technology company's self-owned data centers alone consumed more than 23 billion liters of freshwater for on-site cooling in 2023. This aligns with the question about Google's DeepMind AlphaFold servers, as Google is a major technology company with AI services."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce cost. This implies that such an action would not increase costs but rather help in lowering them.","0","is_blank","[""xia2024""]","is_blank","is_blank","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce cost. This implies that such an action would not increase costs but rather help in lowering them."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","","","kWh","[]","is_blank","is_blank",""
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context states that under the high-growth scenario, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028. The question asks for the projected amount for 2030, but the provided context only includes projections up to 2028. Since there is no data for 2030, the answer cannot be determined from the given context.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context states that under the high-growth scenario, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028. The question asks for the projected amount for 2030, but the provided context only includes projections up to 2028. Since there is no data for 2030, the answer cannot be determined from the given context."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context includes Table IV which provides the estimated cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU. The table lists the cost as $32.7 under the 'Cost ($)' column for the A40 48GB GPU.","32.7","USD","[""xia2024""]","is_blank","is_blank","The context includes Table IV which provides the estimated cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU. The table lists the cost as $32.7 under the 'Cost ($)' column for the A40 48GB GPU."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context mentions that for Mixtral-S (sparse) on the MATH dataset, the maximum batch size supported on an NVIDIA A40-48GB GPU is 3 samples. This information is found in Table III of the provided document.","3","samples","[""xia2024""]","is_blank","is_blank","The context mentions that for Mixtral-S (sparse) on the MATH dataset, the maximum batch size supported on an NVIDIA A40-48GB GPU is 3 samples. This information is found in Table III of the provided document."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.' This directly contradicts the statement that GPU-level monitoring is recommended as the preferred method.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.' This directly contradicts the statement that GPU-level monitoring is recommended as the preferred method."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in the 2023 article by Luccioni et al., training accounted for only half (50%) of BLOOM's overall emissions when using a Life Cycle Assessment approach.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The context states that in the 2023 article by Luccioni et al., training accounted for only half (50%) of BLOOM's overall emissions when using a Life Cycle Assessment approach."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context explicitly states in Table II that the bare minimum hardware required for LLaMA 7B on V100 32GB GPUs is 1 GPU. This is under the assumption of no compression or quantization, as mentioned in the same table's description.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context explicitly states in Table II that the bare minimum hardware required for LLaMA 7B on V100 32GB GPUs is 1 GPU. This is under the assumption of no compression or quantization, as mentioned in the same table's description."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context snippet [ref_id=li2025b] includes a table (Table 1) that specifically lists the number of user requests required to consume a 500ml bottle of water when training GPT-3 in Arizona. The table shows that in Arizona, it takes 16.7 requests to consume 500ml of water.","16.7","requests","[""li2025b""]","is_blank","is_blank","The context snippet [ref_id=li2025b] includes a table (Table 1) that specifically lists the number of user requests required to consume a 500ml bottle of water when training GPT-3 in Arizona. The table shows that in Arizona, it takes 16.7 requests to consume 500ml of water."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context snippet from ref_id=dodge2022 explicitly states that the average US home energy use emits 8.3 metric tons of CO2 per year, which directly answers the question.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The context snippet from ref_id=dodge2022 explicitly states that the average US home energy use emits 8.3 metric tons of CO2 per year, which directly answers the question."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context mentions that JetMoE-8B outperforms the Llama2-7B model and discusses its performance on the OpenLLM Leaderboard, but does not provide a specific GSM8k benchmark score.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context mentions that JetMoE-8B outperforms the Llama2-7B model and discusses its performance on the OpenLLM Leaderboard, but does not provide a specific GSM8k benchmark score."
