"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that U.S. data center energy consumption rose by about 4% from 2010 to 2014, which represents the average increase over that period.","4","percent","[""wu2021b""]","is_blank","is_blank","The context states that U.S. data center energy consumption rose by about 4% from 2010 to 2014, which represents the average increase over that period."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The report states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","is_blank","The report states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","data centers","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The Morrison 2025 study states that NVIDIA does not publish embodied carbon data, so they adopt the estimate from Luccioni et al. (2023) of 3700 kg CO₂eq per 8‑node server, which translates to 463 kg CO₂eq per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","is_blank","The Morrison 2025 study states that NVIDIA does not publish embodied carbon data, so they adopt the estimate from Luccioni et al. (2023) of 3700 kg CO₂eq per 8‑node server, which translates to 463 kg CO₂eq per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The FLM‑101B was trained within a $100,000 budget (li2025a), while the amortized hardware and energy cost for GPT‑4 is $40 million (cottier2024). Dividing 40 million by 100 k gives a factor of 400.","400","ratio","[""li2025a"", ""cottier2024""]","is_blank","is_blank","The FLM‑101B was trained within a $100,000 budget (li2025a), while the amortized hardware and energy cost for GPT‑4 is $40 million (cottier2024). Dividing 40 million by 100 k gives a factor of 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that FLM-101B’s average score on the Open LLM Leaderboard is 43.94, and Table 4 lists the same average value.","43.94","score","[""li2025a""]","is_blank","is_blank","The context states that FLM-101B’s average score on the Open LLM Leaderboard is 43.94, and Table 4 lists the same average value."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The cited document states that the typical microprocessor in 2021 runs at 5,000,000 kHz compared to 740 kHz for the Intel 4004, which is described as a ""more than 6,750 fold improvement in processor clock speed.""","6750","fold","[""wu2021b""]","is_blank","is_blank","The cited document states that the typical microprocessor in 2021 runs at 5,000,000 kHz compared to 740 kHz for the Intel 4004, which is described as a ""more than 6,750 fold improvement in processor clock speed."""
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","days","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The table of SGLang benchmarking results lists the GPU Power Usage for Llama 3.2 1B at an 8 request/s frequency as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The table of SGLang benchmarking results lists the GPU Power Usage for Llama 3.2 1B at an 8 request/s frequency as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","tons","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper states that training FLM‑101B with the growth strategy takes 21.54 days, which is a 72% time‑saving compared to training a 101B model from scratch.","72","percent","[""li2025a""]","is_blank","is_blank","The paper states that training FLM‑101B with the growth strategy takes 21.54 days, which is a 72% time‑saving compared to training a 101B model from scratch."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","Unable to answer with confidence based on the provided documents.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The Dodge et al. (2022) study reports that the 6.1 billion‑parameter transformer was trained for 8 days and only reached 13% of the full training. They estimate that completing the full run would require about 60 days.","60","days","[""dodge2022""]","is_blank","is_blank","The Dodge et al. (2022) study reports that the 6.1 billion‑parameter transformer was trained for 8 days and only reached 13% of the full training. They estimate that completing the full run would require about 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The preface of the report states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","is_blank","is_blank","The preface of the report states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN’s Global E‑Waste Monitor 2024 reports that roughly 22% of e‑waste is formally collected and recycled.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The UN’s Global E‑Waste Monitor 2024 reports that roughly 22% of e‑waste is formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","MWh","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The hyperparameter table for JetMoE‑8B lists ""Nexperts 8"", and the text explicitly states that each MoE layer uses 8 experts.","8","experts","[""shen2024""]","is_blank","is_blank","The hyperparameter table for JetMoE‑8B lists ""Nexperts 8"", and the text explicitly states that each MoE layer uses 8 experts."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The provided context does not contain any explicit numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU. No figure or table lists this specific measurement, so the answer cannot be derived from the given documents.","is_blank","second","[""is_blank""]","is_blank","is_blank","The provided context does not contain any explicit numeric value for the total execution time of a dense BlackMamba model with batch size 30 on an NVIDIA A40 GPU. No figure or table lists this specific measurement, so the answer cannot be derived from the given documents."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The document states that the total cost of FLM‑101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs attributed to the English portion.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The document states that the total cost of FLM‑101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs attributed to the English portion."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for the financial sentiment classification study explicitly lists the processor used in the hardware section: an 11th Gen Intel Core i7‑1165G7.","11th Gen Intel Core i7-1165G7 processor","is_blank","[""khan2025""]","is_blank","is_blank","The experimental setup for the financial sentiment classification study explicitly lists the processor used in the hardware section: an 11th Gen Intel Core i7‑1165G7."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","is_blank","is_blank","models","[""is_blank""]","is_blank","is_blank","is_blank"
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The document states that increasing GPU utilization to 80% for language model training reduces the overall carbon footprint by a factor of 3×.","3","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that increasing GPU utilization to 80% for language model training reduces the overall carbon footprint by a factor of 3×."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) report that the total compute used during model development is between 1.2‑times and 4‑times larger than the compute of the final training run, with a median estimate of 2.2‑times.","2.2","multiplier","[""cottier2024""]","is_blank","is_blank","Cottier et al. (2025) report that the total compute used during model development is between 1.2‑times and 4‑times larger than the compute of the final training run, with a median estimate of 2.2‑times."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The document states that a 6.1 billion‑parameter transformer was trained for 8 days on 256 A100 GPUs, consuming 13.8 MWh, and that a full training run would take 60 days. Multiplying by 60/8 gives an estimated total energy consumption of 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","is_blank","The document states that a 6.1 billion‑parameter transformer was trained for 8 days on 256 A100 GPUs, consuming 13.8 MWh, and that a full training run would take 60 days. Multiplying by 60/8 gives an estimated total energy consumption of 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","is_blank","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The paper states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The paper states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","is_blank","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The document states that with the growth strategy the total time cost for training FLM‑101B is 21.54 days, indicating the wall‑clock time required.","21.54","days","[""li2025a""]","is_blank","is_blank","The document states that with the growth strategy the total time cost for training FLM‑101B is 21.54 days, indicating the wall‑clock time required."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The provided excerpts from the Facebook study state that a vast majority of model experimentation workflows use GPUs at only 30‑50% capacity, leaving significant room for improvement. This directly contradicts the claim that the majority use GPUs at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The provided excerpts from the Facebook study state that a vast majority of model experimentation workflows use GPUs at only 30‑50% capacity, leaving significant room for improvement. This directly contradicts the claim that the majority use GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The provided excerpts explicitly state that GPT‑3’s training consumed 1,287 MWh of electricity.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The provided excerpts explicitly state that GPT‑3’s training consumed 1,287 MWh of electricity."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context states that the AI Energy Score project was adapted to provide a unified, standardized approach for comparing inference efficiency across AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that the AI Energy Score project was adapted to provide a unified, standardized approach for comparing inference efficiency across AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The provided excerpts describe the existence of a kernel‑level MoE time breakdown (Fig. 6) but do not give any specific numeric values for the longest kernel execution time for the dense BlackMamba model with batch size 30 on an NVIDIA A40‑48 GB GPU. Therefore the question cannot be answered with confidence based on the supplied documents.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The provided excerpts describe the existence of a kernel‑level MoE time breakdown (Fig. 6) but do not give any specific numeric values for the longest kernel execution time for the dense BlackMamba model with batch size 30 on an NVIDIA A40‑48 GB GPU. Therefore the question cannot be answered with confidence based on the supplied documents."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The hyperparameter table and accompanying text state that each layer uses 8 experts and selects the top‑k = 2 experts for activation.","2","experts","[""shen2024""]","is_blank","is_blank","The hyperparameter table and accompanying text state that each layer uses 8 experts and selects the top‑k = 2 experts for activation."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The provided context from Schwartz et al. (2019) states that the compute required for deep learning research increased 300,000× from 2012 to 2018, not 200,000×.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The provided context from Schwartz et al. (2019) states that the compute required for deep learning research increased 300,000× from 2012 to 2018, not 200,000×."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","percent","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The report states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023.","22","data centers","[""amazon2023""]","is_blank","is_blank","The report states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. 2025 minus 1956 equals 69, so the field is approximately 69 years old in 2025.","69","years","[""stone2022""]","is_blank","is_blank","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. 2025 minus 1956 equals 69, so the field is approximately 69 years old in 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 2019 Strubell et al. study estimated the five‑car emissions from the energy required to automate model development via neural architecture search (NAS), a large‑scale procedure performed infrequently.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","The 2019 Strubell et al. study estimated the five‑car emissions from the energy required to automate model development via neural architecture search (NAS), a large‑scale procedure performed infrequently."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","percent","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","samples","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The report states that in 2023 Amazon added battery storage projects and now holds 1.3 GW of storage capacity, up from 445 MW in 2022.","1.3","GW","[""amazon2023""]","is_blank","is_blank","The report states that in 2023 Amazon added battery storage projects and now holds 1.3 GW of storage capacity, up from 445 MW in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The document states that GPT‑4o’s projected annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The document states that GPT‑4o’s projected annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","is_blank","is_blank","PUE","[""is_blank""]","is_blank","is_blank","is_blank"
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The document states that JetMoE-8B contains 8 billion parameters but only activates 2 billion of them for each input token during inference.","2B","parameters","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B contains 8 billion parameters but only activates 2 billion of them for each input token during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The table in the Luccioni et al. (2025) appendix lists the GHG emissions for the Llama 7B model as 14 tCO₂e.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The table in the Luccioni et al. (2025) appendix lists the GHG emissions for the Llama 7B model as 14 tCO₂e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","is_blank","is_blank","electric delivery vans","[""is_blank""]","is_blank","is_blank","is_blank"
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The document explicitly states that operational environmental impacts include GHG emissions from servers and data center cooling, contradicting the claim that they do not include such emissions.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The document explicitly states that operational environmental impacts include GHG emissions from servers and data center cooling, contradicting the claim that they do not include such emissions."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The table of energy consumption for the o3 model lists the value for long prompts (10k input‑1.5k output) as 12.222 Wh. This directly answers the question.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The table of energy consumption for the o3 model lists the value for long prompts (10k input‑1.5k output) as 12.222 Wh. This directly answers the question."
"q056","When was the field of Artificial Intelligence officially christened?","The provided excerpts state that the field of Artificial Intelligence was officially born and christened at a 1956 workshop.","1956","year","[""stone2022""]","is_blank","is_blank","The provided excerpts state that the field of Artificial Intelligence was officially born and christened at a 1956 workshop."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","is_blank","is_blank","WUE","[""is_blank""]","is_blank","is_blank","is_blank"
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The provided excerpts from the document state that approximately 770 million people, about one in ten, do not have access to a stable supply of electricity, directly supporting the claim.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The provided excerpts from the document state that approximately 770 million people, about one in ten, do not have access to a stable supply of electricity, directly supporting the claim."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The document states that for a maximum generation length of 512 tokens, each output token consumes roughly 3–4 Joules of energy.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","The document states that for a maximum generation length of 512 tokens, each output token consumes roughly 3–4 Joules of energy."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that converting RM2 from 32‑bit to 16‑bit reduces its overall model size by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","The context states that converting RM2 from 32‑bit to 16‑bit reduces its overall model size by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context states that the 5‑10% reduction estimate originates from BCG reports, but the reasoning is unclear and the underlying calculations are not detailed beyond BCG’s experience. No specific, publicly available calculations translating individual projects to a global scale are provided, and the claim lacks the rigorous scientific grounding required.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that the 5‑10% reduction estimate originates from BCG reports, but the reasoning is unclear and the underlying calculations are not detailed beyond BCG’s experience. No specific, publicly available calculations translating individual projects to a global scale are provided, and the claim lacks the rigorous scientific grounding required."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","USD","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The provided context states that in BlackMamba sparse fine‑tuning on an NVIDIA A40‑48GB GPU with batch size = 1, the optimizer stage accounts for up to 53% of the total running time.","53","percent","[""xia2024""]","is_blank","is_blank","The provided context states that in BlackMamba sparse fine‑tuning on an NVIDIA A40‑48GB GPU with batch size = 1, the optimizer stage accounts for up to 53% of the total running time."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Google Translate processes about 1 billion queries per day and that a comparable multi‑purpose model (Flan‑T5‑xxl) consumes 0.083 kWh per 1,000 queries. 0.083 kWh/1,000 queries equals 0.000083 kWh per query. 1 billion queries × 0.000083 kWh/query = 83,000 kWh, which is 83 MWh per day.","83","MWh","[""luccioni2024""]","is_blank","is_blank","The context states that Google Translate processes about 1 billion queries per day and that a comparable multi‑purpose model (Flan‑T5‑xxl) consumes 0.083 kWh per 1,000 queries. 0.083 kWh/1,000 queries equals 0.000083 kWh per query. 1 billion queries × 0.000083 kWh/query = 83,000 kWh, which is 83 MWh per day."
"q067","What was the average global data center PUE in 2023?","The provided excerpts state that the average global data center Power Usage Effectiveness (PUE) in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The provided excerpts state that the average global data center Power Usage Effectiveness (PUE) in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","is_blank","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","is_blank"
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The provided excerpts state that Gemini Ultra’s R&D staff costs, including equity, represent the highest fraction among the frontier models at 49%.","49","percent","[""cottier2024""]","is_blank","is_blank","The provided excerpts state that Gemini Ultra’s R&D staff costs, including equity, represent the highest fraction among the frontier models at 49%."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context repeatedly states that the inaugural Study Panel was a seventeen‑member panel.","17","people","[""stone2022""]","is_blank","is_blank","The context repeatedly states that the inaugural Study Panel was a seventeen‑member panel."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The document states that manufacturing carbon cost accounts for 74% of the total footprint of client devices, indicating that 74% of a client device’s total carbon footprint is due to its manufacturing.","74","percent","[""wu2021a""]","is_blank","is_blank","The document states that manufacturing carbon cost accounts for 74% of the total footprint of client devices, indicating that 74% of a client device’s total carbon footprint is due to its manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The document states that while models with more parameters tend to consume more energy, this is not always the case. For example, Phi‑3 Small (7B) can consume less energy than Phi‑3 Mini (3.8B) at larger batch sizes due to architectural differences.","0","is_blank","[""chung2025""]","is_blank","is_blank","The document states that while models with more parameters tend to consume more energy, this is not always the case. For example, Phi‑3 Small (7B) can consume less energy than Phi‑3 Mini (3.8B) at larger batch sizes due to architectural differences."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The report explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, indicating they are not concerned about such a threat.","0","is_blank","[""stone2022""]","is_blank","is_blank","The report explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, indicating they are not concerned about such a threat."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","tCO2e","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","tCO2e","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The document states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5‑year period from Yr1‑Q1 to Yr2‑Q2 (2019‑2021).","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5‑year period from Yr1‑Q1 to Yr2‑Q2 (2019‑2021)."
"q079","How many miles is the Earth from the Sun?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","miles","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that AlphaGo, a program developed by Google DeepMind, beat the human Go champion in a five‑game match, indicating that it defeated the champion.","1","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that AlphaGo, a program developed by Google DeepMind, beat the human Go champion in a five‑game match, indicating that it defeated the champion."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The document explains that continuous batching replaces finished requests with new ones to keep the GPU busy, thereby reducing idle time.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","The document explains that continuous batching replaces finished requests with new ones to keep the GPU busy, thereby reducing idle time."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The document states that the entire alignment process, which includes both dSFT and dDPO fine‑tuning, takes 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The document states that the entire alignment process, which includes both dSFT and dDPO fine‑tuning, takes 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The table shows that for a 100 TPS offline workload, InferSave selected g4dn.xlarge at a total price of $2.13, while Max‑Performance selected g6e.xlarge at $2.699. The difference is about 26.7% more expensive.","26.7","percent","[""kim2025""]","is_blank","is_blank","The table shows that for a 100 TPS offline workload, InferSave selected g4dn.xlarge at a total price of $2.13, while Max‑Performance selected g6e.xlarge at $2.699. The difference is about 26.7% more expensive."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","g CO2eq","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The 2025 study’s appendix lists GPU energy consumption for 1,000 inference queries, showing a minimum of 0.06 Wh (bert‑tiny) and a maximum of over 3,426 Wh (Command‑R Plus).","0.06-3426","Wh","[""luccioni2025c""]","is_blank","is_blank","The 2025 study’s appendix lists GPU energy consumption for 1,000 inference queries, showing a minimum of 0.06 Wh (bert‑tiny) and a maximum of over 3,426 Wh (Command‑R Plus)."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The article explicitly states that there is no one‑size‑fits‑all solution for either ethics or sustainability, indicating that researchers do not believe a universal approach can be developed.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The article explicitly states that there is no one‑size‑fits‑all solution for either ethics or sustainability, indicating that researchers do not believe a universal approach can be developed."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","kg of CO2e/KWh","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The paper describes using Hivemind, a PyTorch-based decentralized framework, to enable distributed spot instance training across multiple clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The paper describes using Hivemind, a PyTorch-based decentralized framework, to enable distributed spot instance training across multiple clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context states that transparency can be expanded to include socio‑technical aspects and the societal/environmental footprint of a system under the term ""social transparency"".","social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that transparency can be expanded to include socio‑technical aspects and the societal/environmental footprint of a system under the term ""social transparency""."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context explicitly states that the highest accuracy in the German public administration text classification experiments was achieved by a traditional linear model that uses pre‑trained sentence embeddings.","linear model with pre‑trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that the highest accuracy in the German public administration text classification experiments was achieved by a traditional linear model that uses pre‑trained sentence embeddings."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The 2025 Chen et al. paper describes the development and deployment of an LLM inference system that uses model‑attention disaggregation. The text explicitly names this system as ""Lamina"" and notes that it incorporates the proposed disaggregation technique in a distributed heterogeneous cluster.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The 2025 Chen et al. paper describes the development and deployment of an LLM inference system that uses model‑attention disaggregation. The text explicitly names this system as ""Lamina"" and notes that it incorporates the proposed disaggregation technique in a distributed heterogeneous cluster."
"q093","How many parameters does the largest T5 model have?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q094","What is the total number of parameters in the JetMoE-8B model?","The document states that JetMoE-8B has 8B parameters and Table 1 lists the total parameter count (Ptotal) as 8B.","8B","parameters","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B has 8B parameters and Table 1 lists the total parameter count (Ptotal) as 8B."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google’s data center water consumption rose by about 20% from 2021 to 2022.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The context states that Google’s data center water consumption rose by about 20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The provided table lists ""Carbon Intensity"" as the metric whose definition is ""CO₂ emissions per unit of electricity consumed"".","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","The provided table lists ""Carbon Intensity"" as the metric whose definition is ""CO₂ emissions per unit of electricity consumed""."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The final growth stage of FLM‑101B corresponds to the 101B model in Table 2, which lists the FLOPs utilization as 52.88 %.","52.88","percent","[""li2025a""]","is_blank","is_blank","The final growth stage of FLM‑101B corresponds to the 101B model in Table 2, which lists the FLOPs utilization as 52.88 %."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The cited study explicitly states that the amortized hardware and energy cost for training GPT‑4 is $40 million, distinguishing it from the higher acquisition cost. This figure represents the estimated amortized training cost.","40M","USD","[""cottier2024""]","is_blank","is_blank","The cited study explicitly states that the amortized hardware and energy cost for training GPT‑4 is $40 million, distinguishing it from the higher acquisition cost. This figure represents the estimated amortized training cost."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","multiplier","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The document states that when training NLP across four continents (C‑8) the throughput drops by 41% compared to the fully local run (A‑8). A 41% drop means the achieved throughput is 59% of the local throughput.","0.59","multiplier","[""erben2023""]","is_blank","is_blank","The document states that when training NLP across four continents (C‑8) the throughput drops by 41% compared to the fully local run (A‑8). A 41% drop means the achieved throughput is 59% of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The 2023 Amazon Sustainability Report states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023.","3.5","liters","[""amazon2023""]","is_blank","is_blank","The 2023 Amazon Sustainability Report states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The study explicitly states that custom tags reduce energy consumption for zero‑shot, one‑shot, and few‑shot prompt engineering techniques in source code completion tasks.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The study explicitly states that custom tags reduce energy consumption for zero‑shot, one‑shot, and few‑shot prompt engineering techniques in source code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The 2025 paper by Luccioni et al. reports that NVIDIA shipped 3.7 million data‑center GPUs in 2024, a figure that is explicitly stated in the text.","3.7 million","GPUs","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper by Luccioni et al. reports that NVIDIA shipped 3.7 million data‑center GPUs in 2024, a figure that is explicitly stated in the text."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The document states that, on average, 44% of the amortized hardware CapEx plus energy cost is attributed to AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","is_blank","The document states that, on average, 44% of the amortized hardware CapEx plus energy cost is attributed to AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","Both the Wu et al. 2021a and Wu et al. 2021b documents state that Facebook’s data centers achieve a Power Usage Effectiveness (PUE) of about 1.10.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","Both the Wu et al. 2021a and Wu et al. 2021b documents state that Facebook’s data centers achieve a Power Usage Effectiveness (PUE) of about 1.10."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context explicitly states that the Finnish project is called ETAIROS (Ethical AI for the Governance of the Society).","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states that the Finnish project is called ETAIROS (Ethical AI for the Governance of the Society)."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The paper explicitly states that Google’s Gemini Ultra had an estimated amortized training cost of $30 million.","30000000","USD","[""cottier2024""]","is_blank","is_blank","The paper explicitly states that Google’s Gemini Ultra had an estimated amortized training cost of $30 million."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The AI Act mandates risk assessment and mitigation for GPAI models with systemic risk, but the provided excerpts state that these provisions lack sufficient emphasis on environmental factors and do not explicitly require environmental risks to be included in the assessment.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The AI Act mandates risk assessment and mitigation for GPAI models with systemic risk, but the provided excerpts state that these provisions lack sufficient emphasis on environmental factors and do not explicitly require environmental risks to be included in the assessment."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","µg/m³","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The life cycle assessment cited in the context states that 115 print books emit the same amount of CO2 as one Amazon Kindle e‑reader.","115","books","[""luccioni2025a""]","is_blank","is_blank","The life cycle assessment cited in the context states that 115 print books emit the same amount of CO2 as one Amazon Kindle e‑reader."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The study reports that disadvantaged communities could experience a per‑household health burden from air pollutants up to 200 times higher than in less‑impacted communities.","200","multiplier","[""han2024""]","is_blank","is_blank","The study reports that disadvantaged communities could experience a per‑household health burden from air pollutants up to 200 times higher than in less‑impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The table in the Appendix B of the zschache2025 document lists the energy consumed for the DS Llama 70B model during inference on the FKTG dataset. It shows 702.06 Wh for a single‑node run and 1899.86 Wh for a double‑node run.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","The table in the Appendix B of the zschache2025 document lists the energy consumed for the DS Llama 70B model during inference on the FKTG dataset. It shows 702.06 Wh for a single‑node run and 1899.86 Wh for a double‑node run."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context explicitly links efficiency gains to higher overall consumption through Jevons Paradox, describing how technological progress that improves efficiency can lead to increased usage and resource consumption.","Jevons Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The context explicitly links efficiency gains to higher overall consumption through Jevons Paradox, describing how technological progress that improves efficiency can lead to increased usage and resource consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The provided context gives the training energy for GPT‑3 (1287 MWh) but does not provide the training energy for Meena, so we cannot determine how many Meena runs would match GPT‑3’s energy.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The provided context gives the training energy for GPT‑3 (1287 MWh) but does not provide the training energy for Meena, so we cannot determine how many Meena runs would match GPT‑3’s energy."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","Table 2 of the 2024 study lists the mean energy consumption per 1,000 inferences for each task. For image generation the mean value is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","Table 2 of the 2024 study lists the mean energy consumption per 1,000 inferences for each task. For image generation the mean value is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The table from Strubell et al. lists the estimated CO₂e for an average American life in one year as 36,156 pounds.","36156","lbs","[""strubell2019""]","is_blank","is_blank","The table from Strubell et al. lists the estimated CO₂e for an average American life in one year as 36,156 pounds."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","Table III shows Mistral‑small’s carbon emissions per inference task decreased from 0.020 kg to 0.015 kg after optimization. Dividing the post‑optimization value by the pre‑optimization value gives a multiplier of 0.015 / 0.020 = 0.75, indicating a 25 % reduction.","0.75","multiplier","[""khan2025""]","is_blank","is_blank","Table III shows Mistral‑small’s carbon emissions per inference task decreased from 0.020 kg to 0.015 kg after optimization. Dividing the post‑optimization value by the pre‑optimization value gives a multiplier of 0.015 / 0.020 = 0.75, indicating a 25 % reduction."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The table from Luccioni et al. lists the training energy for BLOOMz‑7B as 51,686 kWh and the fine‑tuning energy as 7,571 kWh. Adding these two values gives the combined energy cost.","59257","kWh","[""luccioni2024""]","is_blank","is_blank","The table from Luccioni et al. lists the training energy for BLOOMz‑7B as 51,686 kWh and the fine‑tuning energy as 7,571 kWh. Adding these two values gives the combined energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The document states that the FLM‑101B model is a 101‑billion‑parameter model, explicitly listing its parameter count in the architecture description.","101B","parameters","[""li2025a""]","is_blank","is_blank","The document states that the FLM‑101B model is a 101‑billion‑parameter model, explicitly listing its parameter count in the architecture description."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The 6.1 B parameter transformer required an estimated 103,500 kWh to train to completion (dodge2022). BLOOMz‑7B consumes 1.0 × 10⁻⁴ kWh per inference (luccioni2024). Dividing the 6.1 B model’s training energy by the per‑inference cost gives roughly 1.0 billion inferences to match that energy budget.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","The 6.1 B parameter transformer required an estimated 103,500 kWh to train to completion (dodge2022). BLOOMz‑7B consumes 1.0 × 10⁻⁴ kWh per inference (luccioni2024). Dividing the 6.1 B model’s training energy by the per‑inference cost gives roughly 1.0 billion inferences to match that energy budget."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The study explicitly states that the total energy used for all model experimentation and evaluation was 754.66 kWh.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The study explicitly states that the total energy used for all model experimentation and evaluation was 754.66 kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","Table 5 in the Luccioni et al. study lists the cost parity for BLOOMz‑7B as 592,570,000 inferences, meaning that this many inferences are needed for the cumulative inference energy to match the training plus fine‑tuning energy.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","Table 5 in the Luccioni et al. study lists the cost parity for BLOOMz‑7B as 592,570,000 inferences, meaning that this many inferences are needed for the cumulative inference energy to match the training plus fine‑tuning energy."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context states that the statements from the population were categorized, processed and published as the FKTG-dataset, which is the dataset used in the experiments.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context states that the statements from the population were categorized, processed and published as the FKTG-dataset, which is the dataset used in the experiments."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","is_blank","is_blank","liters","[""is_blank""]","is_blank","is_blank","is_blank"
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The provided context does not contain information about the percentage of recycled rare earth metals used in NVIDIA H100 GPUs manufactured in 2024.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain information about the percentage of recycled rare earth metals used in NVIDIA H100 GPUs manufactured in 2024."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The Strubell et al. table lists the CO₂e for a single passenger round‑trip between San Francisco and New York as 1,984 lbs, which is about 0.9 tCO₂e. Dividing the Evolved Transformer NAS’s 3.2 tCO₂e by 0.9 tCO₂e per passenger gives roughly 3.6 passengers.","3.6","passengers","[""strubell2019""]","is_blank","is_blank","The Strubell et al. table lists the CO₂e for a single passenger round‑trip between San Francisco and New York as 1,984 lbs, which is about 0.9 tCO₂e. Dividing the Evolved Transformer NAS’s 3.2 tCO₂e by 0.9 tCO₂e per passenger gives roughly 3.6 passengers."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The May 2025 data from OpenRouter shows that 84% of token usage was through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","is_blank","The May 2025 data from OpenRouter shows that 84% of token usage was through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The table of baseline configurations (Table II) lists the bare‑minimum hardware for each LLaMA variant. For the 13B model it shows that one 80 GB A100 GPU is sufficient when no compression or quantization is applied.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The table of baseline configurations (Table II) lists the bare‑minimum hardware for each LLaMA variant. For the 13B model it shows that one 80 GB A100 GPU is sufficient when no compression or quantization is applied."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","is_blank","is_blank","metric tons","[""is_blank""]","is_blank","is_blank","is_blank"
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The document reports that a mix of 2 A100 GPUs and 1 A10G GPU yields a 24 % cost saving compared to an A100‑only strategy.","24","percent","[""griggs2024""]","is_blank","is_blank","The document reports that a mix of 2 A100 GPUs and 1 A10G GPU yields a 24 % cost saving compared to an A100‑only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The table from Chen et al. lists the price per chip for the NVIDIA H20 as $4.63 per hour.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","The table from Chen et al. lists the price per chip for the NVIDIA H20 as $4.63 per hour."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The excerpts from Luccioni and Hernandez‑Garcia’s work state that most carbon‑footprint analyses gather information manually by writing to authors, indicating that automatic data collection without author contact is not the norm.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The excerpts from Luccioni and Hernandez‑Garcia’s work state that most carbon‑footprint analyses gather information manually by writing to authors, indicating that automatic data collection without author contact is not the norm."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The document states that in 2023 the public health cost of U.S. data centers was about 44% of their total electricity cost, using the average attribution method.","44","percent","[""han2024""]","is_blank","is_blank","The document states that in 2023 the public health cost of U.S. data centers was about 44% of their total electricity cost, using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","Table II lists the bare‑minimum hardware for each LLaMA variant. For the 7B model it shows that only one A100 80GB GPU is required, with a maximum batch size of 64, when no compression or quantization is applied.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","Table II lists the bare‑minimum hardware for each LLaMA variant. For the 7B model it shows that only one A100 80GB GPU is required, with a maximum batch size of 64, when no compression or quantization is applied."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The abstract of the paper states that experimental results show a reduction in energy consumption and carbon emissions of up to 45% after quantization, directly supporting the claim.","1","is_blank","[""khan2025""]","is_blank","is_blank","The abstract of the paper states that experimental results show a reduction in energy consumption and carbon emissions of up to 45% after quantization, directly supporting the claim."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The documents state that the researchers contacted more than 500 authors and were only able to collect 95 answers.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","The documents state that the researchers contacted more than 500 authors and were only able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","USD per hour","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The table of public health costs for training a Llama‑3.1 scale model lists Altoona, IA with a health cost that is 122% of the electricity cost.","122","percent","[""han2024""]","is_blank","is_blank","The table of public health costs for training a Llama‑3.1 scale model lists Altoona, IA with a health cost that is 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The document states that JetMoE-8B was trained on 1.25 trillion tokens of primarily English data from web documents, mathematics, and code.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The document states that JetMoE-8B was trained on 1.25 trillion tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The table of Amazon Renewable Energy Projects as of January 2024 lists 36 projects in the United Kingdom.","36","projects","[""amazon2023""]","is_blank","is_blank","The table of Amazon Renewable Energy Projects as of January 2024 lists 36 projects in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The Amazon Sustainability Report 2023 includes a table of workforce representation by gender. In the row for ""Amazon Workforce (All Levels)"" the U.S. column lists 52.9% for men, indicating that 52.9% of Amazon’s U.S. workforce identified as men in 2023.","52.9","percent","[""amazon2023""]","is_blank","is_blank","The Amazon Sustainability Report 2023 includes a table of workforce representation by gender. In the row for ""Amazon Workforce (All Levels)"" the U.S. column lists 52.9% for men, indicating that 52.9% of Amazon’s U.S. workforce identified as men in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The provided context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b"", ""23""]","is_blank","is_blank","The provided context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The paper explicitly states that a new metric called the granularity metric was introduced to quantify the ratio of calculation to communication time for distributed training across continents.","granularity metric","is_blank","[""erben2023""]","is_blank","is_blank","The paper explicitly states that a new metric called the granularity metric was introduced to quantify the ratio of calculation to communication time for distributed training across continents."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","times","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines the term as ""Water withdrawal"", describing freshwater taken from ground or surface sources for various uses.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The context defines the term as ""Water withdrawal"", describing freshwater taken from ground or surface sources for various uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The documents state that the Standing Committee forms a Study Panel every five years to assess the current state of AI.","5","years","[""stone2022""]","is_blank","is_blank","The documents state that the Standing Committee forms a Study Panel every five years to assess the current state of AI."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The provided excerpts from the Wu et al. (2021) paper cite Deloitte 2021, stating that the average U.S. household has 25 connected devices.","25","devices","[""wu2021b""]","is_blank","is_blank","The provided excerpts from the Wu et al. (2021) paper cite Deloitte 2021, stating that the average U.S. household has 25 connected devices."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","MWh","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011, contradicting the claim that it did not beat them.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011, contradicting the claim that it did not beat them."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The Luccioni et al. study explicitly states that 10–50 queries to GPT‑3 consume about half a liter of water.","10–50 queries","queries","[""luccioni2025a""]","is_blank","is_blank","The Luccioni et al. study explicitly states that 10–50 queries to GPT‑3 consume about half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The table in the provided context lists MT‑Bench scores for JetMoE‑8B‑chat and Llama‑2‑13b‑chat after alignment, showing JetMoE‑8B‑chat scored 6.681 versus 6.650 for Llama‑2‑13b‑chat.","6.681","score","[""shen2024""]","is_blank","is_blank","The table in the provided context lists MT‑Bench scores for JetMoE‑8B‑chat and Llama‑2‑13b‑chat after alignment, showing JetMoE‑8B‑chat scored 6.681 versus 6.650 for Llama‑2‑13b‑chat."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The document states that GPT‑3 consumes a 500 mL bottle of water for roughly 10–50 medium‑length responses, depending on deployment conditions.","10-50","responses","[""li2025b""]","is_blank","is_blank","The document states that GPT‑3 consumes a 500 mL bottle of water for roughly 10–50 medium‑length responses, depending on deployment conditions."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The paper states that compared to a single GPU type, Mélange can cut deployment costs by up to 77% in conversational (short‑context) settings.","77","percent","[""griggs2024""]","is_blank","is_blank","The paper states that compared to a single GPU type, Mélange can cut deployment costs by up to 77% in conversational (short‑context) settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The documents state that the bare‑minimum configuration for running the 65B LLaMA model without any compression or quantization is 4 A100 GPUs with 80 GB memory each. This is explicitly mentioned in the text and confirmed by Table II, which lists 4 A100 GPUs as the minimum for the 65B variant.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The documents state that the bare‑minimum configuration for running the 65B LLaMA model without any compression or quantization is 4 A100 GPUs with 80 GB memory each. This is explicitly mentioned in the text and confirmed by Table II, which lists 4 A100 GPUs as the minimum for the 65B variant."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The provided excerpts state that training a Llama‑3.1‑scale model can emit air pollutants equivalent to more than 10,000 round trips between Los Angeles and New York City.","10,000+","round trips","[""han2024"", ""luccioni2025c""]","is_blank","is_blank","The provided excerpts state that training a Llama‑3.1‑scale model can emit air pollutants equivalent to more than 10,000 round trips between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The study explicitly states that the total emissions from all model experimentation and evaluation were 178.97 kg of CO₂eq.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The study explicitly states that the total emissions from all model experimentation and evaluation were 178.97 kg of CO₂eq."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that estimating energy consumption using a GPU’s TDP almost always overestimates actual usage, as GPUs rarely draw maximum power continuously. It even notes a worst‑case overestimation factor of 4.1, indicating that TDP‑based estimates are not reliable or accurate.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context states that estimating energy consumption using a GPU’s TDP almost always overestimates actual usage, as GPUs rarely draw maximum power continuously. It even notes a worst‑case overestimation factor of 4.1, indicating that TDP‑based estimates are not reliable or accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT‑4o mini consumes more energy than GPT‑4o, with a 20% increase on long queries and a higher per‑query Wh value (3.098 Wh vs 2.875 Wh). Therefore, GPT‑4o mini does not consume less energy per query.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context states that GPT‑4o mini consumes more energy than GPT‑4o, with a 20% increase on long queries and a higher per‑query Wh value (3.098 Wh vs 2.875 Wh). Therefore, GPT‑4o mini does not consume less energy per query."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The figure for Mixtral‑CS‑A100‑40GB shows the throughput curve for the dense configuration. The plotted point for batch size 1 on the dense line is approximately 0.3 qps, while the text confirms that batch size 2 achieves 0.5 qps for the dense model, indicating the 0.3 qps value for batch size 1.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","The figure for Mixtral‑CS‑A100‑40GB shows the throughput curve for the dense configuration. The plotted point for batch size 1 on the dense line is approximately 0.3 qps, while the text confirms that batch size 2 achieves 0.5 qps for the dense model, indicating the 0.3 qps value for batch size 1."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The 2025 paper notes that direct environmental disclosures peaked in 2022 and then declined afterward, with a notable reversal in the trend and a majority of models falling into the no-disclosure category by early 2025.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The 2025 paper notes that direct environmental disclosures peaked in 2022 and then declined afterward, with a notable reversal in the trend and a majority of models falling into the no-disclosure category by early 2025."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The table in the Griggs et al. (2024) document lists the normalized on‑demand price for each GPU type. For the H100, the price is shown as $7.5164 per hour.","7.5164","USD per hour","[""griggs2024""]","is_blank","is_blank","The table in the Griggs et al. (2024) document lists the normalized on‑demand price for each GPU type. For the H100, the price is shown as $7.5164 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","is_blank","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","is_blank"
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama‑2‑70B at BF16 precision with 2 NVIDIA A100 GPUs costs over $5,200 per month. Dividing that monthly cost by 30 days and 24 hours gives an hourly estimate of roughly $7.22.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context states that serving Llama‑2‑70B at BF16 precision with 2 NVIDIA A100 GPUs costs over $5,200 per month. Dividing that monthly cost by 30 days and 24 hours gives an hourly estimate of roughly $7.22."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that increasing the BLEU score from 5 to 40 for GPT‑3 requires a model 1,000× larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that increasing the BLEU score from 5 to 40 for GPT‑3 requires a model 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","Unable to answer with confidence based on the provided documents.","is_blank","miles","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The inference energy per query for BLOOMz-7B is 1.0 × 10⁻⁴ kWh. One download generating 1 million inferences would consume 1 million × 0.0001 kWh = 100 kWh. With 606,096 downloads, the total energy is 606,096 × 100 kWh = 60,609,600 kWh, which equals 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","is_blank","The inference energy per query for BLOOMz-7B is 1.0 × 10⁻⁴ kWh. One download generating 1 million inferences would consume 1 million × 0.0001 kWh = 100 kWh. With 606,096 downloads, the total energy is 606,096 × 100 kWh = 60,609,600 kWh, which equals 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The report on JetMoE-8B explicitly states that the model was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The report on JetMoE-8B explicitly states that the model was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The paper states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The paper states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","is_blank","is_blank","FLOPS","[""is_blank""]","is_blank","is_blank","is_blank"
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The documents state that the bare‑minimum configuration for running the 65B LLaMA model on V100 GPUs is 8 GPUs with 32 GB each. Table II also lists 8 V100 GPUs as the required count for the 65B model.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The documents state that the bare‑minimum configuration for running the 65B LLaMA model on V100 GPUs is 8 GPUs with 32 GB each. Table II also lists 8 V100 GPUs as the required count for the 65B model."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","is_blank","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","is_blank"
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The provided context does not contain a specific numeric value for AlexNet’s top‑1 ImageNet accuracy. It only mentions that AlexNet has lower accuracy than ResNet, but no exact figure is given.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain a specific numeric value for AlexNet’s top‑1 ImageNet accuracy. It only mentions that AlexNet has lower accuracy than ResNet, but no exact figure is given."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The document states that FLM-101B was trained on a cluster of 24 DGX‑A800 GPU servers, each server containing 8 GPUs. Multiplying 24 servers by 8 GPUs per server yields 192 GPUs in total.","192","GPUs","[""li2025a""]","is_blank","is_blank","The document states that FLM-101B was trained on a cluster of 24 DGX‑A800 GPU servers, each server containing 8 GPUs. Multiplying 24 servers by 8 GPUs per server yields 192 GPUs in total."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","lifetimes","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The provided context states that FAIR’s RoBERTa, trained on 160 GB of text, required around 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The provided context states that FAIR’s RoBERTa, trained on 160 GB of text, required around 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The Amazon Sustainability Report states that the 50 new on‑site solar systems added in 2023 generate about 123,000 MWh annually and avoid roughly 47,500 metric tons of CO₂e compared with non‑renewable electricity.","47500","metric tons","[""amazon2023""]","is_blank","is_blank","The Amazon Sustainability Report states that the 50 new on‑site solar systems added in 2023 generate about 123,000 MWh annually and avoid roughly 47,500 metric tons of CO₂e compared with non‑renewable electricity."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context states that the LLMs were deployed using the vllm library, which runs on a Ray cluster for multi‑node computations, indicating that vllm is the framework used for deployment across multiple GPUs and nodes.","vllm","is_blank","[""zschache2025""]","is_blank","is_blank","The context states that the LLMs were deployed using the vllm library, which runs on a Ray cluster for multi‑node computations, indicating that vllm is the framework used for deployment across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The table comparing single‑node and double‑node deployments shows that the Llama 3.1 70B model consumed 161.59 Wh on one node and 304.77 Wh on two nodes, yielding a factor of 304.77 / 161.59 ≈ 1.89.","1.89","multiplier","[""zschache2025""]","is_blank","is_blank","The table comparing single‑node and double‑node deployments shows that the Llama 3.1 70B model consumed 161.59 Wh on one node and 304.77 Wh on two nodes, yielding a factor of 304.77 / 161.59 ≈ 1.89."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The provided context does not contain any information about the amount of water consumed per ChatGPT user session in 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the amount of water consumed per ChatGPT user session in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The paper states that scaling a 0.42 Wh short query to 700 million queries per day results in annual electricity comparable to 35,000 U.S. homes.","35,000","homes","[""jegham2025""]","is_blank","is_blank","The paper states that scaling a 0.42 Wh short query to 700 million queries per day results in annual electricity comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The 2025 paper cites Microsoft’s own corporate report, stating a 34% rise in global water consumption from 2021 to 2022.","34","percent","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper cites Microsoft’s own corporate report, stating a 34% rise in global water consumption from 2021 to 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The provided excerpt explicitly states that for Yelp sentiment analysis, traditional models perform considerably worse than large language models (LLMs), indicating that their accuracy is not comparable to that of LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The provided excerpt explicitly states that for Yelp sentiment analysis, traditional models perform considerably worse than large language models (LLMs), indicating that their accuracy is not comparable to that of LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","PUE","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The analysis states that the projected total number of GPT‑4o queries in 2025 is about 772 billion, based on a growth pattern from January to December.","772","queries","[""jegham2025""]","is_blank","is_blank","The analysis states that the projected total number of GPT‑4o queries in 2025 is about 772 billion, based on a growth pattern from January to December."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","Table 3 in the JetMoE paper lists the OpenLLM Leaderboard average scores for several models, showing JetMoE-8B achieving an average of 53.0.","53.0","score","[""shen2024""]","is_blank","is_blank","Table 3 in the JetMoE paper lists the OpenLLM Leaderboard average scores for several models, showing JetMoE-8B achieving an average of 53.0."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","Unable to answer with confidence based on the provided documents.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The document states that the open‑source exemption should be eliminated and that open‑source models must adhere to the same reporting standards as proprietary models, contradicting the claim that they are fully exempt unless they pose systemic risk.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The document states that the open‑source exemption should be eliminated and that open‑source models must adhere to the same reporting standards as proprietary models, contradicting the claim that they are fully exempt unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","is_blank","is_blank","PUE","[""is_blank""]","is_blank","is_blank","is_blank"
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The document explicitly states that for the OPT‑2.7B model running on an AWS g4dn.xlarge instance, the KV Cache grows to 5.312 GB when the batch size is increased to 32.","5.312","GB","[""kim2025""]","is_blank","is_blank","The document explicitly states that for the OPT‑2.7B model running on an AWS g4dn.xlarge instance, the KV Cache grows to 5.312 GB when the batch size is increased to 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The cited excerpts from Cottier et al. (2024) explicitly state that for the four notable models—GPT‑3, OPT‑175B, GPT‑4, and Gemini Ultra—R&D staff costs (including equity) account for between 29 % and 49 % of the total amortized cost.","29-49%","percent","[""cottier2024""]","is_blank","is_blank","The cited excerpts from Cottier et al. (2024) explicitly state that for the four notable models—GPT‑3, OPT‑175B, GPT‑4, and Gemini Ultra—R&D staff costs (including equity) account for between 29 % and 49 % of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The documents state that energy consumption during inference was measured using the CodeCarbon package, which tracks GPU and CPU power usage.","CodeCarbon","is_blank","[""zschache2025""]","is_blank","is_blank","The documents state that energy consumption during inference was measured using the CodeCarbon package, which tracks GPU and CPU power usage."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The analysis of 100 news articles found that 53% of them cited the 3 Wh estimate or the claim that a ChatGPT query uses ten times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The analysis of 100 news articles found that 53% of them cited the 3 Wh estimate or the claim that a ChatGPT query uses ten times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","is_blank","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The excerpts state that energy per second—and consequently energy per response—rises as the number of GPU shards increases. For example, the text notes an average increase in energy per second with more shards and that energy per response estimates increase with shard count.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The excerpts state that energy per second—and consequently energy per response—rises as the number of GPU shards increases. For example, the text notes an average increase in energy per second with more shards and that energy per response estimates increase with shard count."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The paper states that mining the rare‑earth metals used in an H100 GPU (0.1% of its mass) consumes an additional 2.2 liters of water per GPU, which converts to 0.0022 kL.","0.0022","kL","[""morrison2025""]","is_blank","is_blank","The paper states that mining the rare‑earth metals used in an H100 GPU (0.1% of its mass) consumes an additional 2.2 liters of water per GPU, which converts to 0.0022 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context indicates that the current AI Act contains an exemption for open‑source models, meaning they are not required to report energy consumption. The proposal to eliminate this exemption is a future recommendation, not a current rule.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context indicates that the current AI Act contains an exemption for open‑source models, meaning they are not required to report energy consumption. The proposal to eliminate this exemption is a future recommendation, not a current rule."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","Both the Luccioni et al. (2025) paper and the Wu et al. (2021) paper state that Amazon, Microsoft, Meta, and Google accounted for almost 30% of all corporate PPAs worldwide in 2020.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","Both the Luccioni et al. (2025) paper and the Wu et al. (2021) paper state that Amazon, Microsoft, Meta, and Google accounted for almost 30% of all corporate PPAs worldwide in 2020."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The document reports that in 2023 the U.S. data centers incurred a total public health cost of about $6.7 billion, calculated using the average attribution method.","6.7","USD","[""han2024""]","is_blank","is_blank","The document reports that in 2023 the U.S. data centers incurred a total public health cost of about $6.7 billion, calculated using the average attribution method."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","multiplier","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The Arena dataset is the short‑context workload. The paper reports that for a 120 ms SLO, Mélange achieves a cost reduction ranging from 15 % to 77 % compared to single‑GPU baselines.","15-77%","percent","[""griggs2024""]","is_blank","is_blank","The Arena dataset is the short‑context workload. The paper reports that for a 120 ms SLO, Mélange achieves a cost reduction ranging from 15 % to 77 % compared to single‑GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The table of carbon emissions (Table 3) lists the net CO₂e for FLM‑101B as 26 tCO₂e, directly answering the question.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The table of carbon emissions (Table 3) lists the net CO₂e for FLM‑101B as 26 tCO₂e, directly answering the question."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The provided excerpts describe execution time breakdowns and throughput characteristics but do not give a specific numeric total execution time for a sparse Mixtral model with batch size 1 on an NVIDIA A40‑48 GB GPU.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The provided excerpts describe execution time breakdowns and throughput characteristics but do not give a specific numeric total execution time for a sparse Mixtral model with batch size 1 on an NVIDIA A40‑48 GB GPU."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context states that public health impacts of AI are highly unevenly distributed, disproportionately affecting low‑income communities and showing a ratio of up to 200 between highest and lowest county‑level per‑household costs.","0","is_blank","[""han2024""]","is_blank","is_blank","The context states that public health impacts of AI are highly unevenly distributed, disproportionately affecting low‑income communities and showing a ratio of up to 200 between highest and lowest county‑level per‑household costs."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The provided excerpt from wu2021b states that GPU theoretical performance per watt doubles every 3‑4 years, citing Sun et al., 2019, which directly supports the claim.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The provided excerpt from wu2021b states that GPU theoretical performance per watt doubles every 3‑4 years, citing Sun et al., 2019, which directly supports the claim."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The paper states that 4‑bit quantization was applied via the open‑source platform Ollama, which also enables local deployment of the models used in the financial sentiment case study.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The paper states that 4‑bit quantization was applied via the open‑source platform Ollama, which also enables local deployment of the models used in the financial sentiment case study."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The paper states that for multi‑cloud spot experiments they used an independent S3‑compatible storage provider, Backblaze B2, and accessed the data via shards in the .tar format using the WebDataset library.","Backblaze B2","is_blank","[""erben2023""]","is_blank","is_blank","The paper states that for multi‑cloud spot experiments they used an independent S3‑compatible storage provider, Backblaze B2, and accessed the data via shards in the .tar format using the WebDataset library."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey on 1 February 2024.","Edward J. Markey","is_blank","[""ebert2024"", ""78""]","is_blank","is_blank","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey on 1 February 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The Chen et al. (2025) table lists the price per chip for the NVIDIA H100 as $11.06 per hour.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","The Chen et al. (2025) table lists the price per chip for the NVIDIA H100 as $11.06 per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context from the Cottier 2024 analysis states that, even if all GPU failures were catastrophic, the expected hardware lifetime would be 3.7 years, which represents the estimated average GPU lifetime before retirement in AI data centers.","3.7","years","[""cottier2024""]","is_blank","is_blank","The context from the Cottier 2024 analysis states that, even if all GPU failures were catastrophic, the expected hardware lifetime would be 3.7 years, which represents the estimated average GPU lifetime before retirement in AI data centers."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","Table II in the cited paper lists the bare‑minimum hardware for each LLaMA variant. For the 13B model it specifies that 2 V100 GPUs with 32 GB each are required.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","Table II in the cited paper lists the bare‑minimum hardware for each LLaMA variant. For the 13B model it specifies that 2 V100 GPUs with 32 GB each are required."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context explicitly states that Google’s Gemma family of language models emitted 1247.61 tCO2e during pre‑training, which is more than four times the ""five cars"" estimate.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context explicitly states that Google’s Gemma family of language models emitted 1247.61 tCO2e during pre‑training, which is more than four times the ""five cars"" estimate."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that Peters et al. (2018) trained ELMo on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours.","336","hours","[""strubell2019""]","is_blank","is_blank","The context states that Peters et al. (2018) trained ELMo on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","L/kWh","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","PUE","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The AWS Sustainability Report states that in North America, moving workloads to AWS can reduce customers’ workload carbon footprints by up to 96% when the electricity used is 100% renewable.","96","percent","[""amazon2023""]","is_blank","is_blank","The AWS Sustainability Report states that in North America, moving workloads to AWS can reduce customers’ workload carbon footprints by up to 96% when the electricity used is 100% renewable."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The document explicitly states that fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU costs $3,460.","3460","USD","[""xia2024""]","is_blank","is_blank","The document explicitly states that fine‑tuning a sparse Mixtral model with 2 million queries on an NVIDIA H100 GPU costs $3,460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The provided study measured power usage during a BERT training run and found that the GPU alone accounted for 74% of the total energy consumption of the system, indicating that GPUs dominate the power draw in typical data‑center AI workloads.","74","percent","[""dodge2022""]","is_blank","is_blank","The provided study measured power usage during a BERT training run and found that the GPU alone accounted for 74% of the total energy consumption of the system, indicating that GPUs dominate the power draw in typical data‑center AI workloads."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The document states that training was performed on a cluster of 12 nodes and that the cluster contained 96 H100 GPUs.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The document states that training was performed on a cluster of 12 nodes and that the cluster contained 96 H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The paper states that during active training the average GPU power per node exceeds 600 W, which is more than 85 % of an H100’s 700 W maximum draw.","600","Watts","[""morrison2025""]","is_blank","is_blank","The paper states that during active training the average GPU power per node exceeds 600 W, which is more than 85 % of an H100’s 700 W maximum draw."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The table in Strubell et al. lists the estimated CO2e for an average human life over one year as 11,023 lbs.","11023","lbs","[""strubell2019""]","is_blank","is_blank","The table in Strubell et al. lists the estimated CO2e for an average human life over one year as 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The document reports that for the 13B model the A100 achieves about a 1.25‑fold increase in inference throughput (words, tokens, or responses per second) compared to the V100.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The document reports that for the 13B model the A100 achieves about a 1.25‑fold increase in inference throughput (words, tokens, or responses per second) compared to the V100."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The document states that a single short GPT‑4o query consumes 0.42 Wh (±0.13 Wh).","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The document states that a single short GPT‑4o query consumes 0.42 Wh (±0.13 Wh)."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that for the 400 TPS SLO, Max‑Performance’s g6e.xlarge costs $2.699, which is about 280% more expensive than InferSave’s top choice.","280","percent","[""kim2025""]","is_blank","is_blank","The context states that for the 400 TPS SLO, Max‑Performance’s g6e.xlarge costs $2.699, which is about 280% more expensive than InferSave’s top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The paper notes that for single‑token classification inference, the V100 or even the A30 GPU is more efficient than newer architectures.","V100 or A30","is_blank","[""zschache2025""]","is_blank","is_blank","The paper notes that for single‑token classification inference, the V100 or even the A30 GPU is more efficient than newer architectures."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The Schwartz 2019 paper explicitly states that Green AI requires reporting the financial cost or ""price tag"" of developing, training, and running models, making the statement true.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The Schwartz 2019 paper explicitly states that Green AI requires reporting the financial cost or ""price tag"" of developing, training, and running models, making the statement true."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The 2025 paper explicitly states that electronic waste worldwide reached 62 million tonnes in 2022.","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","The 2025 paper explicitly states that electronic waste worldwide reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","is_blank","is_blank","Watts","[""is_blank""]","is_blank","is_blank","is_blank"
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The document states that training GPT‑3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","is_blank","The document states that training GPT‑3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The document states that Facebook’s recommendation and ranking model sizes grew by a factor of 20 between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The document states that Facebook’s recommendation and ranking model sizes grew by a factor of 20 between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The paper reports cross‑efficiency DEA scores for 30 LLMs and states that OpenAI’s reasoning model ""o3‑mini"" achieved the highest score of 0.884, placing it at the top of the eco‑efficiency frontier.","o3‑mini","is_blank","[""jegham2025""]","is_blank","is_blank","The paper reports cross‑efficiency DEA scores for 30 LLMs and states that OpenAI’s reasoning model ""o3‑mini"" achieved the highest score of 0.884, placing it at the top of the eco‑efficiency frontier."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context states that the current average lifetime for cell phones is less than 3 years, which directly supports the claim that smartphones have lifetimes below this threshold and contribute to e‑waste concerns.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context states that the current average lifetime for cell phones is less than 3 years, which directly supports the claim that smartphones have lifetimes below this threshold and contribute to e‑waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The intra‑zone experiments with T4 GPUs show that adding a second GPU does not improve throughput due to the Hivemind penalty, indicating that per‑GPU speedup is not nearly linear for CV models.","0","is_blank","[""erben2023""]","is_blank","is_blank","The intra‑zone experiments with T4 GPUs show that adding a second GPU does not improve throughput due to the Hivemind penalty, indicating that per‑GPU speedup is not nearly linear for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The document explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","is_blank","The document explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The excerpts from the document state that LLM decoding is memory‑intensive and low compute‑intensity, leading to underutilization of GPU compute units and lower power draw. In contrast, diffusion models are described as significantly more compute‑intensive and consume nearly the GPU’s maximum power. This directly supports the claim that LLMs generally have lower power draw during inference than diffusion models.","1","is_blank","[""chung2025""]","is_blank","is_blank","The excerpts from the document state that LLM decoding is memory‑intensive and low compute‑intensity, leading to underutilization of GPU compute units and lower power draw. In contrast, diffusion models are described as significantly more compute‑intensive and consume nearly the GPU’s maximum power. This directly supports the claim that LLMs generally have lower power draw during inference than diffusion models."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","","","percent","[]","is_blank","is_blank",""
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","percent","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that after optimization, metrics such as accuracy and F1 score are slightly lower, indicating a trade‑off rather than an improvement. Therefore, the claim that they always improved is false.","0","is_blank","[""khan2025""]","is_blank","is_blank","The context states that after optimization, metrics such as accuracy and F1 score are slightly lower, indicating a trade‑off rather than an improvement. Therefore, the claim that they always improved is false."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The Strubell et al. paper cites the EPA 2018 figure for average CO₂ emissions in the U.S. and gives the conversion factor as 0.954 pounds of CO₂ per kilowatt‑hour.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","The Strubell et al. paper cites the EPA 2018 figure for average CO₂ emissions in the U.S. and gives the conversion factor as 0.954 pounds of CO₂ per kilowatt‑hour."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The document cites a recent study that projects global AI electricity consumption for 2027 to be between 85 and 134 TWh.","85–134","TWh","[""li2025b""]","is_blank","is_blank","The document cites a recent study that projects global AI electricity consumption for 2027 to be between 85 and 134 TWh."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The 2023 Amazon Sustainability Report states that in Europe Amazon delivered 150 million packages via electric vehicles.","150","packages","[""amazon2023""]","is_blank","is_blank","The 2023 Amazon Sustainability Report states that in Europe Amazon delivered 150 million packages via electric vehicles."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload uses 128 input tokens and 512 output tokens per request, and evaluates 3000 requests. Multiplying the per‑request token count (640) by the number of requests (3000) gives the total tokens processed.","1920000","tokens","[""kim2025""]","is_blank","is_blank","The online inference workload uses 128 input tokens and 512 output tokens per request, and evaluates 3000 requests. Multiplying the per‑request token count (640) by the number of requests (3000) gives the total tokens processed."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The excerpts from the document state that the AI Act does not require disclosure of greenhouse gas emissions from AI applications, including those used in oil and gas exploration. Specifically, it notes that the Act ""fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration.""","0","is_blank","[""ebert2024""]","is_blank","is_blank","The excerpts from the document state that the AI Act does not require disclosure of greenhouse gas emissions from AI applications, including those used in oil and gas exploration. Specifically, it notes that the Act ""fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration."""
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The paper states that for the short DenseNet 201 job, Flexible Start can achieve reductions greater than 30% in several regions and up to 80% specifically in the West US region.","80","percent","[""dodge2022""]","is_blank","is_blank","The paper states that for the short DenseNet 201 job, Flexible Start can achieve reductions greater than 30% in several regions and up to 80% specifically in the West US region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The study reports mean energy consumption per 1,000 inferences of 0.002 kWh for text classification and 2.9 kWh for image generation, indicating that image generation requires roughly 2.9 / 0.002 ≈ 1450 times more energy than text classification.","1450","times","[""luccioni2024""]","is_blank","is_blank","The study reports mean energy consumption per 1,000 inferences of 0.002 kWh for text classification and 2.9 kWh for image generation, indicating that image generation requires roughly 2.9 / 0.002 ≈ 1450 times more energy than text classification."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The provided excerpts from the study explicitly state that traditional models perform considerably worse than large language models on the Yelp sentiment analysis task, indicating that their accuracy is not comparable to that of LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The provided excerpts from the study explicitly state that traditional models perform considerably worse than large language models on the Yelp sentiment analysis task, indicating that their accuracy is not comparable to that of LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The table in the Amazon Sustainability Report lists the number of renewable energy projects by location. For the United States it shows 244 projects, indicating that 244 Amazon Renewable Energy Projects were announced in the U.S. as of January 2024.","244","projects","[""amazon2023""]","is_blank","is_blank","The table in the Amazon Sustainability Report lists the number of renewable energy projects by location. For the United States it shows 244 projects, indicating that 244 Amazon Renewable Energy Projects were announced in the U.S. as of January 2024."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The table from the Strubell 2019 document lists the percentage of energy sourced from renewable sources for Amazon‑AWS in 2018, showing 17%.","17","percent","[""strubell2019""]","is_blank","is_blank","The table from the Strubell 2019 document lists the percentage of energy sourced from renewable sources for Amazon‑AWS in 2018, showing 17%."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors explicitly state that energy consumption should be reported at the cumulative server level, noting that this level captures total computation-related power usage and is more feasible than finer-grained measurements.","cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The authors explicitly state that energy consumption should be reported at the cumulative server level, noting that this level captures total computation-related power usage and is more feasible than finer-grained measurements."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The experiment measured power draw of GPU, CPU, and DRAM while training BERT‑base on a single NVIDIA TITAN X. Table 1 shows the GPU consumed 187.1 W out of a total 251.6 W, which is 74% of the total energy consumption.","74","percent","[""dodge2022""]","is_blank","is_blank","The experiment measured power draw of GPU, CPU, and DRAM while training BERT‑base on a single NVIDIA TITAN X. Table 1 shows the GPU consumed 187.1 W out of a total 251.6 W, which is 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The excerpt from Griggs et al. explicitly states that serving Llama2‑70b at BF16 precision requires two NVIDIA A100‑80GB GPUs.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The excerpt from Griggs et al. explicitly states that serving Llama2‑70b at BF16 precision requires two NVIDIA A100‑80GB GPUs."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The document reports that the iterative hardware‑software optimization led to a 28.5% reduction in Facebook’s operational energy footprint over the two‑year period (2019‑2021).","28.5","percent","[""wu2021a""]","is_blank","is_blank","The document reports that the iterative hardware‑software optimization led to a 28.5% reduction in Facebook’s operational energy footprint over the two‑year period (2019‑2021)."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","is_blank","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","is_blank"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The document explicitly states that the estimated upfront hardware acquisition cost for GPT‑4 is $800 million.","800000000","USD","[""cottier2024""]","is_blank","is_blank","The document explicitly states that the estimated upfront hardware acquisition cost for GPT‑4 is $800 million."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The text states that Sustainable AI was proposed to both use AI in climate‑positive applications and to improve the environmental sustainability of AI approaches, so it is not limited to only climate‑positive uses.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The text states that Sustainable AI was proposed to both use AI in climate‑positive applications and to improve the environmental sustainability of AI approaches, so it is not limited to only climate‑positive uses."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The figure and accompanying text show that the ground truth maximum batch size for fine‑tuning Mixtral on an NVIDIA A100‑40GB GPU is 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","The figure and accompanying text show that the ground truth maximum batch size for fine‑tuning Mixtral on an NVIDIA A100‑40GB GPU is 28 samples."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The document states that when the server is overloaded, swapping consistently consumes less energy than recomputation, as shown in Figure 8 and the accompanying text.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","The document states that when the server is overloaded, swapping consistently consumes less energy than recomputation, as shown in Figure 8 and the accompanying text."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","Unable to answer with confidence based on the provided documents.","is_blank","percent","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The provided context does not contain any information about McKinsey projections for data center electricity consumption in 2030.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about McKinsey projections for data center electricity consumption in 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The paper’s Figure 4 shows that for the 6 B‑parameter transformer, doubling the run time with the Pause‑and‑Resume strategy can reduce emissions by up to about 25 %. This is the highest savings reported for that model.","25","percent","[""dodge2022""]","is_blank","is_blank","The paper’s Figure 4 shows that for the 6 B‑parameter transformer, doubling the run time with the Pause‑and‑Resume strategy can reduce emissions by up to about 25 %. This is the highest savings reported for that model."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The provided excerpts state that JetMoE-8B activates only 2B of its 8B parameters per token, which results in a reduction of inference computation by about 70% relative to Llama2-7B.","70","percent","[""shen2024""]","is_blank","is_blank","The provided excerpts state that JetMoE-8B activates only 2B of its 8B parameters per token, which results in a reduction of inference computation by about 70% relative to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context cites the seminal 2019 study by Strubell et al. and reports that the carbon footprint of training BERT was quantified as 626,155 pounds of CO2e.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context cites the seminal 2019 study by Strubell et al. and reports that the carbon footprint of training BERT was quantified as 626,155 pounds of CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT‑3 was trained with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b""]","is_blank","is_blank","The context explicitly states that GPT‑3 was trained with an estimated training energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The documents state that the MoE layer is the most expensive part of fine‑tuning and is a prime target for optimization, indicating it is often focused on when improving performance.","1","is_blank","[""xia2024""]","is_blank","is_blank","The documents state that the MoE layer is the most expensive part of fine‑tuning and is a prime target for optimization, indicating it is often focused on when improving performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The analytical model and the accompanying figure (Fig. 13) in the paper project the maximum batch size for fine‑tuning Mixtral on an NVIDIA A40 (48 GB) GPU. The figure labels the A40 entry with a batch size of 28 samples.","28","samples","[""xia2024""]","is_blank","is_blank","The analytical model and the accompanying figure (Fig. 13) in the paper project the maximum batch size for fine‑tuning Mixtral on an NVIDIA A40 (48 GB) GPU. The figure labels the A40 entry with a batch size of 28 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The excerpts from the paper state that for the CV task, which has high granularity, intercontinental training results in a slowdown of only 7% compared to the fully local experiment. This directly supports the claim.","1","is_blank","[""erben2023""]","is_blank","is_blank","The excerpts from the paper state that for the CV task, which has high granularity, intercontinental training results in a slowdown of only 7% compared to the fully local experiment. This directly supports the claim."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The study explicitly states that the BERT-based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g of CO₂eq per 1,000 text classification queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The study explicitly states that the BERT-based model bert‑base‑multilingual‑uncased‑sentiment emits 0.32 g of CO₂eq per 1,000 text classification queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The document states that the emissions for training BERT vary from about 7 k grams in the most efficient region to about 26 k grams in the least efficient region, giving a difference of roughly 19 k grams.","19","grams","[""dodge2022""]","is_blank","is_blank","The document states that the emissions for training BERT vary from about 7 k grams in the most efficient region to about 26 k grams in the least efficient region, giving a difference of roughly 19 k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The document states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The document states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The table in the Morrison 2025 paper lists the water consumption for the OLMo 60M model as 1.6 kL, which is equivalent to 5 days of water usage for an average U.S. person.","5","days","[""morrison2025""]","is_blank","is_blank","The table in the Morrison 2025 paper lists the water consumption for the OLMo 60M model as 1.6 kL, which is equivalent to 5 days of water usage for an average U.S. person."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","Unable to answer with confidence based on the provided documents.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The paper states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, implying that such an addition would lower, not raise, expenses.","0","is_blank","[""xia2024""]","is_blank","is_blank","The paper states that adding compute resources to accelerate the MoE layers is a way to further reduce cost, implying that such an addition would lower, not raise, expenses."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The table of carbon emissions lists the energy consumption for FLM‑101B as 40 MkWh, which is the total energy used during its pre‑training.","40","kWh","[""li2025a""]","is_blank","is_blank","The table of carbon emissions lists the energy consumption for FLM‑101B as 40 MkWh, which is the total energy used during its pre‑training."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","Unable to answer with confidence based on the provided documents.","is_blank","USD","[""is_blank""]","is_blank","is_blank","Unable to answer with confidence based on the provided documents."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The table in the provided document lists the estimated cost for fine‑tuning Mixtral on the GSM8K (GS) dataset with a sparse MoE configuration. For an NVIDIA A40‑48GB GPU, the cost column shows $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","The table in the provided document lists the estimated cost for fine‑tuning Mixtral on the GSM8K (GS) dataset with a sparse MoE configuration. For an NVIDIA A40‑48GB GPU, the cost column shows $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}","is_blank","samples","is_blank","is_blank","is_blank","Error: An error occurred (ValidationException) when calling the InvokeModel operation: {""error"":{""code"":""validation_error"",""message"":""ErrorEvent { error: APIError { type: \""BadRequestError\"", code: Some(400), message: \""EngineCore encountered an issue. See stack trace (above) for the root cause.\"", param: None } }"",""param"":null,""type"":""invalid_request_error""}}"
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","is_blank","is_blank","seconds","[""is_blank""]","is_blank","is_blank","is_blank"
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The document explicitly states that GPU‑level monitoring is not recommended for overall energy measurements, indicating the opposite of the claim.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The document explicitly states that GPU‑level monitoring is not recommended for overall energy measurements, indicating the opposite of the claim."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The article states that training accounted for only half of BLOOM’s overall emissions, indicating 50%.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The article states that training accounted for only half of BLOOM’s overall emissions, indicating 50%."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","Table II lists the bare‑minimum hardware for each LLaMA variant. For the 7B model on V100 32GB GPUs, the count is 1, indicating that a single V100 GPU is sufficient for inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","Table II lists the bare‑minimum hardware for each LLaMA variant. For the 7B model on V100 32GB GPUs, the count is 1, indicating that a single V100 GPU is sufficient for inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The table in the provided context lists the number of requests that would consume a 500 ml bottle of water for each data center location. For Arizona, the table shows 16.7 requests.","16.7","requests","[""li2025b""]","is_blank","is_blank","The table in the provided context lists the number of requests that would consume a 500 ml bottle of water for each data center location. For Arizona, the table shows 16.7 requests."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context from Dodge et al. explicitly states that the average U.S. home’s yearly energy use emits 8.30 metric tons of CO₂.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The context from Dodge et al. explicitly states that the average U.S. home’s yearly energy use emits 8.30 metric tons of CO₂."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","is_blank","is_blank","score","[""is_blank""]","is_blank","is_blank","is_blank"
